[
  {
    "objectID": "ws24-25/sum.html",
    "href": "ws24-25/sum.html",
    "title": "My Uni Notes",
    "section": "",
    "text": "##Week 1\n###Lecture 1\n\ndate:\n\n###Lecture 2\n\ndate:\n\n##Week 2\n###Lecture 1\n\ndate:\n\n###Lecture 2\n\ndate:\n\n##Week 3\n###Lecture 1\n\ndate:\n\n###Lecture 2\n\ndate:\n\n##Week 4\n###Lecture 1\n\ndate:\n\n###Lecture 2\n\ndate:\n\n##Week 5\n###Lecture 1\n\ndate:\n\n###Lecture 2\n\ndate:\n\n##Week 6\n###Lecture 1\n\ndate:\n\n###Lecture 2\n\ndate:\n\n##Week 7\n###Lecture 1\n\ndate:\n\n###Lecture 2\n\ndate:\n\n##Week 8\n###Lecture 1\n\ndate:\n\n###Lecture 2\n\ndate:\n\n##Week 9\n###Lecture 1\n\ndate:\n\n###Lecture 2\n\ndate:\n\n##Week 10\n###Lecture 1\n\ndate:\n\n###Lecture 2\n\ndate:\n\n##Week 11\n###Lecture 1\n\ndate:\n\n###Lecture 2\n\ndate:\n\n##Week 12\n###Lecture 1\n\ndate:\n\n###Lecture 2\n\ndate:\n\n##Week 13\n###Lecture 1\n\ndate:\n\n###Lecture 2\n\ndate:\n\n##Week 14\n###Lecture 1\n\ndate:\n\n###Lecture 2\n\ndate:\n\n##Week 15\n###Lecture 1\n\ndate:\n\n###Lecture 2\n\ndate:"
  },
  {
    "objectID": "ws24-25/its/index.html",
    "href": "ws24-25/its/index.html",
    "title": "IT-Security",
    "section": "",
    "text": "weekly summary\nnotes\nsolutions"
  },
  {
    "objectID": "ws24-25/its/index.html#section",
    "href": "ws24-25/its/index.html#section",
    "title": "IT-Security",
    "section": "",
    "text": "weekly summary\nnotes\nsolutions"
  },
  {
    "objectID": "ws24-25/isw/index.html",
    "href": "ws24-25/isw/index.html",
    "title": "Software Engineering",
    "section": "",
    "text": "weekly summary\nnotes\nsolutions",
    "crumbs": [
      "Software Engineering"
    ]
  },
  {
    "objectID": "ws24-25/isw/index.html#section",
    "href": "ws24-25/isw/index.html#section",
    "title": "Software Engineering",
    "section": "",
    "text": "weekly summary\nnotes\nsolutions",
    "crumbs": [
      "Software Engineering"
    ]
  },
  {
    "objectID": "ws24-25/ds/sum.html",
    "href": "ws24-25/ds/sum.html",
    "title": "My Uni Notes",
    "section": "",
    "text": "date:\n\n\n\n\n\ndate:"
  },
  {
    "objectID": "ws24-25/ds/sum.html#week-1",
    "href": "ws24-25/ds/sum.html#week-1",
    "title": "My Uni Notes",
    "section": "",
    "text": "date:\n\n\n\n\n\ndate:"
  },
  {
    "objectID": "ws24-25/ds/sum.html#week-2",
    "href": "ws24-25/ds/sum.html#week-2",
    "title": "My Uni Notes",
    "section": "Week 2",
    "text": "Week 2\n\nLecture 1\n\ndate:\n\n\n\nLecture 2\n\ndate:"
  },
  {
    "objectID": "ws24-25/ds/sum.html#week-3",
    "href": "ws24-25/ds/sum.html#week-3",
    "title": "My Uni Notes",
    "section": "Week 3",
    "text": "Week 3\n\nLecture 1\n\ndate:\n\n\n\nLecture 2\n\ndate:"
  },
  {
    "objectID": "ws24-25/ds/sum.html#week-4",
    "href": "ws24-25/ds/sum.html#week-4",
    "title": "My Uni Notes",
    "section": "Week 4",
    "text": "Week 4\n\nLecture 1\n\ndate:\n\n\n\nLecture 2\n\ndate:"
  },
  {
    "objectID": "ws24-25/ds/sum.html#week-5",
    "href": "ws24-25/ds/sum.html#week-5",
    "title": "My Uni Notes",
    "section": "Week 5",
    "text": "Week 5\n\nLecture 1\n\ndate:\n\n\n\nLecture 2\n\ndate:"
  },
  {
    "objectID": "ws24-25/ds/sum.html#week-6",
    "href": "ws24-25/ds/sum.html#week-6",
    "title": "My Uni Notes",
    "section": "Week 6",
    "text": "Week 6\n\nLecture 1\n\ndate:\n\n\n\nLecture 2\n\ndate:"
  },
  {
    "objectID": "ws24-25/ds/sum.html#week-7",
    "href": "ws24-25/ds/sum.html#week-7",
    "title": "My Uni Notes",
    "section": "Week 7",
    "text": "Week 7\n\nLecture 1\n\ndate:\n\n\n\nLecture 2\n\ndate:"
  },
  {
    "objectID": "ws24-25/ds/sum.html#week-8",
    "href": "ws24-25/ds/sum.html#week-8",
    "title": "My Uni Notes",
    "section": "Week 8",
    "text": "Week 8\n\nLecture 1\n\ndate:\n\n\n\nLecture 2\n\ndate:"
  },
  {
    "objectID": "ws24-25/ds/sum.html#week-9",
    "href": "ws24-25/ds/sum.html#week-9",
    "title": "My Uni Notes",
    "section": "Week 9",
    "text": "Week 9\n\nLecture 1\n\ndate:\n\n\n\nLecture 2\n\ndate:"
  },
  {
    "objectID": "ws24-25/ds/sum.html#week-10",
    "href": "ws24-25/ds/sum.html#week-10",
    "title": "My Uni Notes",
    "section": "Week 10",
    "text": "Week 10\n\nLecture 1\n\ndate:\n\n\n\nLecture 2\n\ndate:"
  },
  {
    "objectID": "ws24-25/ds/sum.html#week-11",
    "href": "ws24-25/ds/sum.html#week-11",
    "title": "My Uni Notes",
    "section": "Week 11",
    "text": "Week 11\n\nLecture 1\n\ndate:\n\n\n\nLecture 2\n\ndate:"
  },
  {
    "objectID": "ws24-25/ds/sum.html#week-12",
    "href": "ws24-25/ds/sum.html#week-12",
    "title": "My Uni Notes",
    "section": "Week 12",
    "text": "Week 12\n\nLecture 1\n\ndate:\n\n\n\nLecture 2\n\ndate:"
  },
  {
    "objectID": "ws24-25/ds/sum.html#week-13",
    "href": "ws24-25/ds/sum.html#week-13",
    "title": "My Uni Notes",
    "section": "Week 13",
    "text": "Week 13\n\nLecture 1\n\ndate:\n\n\n\nLecture 2\n\ndate:"
  },
  {
    "objectID": "ws24-25/ds/sum.html#week-14",
    "href": "ws24-25/ds/sum.html#week-14",
    "title": "My Uni Notes",
    "section": "Week 14",
    "text": "Week 14\n\nLecture 1\n\ndate:\n\n\n\nLecture 2\n\ndate:"
  },
  {
    "objectID": "ws24-25/ds/sum.html#week-15",
    "href": "ws24-25/ds/sum.html#week-15",
    "title": "My Uni Notes",
    "section": "Week 15",
    "text": "Week 15\n\nLecture 1\n\ndate:\n\n\n\nLecture 2\n\ndate:"
  },
  {
    "objectID": "ws23-24/num/sum.html",
    "href": "ws23-24/num/sum.html",
    "title": "Num Weekly Summary",
    "section": "",
    "text": "W01/VL01\n\ndate:\nsummary:\n\nW01/VL02\n\ndate:\nsummary:\n\nW02/VL03\n\ndate:\nsummary:\n\nW02/VL04:\n\ndate:\nsummary:\n\nW03/VL05\n\ndate: 31/10/2023, Tue\nsummary: \\(LR\\)-Decomposition, uniqueness, existence, algorithm, complexity, error analysis\n\nScript pages: 21 (satz 2.5) - 28 (2.3 Error Analysis of LR)\nLR Zerlegung ist eindeutig\nProduct of two triangular matrices is also triangular (left or right)\nInverse of a triangular matrix is triangular (left or right)\nProduct of two left-triangular matrices with \\(a_{ii} = 1\\) is also a left-triangular matrix with \\(\\tilde{a}_{ii} = 1\\)\n\\(LR\\)-Decomposition method using Gauss decomposition (Alg 2.7) (?).\nExistence of the \\(LR\\)-decomposition (Satz 2.8)\nPractical version of \\(LR\\)-decomposition Algorithm (not Alg 2.7) saves space (Alg 2.9).\nComplexity of Alg 2.9 and solving a LSE (De: LGS)\nError analysis of \\(LR\\)-Decomposition.\n\n\nW03/VL06\n\ndate: 02/11/23, Thu\nsummary: Script 28 - 33. Error estimation of Alg 2.9, forwards and backwards error, conditioning of a function, conditioning number.\n\nError estimation of Alg 2.9 with proof (Lemma 2.13 & Satz 2.14)\nAposteriori error estimation of Alg 2.9 (Satz 2.16)\nrelationship between backwards and forwards error, a python example demonstrating that they aren’t necessarily related (?)\nConditioning of a function (Ch 2.4.1).\nConditioning number\n\n\nW04/VL07\n\ndate: 07/11/23, Tue\nsummary: Script 33 - 42.\n\nQuestion: How much effect does the error in \\(A\\) & \\(b\\) have on the solutuion of an LEQ/LGS?\nConditioning of a matrix & its proof. (Prop 2.20)\nError analysis of LEQ/LGS’s (Ch 2.4.3)\nConvergent Matrix sequences and matrix series (Ch 2.4.4.)\n\nNeumann Series (2.4.9, P:37)\nSpectral radius of a matrix\n\nConvergent Matrix sequences are used in the proof of error estimation of LEQ/LGS’s. Proof of 2.2.1\nPivoting Strategies to increase \\(LR\\)-decomposition algorithm stability (Ch. 2.5) via exchanging rows during the steps of the algorithm.\n\nPermutation Matrix (Def 2.25)\n\n\n\nW04/VL08\n\ndate: 09/11/23, Thu\nsummary: Script 43 - 51\n\nnote: numbering shifted 1 up, due to a new example\nAlg. 2.27: \\(LR\\)-Decomposition with column pivot search.\n\nSatz 2.30: \\(\\forall\\) regular matrix \\(A \\in \\mathbb{R}^{n\\times n}\\, \\exists\\) a Permutations matrix \\(P_{\\pi}\\), s.t \\(LR = P_{\\pi}A\\) and its proof.\nA problematic matrix for this method: Wilkinson matrix. Such problems can be avoided with “Full Pivot Search/Vollpivotsuche”.\nBut fullpivot search has the disatvantageous complexity \\(\\mathcal{O}(n^3)\\)\n\nCholesky Decomposition: An efficient method for Symmetric Positive Definite (SPD) Matrices.\n\nDefinition & Properties of SPD matrices. (Def 2.32 & Satz 2.33)\nSatz 3.34: \\(A\\in\\mathbb{R}^{n\\times n} \\text{\\, SPD \\, }\\Rightarrow \\exists \\text{\\, upper triangular} \\, R\\in \\mathbb{R}^{n\\times n} \\text{\\, s.t.\\, } A = R^TR\\) (Cholesky decomposition)\nProof of Satz 3.34\nAlgorithm for Cholesky Decomposition (Alg 2.35) and its complexity (\\(\\frac{n^3}{3} + \\mathcal{O}(n^2)\\)). (2 times more efficient than usual decommposition)\n\n\\(LR\\)-Decomposition for Band-matrices\n\nSparce matrices (schwach besetzte matrix) \\(\\approx\\) many entries are 0.\nhow sparce matrices are stored efficiently: For example\n\nA = 0 5 0    C = (2, 3) {non-nul columns}\n    0 0 0    R = (1, 3) {non-null rows}\n    0 0 7    X = (5, 7) {actual entries in these coordinates}\n\nDefinition of a band matrix (Def 2.38)\n\\(LR\\)-decomposition for Band matrices.\n\n\n\nW05/VL09\n\ndate: 14.11.23, Tue\nsummary: Skript 51 - 55 (togehter with topics form appendix)\n\nReview of some LA topics (from Appendix):\n\nDef of orthogonal matrix\nDef of unitary matrix\nLemma A.57/Satz A.58: Singular value decomposition (SVD) and its proof.\nProgramming example demonstrating uses of SVD for image compression.\nProp A.61 regarding SVD (?)\n\nIntro to new Ch 3 - Interpolation & Approximation\n\nOverview of different types of interpolating functions: polynomial, rational (polynomial), spline, neural network\nIntro Polynomial Interpolation:\n\nDef of vector space of polynomials of degree \\(n\\): \\(\\mathbb{P}_n\\).\nDef 3.1: Lagrange Interpolation Polynomials\n\n\n\n\nW05/VL10\n\ndate: 16.11.23, Thu\nsummary: Skript: 55 - 64\n\nLagrange interpolation\n\nLagrange polynomials \\(\\{l_i\\}_{i=0\\dots n}\\) Form a basis for \\(\\mathbb{P}_n\\) & its proof.\n\nGeneral interpolation with a general basis \\(\\{b_i\\}_{i=0\\dots n}\\) \\(\\Rightarrow\\) Vandermonde Matrix. note: Vandermonde matrix for Lagrange Basis is simply the identity matrix. Vandermonde matrix is very badly conditioned for the monomial basis \\(\\{x^i\\}_{i=0}^n\\)\nError analysis of Lagrange interpolation. Certain properties of the function that is to be interpolated determine the precision of the error analysis (like the smoothness of the function.) (Satz 3.6)\nNeville’s Schema\n\n\nW06/VL11\n\ndate: 21.11.23, Tue\nsummary: Skript 64 - 71\n\nNewton Interpolation (Ch 3.5)\n\nDividierende Differenzen\nLemma 3.11 and its proof via induction.\n\nIntepolation error\nHermite Interpolation\n(3.7 is skipped)\n3.8 Conditioning and Approximation\nWeierstrass Theorem (3.25)\n\n\nW06/VL12\n\ndate: 23.11.23, Thu\nsummary: Skript 71 - 81\n\nreview of error of polynomial interpolation: Bsp 3.21, 3.22.\nRunge Phenomenon\nWeierstrass Theorem (3.25) and its relationship to polynomial interpolation.\nError of the best approximation (Satz 3.26)\n\nDef 3.23: Lebesgue Constant \\(\\Lambda_n\\)\n\nConditioning of the interpolation, what happens when we interpolate \\(f + \\delta\\) instead of \\(f\\)?\nIf the function is only known to be continuous (but not necessarily differentiable), there is another measure for how strong a function “fluctuates”: Modulus of continuity (Stetigkeitsmodul) (Def 3.27)\nJackson’s Theorem (3.28) given without proof.\nCorollary (3.29) - its proof.\nHow to keep \\(\\Lambda_n\\) small? \\(\\rightarrow\\) Chebyshev Interpolation, Chebyshev Points\n\nGoal: find appropriate points \\(\\{x_i\\}_{i=1\\dots n}\\) s.t. \\(\\Lambda_n\\) is minimal.\nDefinition of Chebyshev Points (Def 3.30): \\(x_i^{(n + 1)} = \\cos{(\\frac{2i + 1}{2n + 2}\\pi)}\\) \nDefinition of Chebyshev Polynomials (Def 3.32)\nLemma 3.33 - some properties of chebyshev polynomials\n\n\n\nW07/VL13\n\ndate: 28.11.23\nsummary: Skript 81 - 96 (some pages were skipped)\n\nClenshaw-Curtis points\nSatz 3.34\nProofs regarding Lebesgue Constats are skipped.\nLemma 3.40:\nSkip until Spline Interpolation\nSpline Interpolation:\n\nError of spline interpolation\nCubic splines"
  },
  {
    "objectID": "ws23-24/ipi/sum.html",
    "href": "ws23-24/ipi/sum.html",
    "title": "IPI Weekly Summary",
    "section": "",
    "text": "Week 1/~\n\ndate:\nsummary:\n\nWeek 1/~\n\ndate:\nsummary:\n\nWeek 2/VL 1\n\ndate:\nsummary:\n\nWeek 2/VL 2:\n\ndate:\nsummary:"
  },
  {
    "objectID": "ws23-24/index.html#courses",
    "href": "ws23-24/index.html#courses",
    "title": "WS 23/24",
    "section": "Courses",
    "text": "Courses\n\nNum\nIPI",
    "crumbs": [
      "Bachelor",
      "WS 23/24"
    ]
  },
  {
    "objectID": "ss25/scicomp/sum/w10.html",
    "href": "ss25/scicomp/sum/w10.html",
    "title": "Week 10",
    "section": "",
    "text": "desing patterns (cont.)\n\nstrategy pattern: example activation in neural networks\ncommand pattern\nadapter pattern\ncomposite pattern (not very relevant for the lecture)\ndecorator pattern (very useful)\ndesign pattern generally adhere to SOLID\n\nprogram desing concepts\n\n\n\n\ndependent and independent base classes",
    "crumbs": [
      "Weekly Summary",
      "Week 10"
    ]
  },
  {
    "objectID": "ss25/scicomp/sum/w10.html#vl-10---17.06.25",
    "href": "ss25/scicomp/sum/w10.html#vl-10---17.06.25",
    "title": "Week 10",
    "section": "",
    "text": "desing patterns (cont.)\n\nstrategy pattern: example activation in neural networks\ncommand pattern\nadapter pattern\ncomposite pattern (not very relevant for the lecture)\ndecorator pattern (very useful)\ndesign pattern generally adhere to SOLID\n\nprogram desing concepts\n\n\n\n\ndependent and independent base classes",
    "crumbs": [
      "Weekly Summary",
      "Week 10"
    ]
  },
  {
    "objectID": "ss25/scicomp/sum/w08.html",
    "href": "ss25/scicomp/sum/w08.html",
    "title": "Week 08",
    "section": "",
    "text": "inheritence (cont.)\n\nprivate / public inheritence\ndescructors, inheriting descructors, destruction order\n\nSOLID principle discussion\ndynamic polymorphisdm, virtual methods, vtables\ninterfaces\nmultiple inheritence - diamong of death\nRAII - resource allocation is acquisition\n\nexample of a custom smart pointer implementation - should be avoided of course",
    "crumbs": [
      "Weekly Summary",
      "Week 08"
    ]
  },
  {
    "objectID": "ss25/scicomp/sum/w08.html#vl-8---03.06.25",
    "href": "ss25/scicomp/sum/w08.html#vl-8---03.06.25",
    "title": "Week 08",
    "section": "",
    "text": "inheritence (cont.)\n\nprivate / public inheritence\ndescructors, inheriting descructors, destruction order\n\nSOLID principle discussion\ndynamic polymorphisdm, virtual methods, vtables\ninterfaces\nmultiple inheritence - diamong of death\nRAII - resource allocation is acquisition\n\nexample of a custom smart pointer implementation - should be avoided of course",
    "crumbs": [
      "Weekly Summary",
      "Week 08"
    ]
  },
  {
    "objectID": "ss25/scicomp/sum/w06.html",
    "href": "ss25/scicomp/sum/w06.html",
    "title": "Week 06",
    "section": "",
    "text": "Standard Functionality\n\nrandom number generation: C rand() is not that good (Pseudo-random) \\(\\Rightarrow\\) &lt;random&gt; in C++11+ is better\nalgorithms header\n\nfor_each\ntransform\ncount_if\nfind_if\ncopy_if\nshuffle\nsort\n…\n\nexceptions\n\nnoexcept (for compiler optimizations)\nmove semantics (short overview)\n\ntime measurements\n\nex.: benchmarking move semantics",
    "crumbs": [
      "Weekly Summary",
      "Week 06"
    ]
  },
  {
    "objectID": "ss25/scicomp/sum/w06.html#lecture-6---20.05.25",
    "href": "ss25/scicomp/sum/w06.html#lecture-6---20.05.25",
    "title": "Week 06",
    "section": "",
    "text": "Standard Functionality\n\nrandom number generation: C rand() is not that good (Pseudo-random) \\(\\Rightarrow\\) &lt;random&gt; in C++11+ is better\nalgorithms header\n\nfor_each\ntransform\ncount_if\nfind_if\ncopy_if\nshuffle\nsort\n…\n\nexceptions\n\nnoexcept (for compiler optimizations)\nmove semantics (short overview)\n\ntime measurements\n\nex.: benchmarking move semantics",
    "crumbs": [
      "Weekly Summary",
      "Week 06"
    ]
  },
  {
    "objectID": "ss25/scicomp/sum/w04.html",
    "href": "ss25/scicomp/sum/w04.html",
    "title": "Week 04",
    "section": "",
    "text": "classes and structs:\n\naccess specifiers. friend keyword\nconversion operators and delegating constructors, explicit keyword.\nconstructors and destructors: rule of five or zero\nmutable keyword\nstatic members, static keyword for functions\ninput / output stream operators\n\nstandard library\n\ncontainers\n\nsequantial containers: std::aray, std::vector, std::deque, std::list, std::forward_list.\nemplace_back() vs push_back()\ncontainer adaptors: implement some data structure in terms of others:\n\nstd::stack, based on std::deque\nstd::queue based on std::deque\nstd::priority_queue based on std::vector\n\nunsorted / sorted associative containers\ncompanion classes: std::pair&lt;T, U&gt;, std::tuple&lt;T..&gt; &lt;T..&gt;\\(\\Rightarrow\\) variadic template\niterators: all container types provide an associated iterator. container T =&gt; T::iterator, allowing iterating over the contents of the container.\n\n\ncurly brace initialization\nforwarding references",
    "crumbs": [
      "Weekly Summary",
      "Week 04"
    ]
  },
  {
    "objectID": "ss25/scicomp/sum/w04.html#l4---06.05.2025",
    "href": "ss25/scicomp/sum/w04.html#l4---06.05.2025",
    "title": "Week 04",
    "section": "",
    "text": "classes and structs:\n\naccess specifiers. friend keyword\nconversion operators and delegating constructors, explicit keyword.\nconstructors and destructors: rule of five or zero\nmutable keyword\nstatic members, static keyword for functions\ninput / output stream operators\n\nstandard library\n\ncontainers\n\nsequantial containers: std::aray, std::vector, std::deque, std::list, std::forward_list.\nemplace_back() vs push_back()\ncontainer adaptors: implement some data structure in terms of others:\n\nstd::stack, based on std::deque\nstd::queue based on std::deque\nstd::priority_queue based on std::vector\n\nunsorted / sorted associative containers\ncompanion classes: std::pair&lt;T, U&gt;, std::tuple&lt;T..&gt; &lt;T..&gt;\\(\\Rightarrow\\) variadic template\niterators: all container types provide an associated iterator. container T =&gt; T::iterator, allowing iterating over the contents of the container.\n\n\ncurly brace initialization\nforwarding references",
    "crumbs": [
      "Weekly Summary",
      "Week 04"
    ]
  },
  {
    "objectID": "ss25/scicomp/sum/w02.html",
    "href": "ss25/scicomp/sum/w02.html",
    "title": "Week 2",
    "section": "",
    "text": "overview of scientific computing\nsupercomputing\nc++ basic concepts:",
    "crumbs": [
      "Weekly Summary",
      "Week 2"
    ]
  },
  {
    "objectID": "ss25/scicomp/sum/w02.html#lecture-2",
    "href": "ss25/scicomp/sum/w02.html#lecture-2",
    "title": "Week 2",
    "section": "",
    "text": "overview of scientific computing\nsupercomputing\nc++ basic concepts:",
    "crumbs": [
      "Weekly Summary",
      "Week 2"
    ]
  },
  {
    "objectID": "ss25/scicomp/sum/index.html",
    "href": "ss25/scicomp/sum/index.html",
    "title": "Weekly Summary",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nTitle\n\n\nDate\n\n\n\n\n\n\nWeek 03\n\n\nApr 29, 2025\n\n\n\n\nWeek 04\n\n\nMay 6, 2025\n\n\n\n\nWeek 05\n\n\nMay 13, 2025\n\n\n\n\nWeek 06\n\n\nMay 20, 2025\n\n\n\n\nWeek 07\n\n\nMay 27, 2025\n\n\n\n\nWeek 08\n\n\nJun 3, 2025\n\n\n\n\nWeek 09\n\n\nJun 10, 2025\n\n\n\n\nWeek 1\n\n\n \n\n\n\n\nWeek 10\n\n\nJun 17, 2025\n\n\n\n\nWeek 11\n\n\nJun 23, 2025\n\n\n\n\nWeek 2\n\n\n \n\n\n\n\n \n\n\n \n\n\n\n\n \n\n\n \n\n\n\n\n \n\n\n \n\n\n\n\n \n\n\n \n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Weekly Summary"
    ]
  },
  {
    "objectID": "ss25/index.html",
    "href": "ss25/index.html",
    "title": "SS 25",
    "section": "",
    "text": "detailed plan",
    "crumbs": [
      "Bachelor",
      "SS 25"
    ]
  },
  {
    "objectID": "ss25/index.html#schedule",
    "href": "ss25/index.html#schedule",
    "title": "SS 25",
    "section": "",
    "text": "detailed plan",
    "crumbs": [
      "Bachelor",
      "SS 25"
    ]
  },
  {
    "objectID": "ss25/index.html#courses",
    "href": "ss25/index.html#courses",
    "title": "SS 25",
    "section": "Courses",
    "text": "Courses\n\nALDA\nIBN\nSciComp",
    "crumbs": [
      "Bachelor",
      "SS 25"
    ]
  },
  {
    "objectID": "ss25/index.html#time",
    "href": "ss25/index.html#time",
    "title": "SS 25",
    "section": "Time",
    "text": "Time",
    "crumbs": [
      "Bachelor",
      "SS 25"
    ]
  },
  {
    "objectID": "ss25/ibn/sum/w10.html",
    "href": "ss25/ibn/sum/w10.html",
    "title": "Week 10",
    "section": "",
    "text": "scheduling is relevant for the process group “ready”\nScheduling in Interactive Systems\n\nreactive and fast\nconcurring processes have to be managed fairly\n\nround-robin scheduling:\n\na queue for process scheduling\nquantum: the\nadventages of short quanta: higher reaction time\ndisadvantages of short quanta: process switching is costly\nmodern OS’s: 10 - 100 ms\nmultiple queues - multilevel queue scheduling\n\nLottery-Scheduling\nscheduling in real time systems (RTS)\nevaluating scheduling strategies:\n\nwe need an evalution criteria\n\nthroughput\nturnaround time\ncpu efficiency\n\n\n\\(\\Rightarrow\\) queuing theory / server theory:\n\nbasic:\n\narrivals\nqueue\nserver\ndepartures\n\nvariations\n\nmultiple queues,\nsome clients leave the system before being served\n…\n\ndefinitnions:\n\nT1 &lt; T2 &lt; T3 &lt; … &lt; Tn are the random arrival times of the clients\n\nwe define a random variable tk := Tk - T_{k - 1}. it’s called the interarrivel time, IAT\nwhat is the distribution of this random variable? poisson-distribution (?)\n\nService or processing time S1, S2, …\n\nwhich configuration is the best according to scheduling theory?\n\n\n\n\n\n\nmodern chips have special chips for communication between components\n\nnorthbridge\nsouthbridge",
    "crumbs": [
      "Weekly Summary",
      "Week 10"
    ]
  },
  {
    "objectID": "ss25/ibn/sum/w10.html#vl-19---16.06.25",
    "href": "ss25/ibn/sum/w10.html#vl-19---16.06.25",
    "title": "Week 10",
    "section": "",
    "text": "scheduling is relevant for the process group “ready”\nScheduling in Interactive Systems\n\nreactive and fast\nconcurring processes have to be managed fairly\n\nround-robin scheduling:\n\na queue for process scheduling\nquantum: the\nadventages of short quanta: higher reaction time\ndisadvantages of short quanta: process switching is costly\nmodern OS’s: 10 - 100 ms\nmultiple queues - multilevel queue scheduling\n\nLottery-Scheduling\nscheduling in real time systems (RTS)\nevaluating scheduling strategies:\n\nwe need an evalution criteria\n\nthroughput\nturnaround time\ncpu efficiency\n\n\n\\(\\Rightarrow\\) queuing theory / server theory:\n\nbasic:\n\narrivals\nqueue\nserver\ndepartures\n\nvariations\n\nmultiple queues,\nsome clients leave the system before being served\n…\n\ndefinitnions:\n\nT1 &lt; T2 &lt; T3 &lt; … &lt; Tn are the random arrival times of the clients\n\nwe define a random variable tk := Tk - T_{k - 1}. it’s called the interarrivel time, IAT\nwhat is the distribution of this random variable? poisson-distribution (?)\n\nService or processing time S1, S2, …\n\nwhich configuration is the best according to scheduling theory?\n\n\n\n\n\n\nmodern chips have special chips for communication between components\n\nnorthbridge\nsouthbridge",
    "crumbs": [
      "Weekly Summary",
      "Week 10"
    ]
  },
  {
    "objectID": "ss25/ibn/sum/w10.html#vl-20---18.06.25",
    "href": "ss25/ibn/sum/w10.html#vl-20---18.06.25",
    "title": "Week 10",
    "section": "VL 20 - 18.06.25",
    "text": "VL 20 - 18.06.25\n\nInternetworking\n\nintroduction and terms:\n\nhosts\ncommunication lines\nrouter and switches\nprotocolls\n\ntcp, ip, http, ethernet, skype\n\ninternet-standards\n\nformat rfc: request for comments\nietf: internet engineering task force\n\ninternet applications\ndns\n\nHistory of Internet\n\nvarious networks needed to be connected which lead to the internet:\nprinciples of internetworking:\n\nminimalism, autonomy\nbest effort model\nstateless router\ndecentralized control\n\ntcp, udp, ip\nethernet\nftp - a simple file transfer protocol\nOSI\n\npacket switching vs ‘leitungsvermittlung’\nprotocoll stack",
    "crumbs": [
      "Weekly Summary",
      "Week 10"
    ]
  },
  {
    "objectID": "ss25/ibn/sum/w08.html",
    "href": "ss25/ibn/sum/w08.html",
    "title": "Week 8",
    "section": "",
    "text": "inodes (cont.)\nMBR (booting)\nFATx - MS-DOS\nUNIX-V7\nstructure of a hard disk\n\nshortest seek time first (sstf)\n\n\n\n\n\nRAID 0\nRAID 1\n\nmirroring\nduplexing\n\nRAID 2 - bit-level striping, too complicated due to the need to synchronize hard disks\nRAID 3 - XOR (reconstructing ), implementation is also very complex\nRAID 4\nRAID 5\nRAID 6 - like RAID 5 but wwith reed-solomon codes instead of XOR.",
    "crumbs": [
      "Weekly Summary",
      "Week 8"
    ]
  },
  {
    "objectID": "ss25/ibn/sum/w08.html#vl-15",
    "href": "ss25/ibn/sum/w08.html#vl-15",
    "title": "Week 8",
    "section": "",
    "text": "inodes (cont.)\nMBR (booting)\nFATx - MS-DOS\nUNIX-V7\nstructure of a hard disk\n\nshortest seek time first (sstf)\n\n\n\n\n\nRAID 0\nRAID 1\n\nmirroring\nduplexing\n\nRAID 2 - bit-level striping, too complicated due to the need to synchronize hard disks\nRAID 3 - XOR (reconstructing ), implementation is also very complex\nRAID 4\nRAID 5\nRAID 6 - like RAID 5 but wwith reed-solomon codes instead of XOR.",
    "crumbs": [
      "Weekly Summary",
      "Week 8"
    ]
  },
  {
    "objectID": "ss25/ibn/sum/w08.html#vl-16---04.06.25",
    "href": "ss25/ibn/sum/w08.html#vl-16---04.06.25",
    "title": "Week 8",
    "section": "VL 16 - 04.06.25",
    "text": "VL 16 - 04.06.25\n\nstructure of file systems, block, blockgroups\nlinux file systems: ext2, ext3, ext4, zfs.\n\n\nDeadlocks\n\ndeadlocks, deadlock example (p. 16)\nhow to prevent deadlock: first we define a system model\nconditions for a resource deadlock\n\nmutual exclusion: …\nhold and wait: …\nno preemption: …\ncircular wait: …\n\nthis conditions must occur simultaneously for a deadlock to take place.\nresourcen-belegungs-graph (RB Graph)\n\nmultiple instances of a resource\n\ncycle detecting algorithms\n\nrecource vectors and resources rest vector\ncurrent allocation matrix\nrequest matrix\n\nhow to resolve deadlocks:",
    "crumbs": [
      "Weekly Summary",
      "Week 8"
    ]
  },
  {
    "objectID": "ss25/ibn/sum/w06.html",
    "href": "ss25/ibn/sum/w06.html",
    "title": "Week 6",
    "section": "",
    "text": "paging and page tables recap\nframe tables\nsize of the page table:\nlogical address space is enormous compared to the size of a process (direct page tables are rarely used nowadays)\n\ninverted page tables\nhierarchical page tables",
    "crumbs": [
      "Weekly Summary",
      "Week 6"
    ]
  },
  {
    "objectID": "ss25/ibn/sum/w06.html#vl-11---19.05.25",
    "href": "ss25/ibn/sum/w06.html#vl-11---19.05.25",
    "title": "Week 6",
    "section": "",
    "text": "paging and page tables recap\nframe tables\nsize of the page table:\nlogical address space is enormous compared to the size of a process (direct page tables are rarely used nowadays)\n\ninverted page tables\nhierarchical page tables",
    "crumbs": [
      "Weekly Summary",
      "Week 6"
    ]
  },
  {
    "objectID": "ss25/ibn/sum/w06.html#vl-12---21.05.25",
    "href": "ss25/ibn/sum/w06.html#vl-12---21.05.25",
    "title": "Week 6",
    "section": "VL 12 - 21.05.25",
    "text": "VL 12 - 21.05.25\n\nrecap hiererchical page tables\nCR3 register for address of the page directory\ncontents of a entry in a page table\n\n\nVirtual Memory\n\nvirtual memory is improvement of swapping\nsome pages of a process might be stored away in the secondary memory - marked by the valid / invalid bit, and swapped in when needed\nprocess of how pages in the secondary memory are swapped in is quite slow and involves operating system software - can’t be done in hardware anymore, therefore much slower.\nThere is an auxillary to locate the swapped out pages\nvictim: the page that is chosen to be swapped out by the operating system\nbelady anomaly\n\n\nPage Swapping Algorithms\n\nOPT\nFIFO (not that good)\nsecond chance algorithm: takes advantage of additional information in the page table entries like\n\nR bit: if a page that is tried to be removed was read, it is not removed but appended at the end\n\nclock algorithm: same as second chance, but uses circular list instead to avoid the actual appending at the end.\nusing M and R together improves it further.",
    "crumbs": [
      "Weekly Summary",
      "Week 6"
    ]
  },
  {
    "objectID": "ss25/ibn/sum/w04.html",
    "href": "ss25/ibn/sum/w04.html",
    "title": "Week 4",
    "section": "",
    "text": "pthread_cond_t, boolean variable cv.(it is actually a signalling / triggering variable)\nposix state variables API: (Slide 11)\n\ncond_wait(condition, mut)\ncond_signal(condition)\ncond_broadcast(condition): set condition to true, wake all sleeping\n\nFlow of using cond vars (condition variables) (Slide 12 important):\n\n(A)-event producer, (B)-event consumer\nexample: archive a data automatically (Slide 14 important)\n\nSemaphores with active waiting - pseudocode. (semaphores require locks, locks require hardware solutions Klausurrelevant)\n\nactive waiting: infinite loop (ineffective). instead \\(\\Rightarrow\\) sleeping.\n\nsemaphores toy implementation in C with structs without active waiting, rather with sleep, and block on the system level.\n\n\n\n\nSlide: 29, …\n\nkeyword synchronized.\nkeywords wait() and notify().\n\nRace conditions, synchronization, semaphores, locks, mutexes are all Klausurrelevant.\n\n\n\n\nProcesses are in general independent and do not have an effect on each other. Nevertheless cooperation among processes is useful. How to achieve the cooperation between processes \\(\\Rightarrow\\) IPC.\n2 main IPC families:\n\nmessage passing - MP (safer)\nshared-memory - SM (faster)",
    "crumbs": [
      "Weekly Summary",
      "Week 4"
    ]
  },
  {
    "objectID": "ss25/ibn/sum/w04.html#vl-7---05.05.25",
    "href": "ss25/ibn/sum/w04.html#vl-7---05.05.25",
    "title": "Week 4",
    "section": "",
    "text": "pthread_cond_t, boolean variable cv.(it is actually a signalling / triggering variable)\nposix state variables API: (Slide 11)\n\ncond_wait(condition, mut)\ncond_signal(condition)\ncond_broadcast(condition): set condition to true, wake all sleeping\n\nFlow of using cond vars (condition variables) (Slide 12 important):\n\n(A)-event producer, (B)-event consumer\nexample: archive a data automatically (Slide 14 important)\n\nSemaphores with active waiting - pseudocode. (semaphores require locks, locks require hardware solutions Klausurrelevant)\n\nactive waiting: infinite loop (ineffective). instead \\(\\Rightarrow\\) sleeping.\n\nsemaphores toy implementation in C with structs without active waiting, rather with sleep, and block on the system level.\n\n\n\n\nSlide: 29, …\n\nkeyword synchronized.\nkeywords wait() and notify().\n\nRace conditions, synchronization, semaphores, locks, mutexes are all Klausurrelevant.\n\n\n\n\nProcesses are in general independent and do not have an effect on each other. Nevertheless cooperation among processes is useful. How to achieve the cooperation between processes \\(\\Rightarrow\\) IPC.\n2 main IPC families:\n\nmessage passing - MP (safer)\nshared-memory - SM (faster)",
    "crumbs": [
      "Weekly Summary",
      "Week 4"
    ]
  },
  {
    "objectID": "ss25/ibn/sum/w04.html#vl-8---07.05.25",
    "href": "ss25/ibn/sum/w04.html#vl-8---07.05.25",
    "title": "Week 4",
    "section": "VL 8 - 07.05.25",
    "text": "VL 8 - 07.05.25\n\nIPC (cont.)\nIPC has variouis APIs / Implementations\n\ndata streams: pipes, sockets, message queues, terminal, …\nevents\nremote procedure call\nshared memory\n\n\nMessage Passing\nmessage passing can be blocking and non-blocking;\n\nblocking = synchronous,\nnon-blocking = asynchronous.\n\nDistributed IPC: communication between distinct computers\n\nsockets: they imitate data systems\nremote proecedure calls\nweb services:\n\n\n\nMessage Passing via Pipes\n\nAnonymous pipes\ninstead of\nls -R &gt;tmp-file.txt\ngrep -ci '\\.jpg$' tmp-file.txt\nwe can do:\nls -R | grep -ci '\\.jpg$' \n\ncommunication is realized by fork(): all the data etc is inherited by the child process.\nclosely related is: producer / consumer problem\n\n\n\nNamed Pipes\nbi-directional communication\n\n\nData descriptors / handles\nDD (file descriptor) is an integer (an index to a file data structure) and not a pointer.\nThere are standard DD values. Any time a process is started in Linux, three “files” are automatically opened that have the following DD values:\n\n0: stdin\n1: stdout\n2: stderr\n\nThese can be redirected in the shell. In order to achidve this the “hard-coded” DDs must “overwritten” \\(\\Rightarrow\\) dup2():\nint dup2(int srcDD, int targetDD)",
    "crumbs": [
      "Weekly Summary",
      "Week 4"
    ]
  },
  {
    "objectID": "ss25/ibn/sum/w02.html",
    "href": "ss25/ibn/sum/w02.html",
    "title": "Week 2",
    "section": "",
    "text": "os api’s:\n\na quick exmaple: system calls in windows, reading a data.\nAPI’s in Windows vs Posix\nWindows Subsystem for Linux (WSL)\n\n\n\n\n\nProcess: an active, executing program\na programm becomes a process when the OS loads the program code to the memory =&gt; same program can be started many times =&gt; multiple processes of the same program\nMemory of a process:\n\nstack: function calls, return addresses, variables local to the stack\nheap: dynamically allocated memory for objects and arbitrarily large data structures.\ndata: global variables, constants.\nprogram code.\n\nstack and heap grow towards each other.\nProcess Control Block (PCB): the way OS managemes processes (process bookkeeping) (implemented as struct in C.)\nProcess management: creation, deletion, etc\n\nfork(): creates an identical child-process.\nexecve(): replace the memory contents of a process.\nwaitpid(): wait for the ending of a child-process.\n_exit(): end the process\n\nexample: a (very) mimimal, toy shell - application of fork()\ninit() in Posix\nprocess creation in windows.\nprocess management\n\n\n\n\nfork\n\n\n\nparent and child processes can be synchonozied with a wait() command in conjunction with fork().\n\n## Tutorial\n\noverview of bash\ngreeting=\"Hello, world!\"\necho \"$greeting\"",
    "crumbs": [
      "Weekly Summary",
      "Week 2"
    ]
  },
  {
    "objectID": "ss25/ibn/sum/w02.html#vl-3---23.04.25",
    "href": "ss25/ibn/sum/w02.html#vl-3---23.04.25",
    "title": "Week 2",
    "section": "",
    "text": "os api’s:\n\na quick exmaple: system calls in windows, reading a data.\nAPI’s in Windows vs Posix\nWindows Subsystem for Linux (WSL)\n\n\n\n\n\nProcess: an active, executing program\na programm becomes a process when the OS loads the program code to the memory =&gt; same program can be started many times =&gt; multiple processes of the same program\nMemory of a process:\n\nstack: function calls, return addresses, variables local to the stack\nheap: dynamically allocated memory for objects and arbitrarily large data structures.\ndata: global variables, constants.\nprogram code.\n\nstack and heap grow towards each other.\nProcess Control Block (PCB): the way OS managemes processes (process bookkeeping) (implemented as struct in C.)\nProcess management: creation, deletion, etc\n\nfork(): creates an identical child-process.\nexecve(): replace the memory contents of a process.\nwaitpid(): wait for the ending of a child-process.\n_exit(): end the process\n\nexample: a (very) mimimal, toy shell - application of fork()\ninit() in Posix\nprocess creation in windows.\nprocess management\n\n\n\n\nfork\n\n\n\nparent and child processes can be synchonozied with a wait() command in conjunction with fork().\n\n## Tutorial\n\noverview of bash\ngreeting=\"Hello, world!\"\necho \"$greeting\"",
    "crumbs": [
      "Weekly Summary",
      "Week 2"
    ]
  },
  {
    "objectID": "ss25/ibn/sum/index.html",
    "href": "ss25/ibn/sum/index.html",
    "title": "Weekly Summary",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nTitle\n\n\nDate\n\n\n\n\n\n\nWeek 1\n\n\nApr 14, 2025\n\n\n\n\nWeek 10\n\n\nJun 16, 2025\n\n\n\n\nWeek 11\n\n\nJun 23, 2025\n\n\n\n\nWeek 2\n\n\nApr 22, 2025\n\n\n\n\nWeek 3\n\n\nApr 29, 2025\n\n\n\n\nWeek 4\n\n\nMay 5, 2025\n\n\n\n\nWeek 5\n\n\nMay 12, 2025\n\n\n\n\nWeek 6\n\n\nMay 19, 2025\n\n\n\n\nWeek 7\n\n\nMay 26, 2025\n\n\n\n\nWeek 8\n\n\nJun 2, 2025\n\n\n\n\nWeek 9\n\n\nJun 11, 2025\n\n\n\n\n \n\n\n \n\n\n\n\n \n\n\n \n\n\n\n\n \n\n\n \n\n\n\n\n \n\n\n \n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Weekly Summary"
    ]
  },
  {
    "objectID": "ss25/alda/sum/w11.html",
    "href": "ss25/alda/sum/w11.html",
    "title": "Week 11",
    "section": "",
    "text": "revision of previous lecture: definition of a heap\n\nfor the heap tree and als its subtrees: root has the highest priority (max heap) \\(\\Rightarrow\\) parent has higher priority than its children\nthe heap tree is perfectly balanced & left-leaning \\(\\Rightarrow\\) flattening as Array is possible\nfor any node k\n\nparent: (k - 1) // 2\nleft child: 2 * k + 1\nright child 2 * k + 2\n\ninsert operation:\n\nappend new elements at the end of the array (O(1))\nwhenever necessary repair the heap-condition with upheap() after inserting\n\nimplementation\nclass Heap:\n    def __init__(self):\n        self.data = []\n\n    def  push(self, priority):\n        self.data.apipend(priority)\n        upheap(self.data, len(self.data) - 1)\n\n    def upheap(a, k):\n        while True:\n            if k == 0: # repair has ended\n                break\n            parent = (k - 1) // 2\n            if a[parent] &gt; a[k]: # heap-cond holds\n                break\n            a[parent], a[k] = a[k], a[parent] # swap\n            k = parent\n\n\ndeleting an element:\n\ndeleting the largest element (the first element):\n\n\nreplace last element (the smallest) with the first element (the largest) and delete the last element. (now the first element is violating the heap cond because it is the smallest)\nrepair the heap condition starting from the root, successively pushing node down\n\ndef pop(self):\n    last = len(self.data) - 1\n    self.data[0] = self[last]\n    del self.data[last]\n    downheap(self.data, last - 1)\n\ndef downheap(a, last):\n    k = 0 # \n    while True:\n        left, right = 2 * k + 1, 2 * k + 2 \n        if left &gt; last: # repair has ended, because k is a leaf\n            break\n        if right &lt;= last and a[right] &gt; a[left]: \n            child = right\n        else: \n            child = left\n        if a[k] &gt; a[child]: break # heap cond  holds\n        a[k], a[child] = a[child], a[k] # swap\n        k = child\nnumber of comparisons O(d) = O(logN)\n\n\n\nthe idea is to first create a heap from the array\nimplementation:\ndef heap_sort(a):\n    N = len(a)\n    for k in range(1, N):\n        upheap(a, k)\n    # a is sorted a a sa heap\n    for k in range(N - 1, 0, - 1): # loop iteration backwards\n        a[0], a[k] = a[k], a[0] # bring the currently largest element to the position k\n        downheap(a, k - 1) # repair the heap condition in the remaining heap \n\n\n\nTreap is a simultaneously both\n\na search tree\na heap\n\nimplementation:\nclass TreapNode:\n    def __init__(self, key, priority, value):\n        self.key = key\n        self.value = value\n        self.priority = priority\n        self.left = self.right = None\n\nidea:\n\nThe tree satisfies the search tree condition w.r.t the key\nThe tree satisfies the heap condition w.r.t. the priority\n\nThe inventor of the Treap DS showed that it is possible and feasible to satisfies both conditions at the same time\n\ne.g. insert:\n\nnormal tree_insert w.r.t the key (priority is ignored) \\(\\Rightarrow\\) search tree condition is satisfied\nrepair the heap condition on the way back of the recursive call stack, if the current node has lower priority than its child (important: the heap-condition can be only violated w.r.t to one of the children, namely in the subtree in which it was inserted)\n\nif the left child has higher priority \\(\\Rightarrow\\) right-rotation\nif the right child has higher priority \\(\\Rightarrow\\) left-rotation\n\n\n\npossible appropriate priorities:\n\nrandom numbers \\(\\Rightarrow\\) the tree is balanced on average\naccess counter \\(\\Rightarrow\\) rotate the element upwards if more often access than the parent (access optimized tree, i.e. important elements are closer to the root - and this is faster.)\n\n\n\n\n\ngiven:\n\na - an unsorted array\np - array, where the indices are stored in a sorted order.\np[i] -&gt; k (index). k is the index of an element before the sorting, s.t. i is the index after the sorting. (p is a permutation of the numbers 0, …, N - 1)\napplications:\n\niff a read-only, in-place is not possible\nif multiple arrays have to be sorted in the same way (?)\n\nimplementation\ndef index_sort(a, p):\n  r = [None] * len(a)\n  for i in range(len(a)):\n      r[i] = a[p[i]]\n  return r\np can be obtained by any sorting algorithm, where the key-functoin accesses the original Array\na = [...] # read-only array, to be sorted \np = list(range(len(a))) # indices 0, ..., N - 1\nquick_sort(p, key_function = lambda k: a[k])\nr = index_sort(a, p)",
    "crumbs": [
      "Weekly Summary",
      "Week 11"
    ]
  },
  {
    "objectID": "ss25/alda/sum/w11.html#vl-20---23.06.25",
    "href": "ss25/alda/sum/w11.html#vl-20---23.06.25",
    "title": "Week 11",
    "section": "",
    "text": "revision of previous lecture: definition of a heap\n\nfor the heap tree and als its subtrees: root has the highest priority (max heap) \\(\\Rightarrow\\) parent has higher priority than its children\nthe heap tree is perfectly balanced & left-leaning \\(\\Rightarrow\\) flattening as Array is possible\nfor any node k\n\nparent: (k - 1) // 2\nleft child: 2 * k + 1\nright child 2 * k + 2\n\ninsert operation:\n\nappend new elements at the end of the array (O(1))\nwhenever necessary repair the heap-condition with upheap() after inserting\n\nimplementation\nclass Heap:\n    def __init__(self):\n        self.data = []\n\n    def  push(self, priority):\n        self.data.apipend(priority)\n        upheap(self.data, len(self.data) - 1)\n\n    def upheap(a, k):\n        while True:\n            if k == 0: # repair has ended\n                break\n            parent = (k - 1) // 2\n            if a[parent] &gt; a[k]: # heap-cond holds\n                break\n            a[parent], a[k] = a[k], a[parent] # swap\n            k = parent\n\n\ndeleting an element:\n\ndeleting the largest element (the first element):\n\n\nreplace last element (the smallest) with the first element (the largest) and delete the last element. (now the first element is violating the heap cond because it is the smallest)\nrepair the heap condition starting from the root, successively pushing node down\n\ndef pop(self):\n    last = len(self.data) - 1\n    self.data[0] = self[last]\n    del self.data[last]\n    downheap(self.data, last - 1)\n\ndef downheap(a, last):\n    k = 0 # \n    while True:\n        left, right = 2 * k + 1, 2 * k + 2 \n        if left &gt; last: # repair has ended, because k is a leaf\n            break\n        if right &lt;= last and a[right] &gt; a[left]: \n            child = right\n        else: \n            child = left\n        if a[k] &gt; a[child]: break # heap cond  holds\n        a[k], a[child] = a[child], a[k] # swap\n        k = child\nnumber of comparisons O(d) = O(logN)\n\n\n\nthe idea is to first create a heap from the array\nimplementation:\ndef heap_sort(a):\n    N = len(a)\n    for k in range(1, N):\n        upheap(a, k)\n    # a is sorted a a sa heap\n    for k in range(N - 1, 0, - 1): # loop iteration backwards\n        a[0], a[k] = a[k], a[0] # bring the currently largest element to the position k\n        downheap(a, k - 1) # repair the heap condition in the remaining heap \n\n\n\nTreap is a simultaneously both\n\na search tree\na heap\n\nimplementation:\nclass TreapNode:\n    def __init__(self, key, priority, value):\n        self.key = key\n        self.value = value\n        self.priority = priority\n        self.left = self.right = None\n\nidea:\n\nThe tree satisfies the search tree condition w.r.t the key\nThe tree satisfies the heap condition w.r.t. the priority\n\nThe inventor of the Treap DS showed that it is possible and feasible to satisfies both conditions at the same time\n\ne.g. insert:\n\nnormal tree_insert w.r.t the key (priority is ignored) \\(\\Rightarrow\\) search tree condition is satisfied\nrepair the heap condition on the way back of the recursive call stack, if the current node has lower priority than its child (important: the heap-condition can be only violated w.r.t to one of the children, namely in the subtree in which it was inserted)\n\nif the left child has higher priority \\(\\Rightarrow\\) right-rotation\nif the right child has higher priority \\(\\Rightarrow\\) left-rotation\n\n\n\npossible appropriate priorities:\n\nrandom numbers \\(\\Rightarrow\\) the tree is balanced on average\naccess counter \\(\\Rightarrow\\) rotate the element upwards if more often access than the parent (access optimized tree, i.e. important elements are closer to the root - and this is faster.)\n\n\n\n\n\ngiven:\n\na - an unsorted array\np - array, where the indices are stored in a sorted order.\np[i] -&gt; k (index). k is the index of an element before the sorting, s.t. i is the index after the sorting. (p is a permutation of the numbers 0, …, N - 1)\napplications:\n\niff a read-only, in-place is not possible\nif multiple arrays have to be sorted in the same way (?)\n\nimplementation\ndef index_sort(a, p):\n  r = [None] * len(a)\n  for i in range(len(a)):\n      r[i] = a[p[i]]\n  return r\np can be obtained by any sorting algorithm, where the key-functoin accesses the original Array\na = [...] # read-only array, to be sorted \np = list(range(len(a))) # indices 0, ..., N - 1\nquick_sort(p, key_function = lambda k: a[k])\nr = index_sort(a, p)",
    "crumbs": [
      "Weekly Summary",
      "Week 11"
    ]
  },
  {
    "objectID": "ss25/alda/sum/w09.html",
    "href": "ss25/alda/sum/w09.html",
    "title": "Week 9",
    "section": "",
    "text": "complexity of the tree-search:\n\nT(tree) = O(number of comparisons)\nnumber of comparisons = length of the path from the root to the target, or “not found”\n\nextreme cases:\n\ndegenerate tree: all nodes have at most one child \\(\\Rightarrow\\) the tree is a linear list \\(\\Rightarrow\\) O(N)\ncomplete tree:\n\nfirst we define depth of a node as the length of the path from the root to the node.\nthen a complete tree is defined as a tree s.t.\n\nevery non-leaf node has two children\nall nodes that have the same depth have the same amount of children\n\\(\\Rightarrow\\) \\(N = 2^d - 1\\), O(d) = O(log(N))\n\n\n\n\n\n\n\ncomplete-tree\n\n\n\nsentinel:\n\nobervation: all invalid (empty) children point to None\nidea: insert None as an actual node, a so-called “sentinel” in the graph:\n\n\n\n\n\nsentinel\n\n\n\nRS-path (root-sentinel path): any path from root to None (sentinel)\n\nm := min(|RS|)\nM := max(|RS|)\n\ngiven these, the definition of a complete tree can be simplified as a tree s.t. m = M\nperfect balanced tree: M &lt;= m + 1, or nodes with less than two children only in the last or forelast levels\n\n\n\n\nperfectly-balanced\n\n\n\nsearch in worst case does M - 1 comparisons which is approx log(N).\nIn order for the tree-search to be efficient, the balance sof the tree should be preserved during the insert / remove operations\nOldest idea: AVL-trees - always pefectly balanced, but this is complicated and actually unneccesary.\nWe weaken the condition of perfectly balanced and instead only require balanced \\(\\Rightarrow\\) Number of comparisons = M - 1 = k * log(N), where k &gt;= 1 is a constant that is independent from N.\n\n\n\n\nRed-Black Trees: standard implementation\nAnderson Trees: simplification of Red-Black Trees, later\nTreap: combination with priority search-trees, i.e. Heaps\n\nHeaps are trees where the the least or largest element are searched.\n\n\n\n\n\nGoal: a simple re-structuring of the tree s.t.\n\nsearch-tree condition is preserved\nbalance is improved (M - m is reduced, or at least doesn’t get larger, because sometimes multiple subsequent rotations are necessary)\n\nTwo types of rotation:\n\nRS-path of the left sub-tree is longer \\(\\Rightarrow\\) right-rotation\nRS-path of the right sub-tree is longer \\(\\Rightarrow\\) left-rotation\n\n\n\n\n\nrotatoin\n\n\n\nnaive idea is the simply assign “5” as the new root, but this won’t always work when new node is inserted, but the core idea is stil correct immediately\nlet’s consider a general situation:\n\n\n\n\ngeneral rotation\n\n\n\nadvantage of rotation: local operation: you only have to ‘know’ the root and its children, not the whole tree\nimplementation:\n\ndef tree_rotate_right(node):\n    new_root = node.left\n    node.left = new_root.right\n    new_root.right = node\n    return new_root # inform Roots parent aboaut change\n\ndef tree_rotate_left(node):\n    new_root = node.right\n    node.right = new_root.left\n    new_root.left = node\n    return new_root\n\nwe haven’t yet implemented when and where the rotation is performed, only how. Various implementations of Red-black Tree address this in different ways. We will come to this later\n\n\n\n\n\ndiffernetiate between horizontal and vertical edges\nonly right children can be connected horizontally\ntwo horizontal edges can follow each other subsequently\n\n\n\n\nanderson-tree\n\n\n\nif we include only the vertical edges wenn calculating the length of the RS-paths then min(|RS|) = max(|RS|), which is virtually perfectly balanced\nthe “actual” depth is at most twice as much as the “virtual” / “effective” one, since at most every 2nd edge horizontal the factor 2 is absorbed in O().",
    "crumbs": [
      "Weekly Summary",
      "Week 9"
    ]
  },
  {
    "objectID": "ss25/alda/sum/w09.html#vl-17---10.06.25",
    "href": "ss25/alda/sum/w09.html#vl-17---10.06.25",
    "title": "Week 9",
    "section": "",
    "text": "complexity of the tree-search:\n\nT(tree) = O(number of comparisons)\nnumber of comparisons = length of the path from the root to the target, or “not found”\n\nextreme cases:\n\ndegenerate tree: all nodes have at most one child \\(\\Rightarrow\\) the tree is a linear list \\(\\Rightarrow\\) O(N)\ncomplete tree:\n\nfirst we define depth of a node as the length of the path from the root to the node.\nthen a complete tree is defined as a tree s.t.\n\nevery non-leaf node has two children\nall nodes that have the same depth have the same amount of children\n\\(\\Rightarrow\\) \\(N = 2^d - 1\\), O(d) = O(log(N))\n\n\n\n\n\n\n\ncomplete-tree\n\n\n\nsentinel:\n\nobervation: all invalid (empty) children point to None\nidea: insert None as an actual node, a so-called “sentinel” in the graph:\n\n\n\n\n\nsentinel\n\n\n\nRS-path (root-sentinel path): any path from root to None (sentinel)\n\nm := min(|RS|)\nM := max(|RS|)\n\ngiven these, the definition of a complete tree can be simplified as a tree s.t. m = M\nperfect balanced tree: M &lt;= m + 1, or nodes with less than two children only in the last or forelast levels\n\n\n\n\nperfectly-balanced\n\n\n\nsearch in worst case does M - 1 comparisons which is approx log(N).\nIn order for the tree-search to be efficient, the balance sof the tree should be preserved during the insert / remove operations\nOldest idea: AVL-trees - always pefectly balanced, but this is complicated and actually unneccesary.\nWe weaken the condition of perfectly balanced and instead only require balanced \\(\\Rightarrow\\) Number of comparisons = M - 1 = k * log(N), where k &gt;= 1 is a constant that is independent from N.\n\n\n\n\nRed-Black Trees: standard implementation\nAnderson Trees: simplification of Red-Black Trees, later\nTreap: combination with priority search-trees, i.e. Heaps\n\nHeaps are trees where the the least or largest element are searched.\n\n\n\n\n\nGoal: a simple re-structuring of the tree s.t.\n\nsearch-tree condition is preserved\nbalance is improved (M - m is reduced, or at least doesn’t get larger, because sometimes multiple subsequent rotations are necessary)\n\nTwo types of rotation:\n\nRS-path of the left sub-tree is longer \\(\\Rightarrow\\) right-rotation\nRS-path of the right sub-tree is longer \\(\\Rightarrow\\) left-rotation\n\n\n\n\n\nrotatoin\n\n\n\nnaive idea is the simply assign “5” as the new root, but this won’t always work when new node is inserted, but the core idea is stil correct immediately\nlet’s consider a general situation:\n\n\n\n\ngeneral rotation\n\n\n\nadvantage of rotation: local operation: you only have to ‘know’ the root and its children, not the whole tree\nimplementation:\n\ndef tree_rotate_right(node):\n    new_root = node.left\n    node.left = new_root.right\n    new_root.right = node\n    return new_root # inform Roots parent aboaut change\n\ndef tree_rotate_left(node):\n    new_root = node.right\n    node.right = new_root.left\n    new_root.left = node\n    return new_root\n\nwe haven’t yet implemented when and where the rotation is performed, only how. Various implementations of Red-black Tree address this in different ways. We will come to this later\n\n\n\n\n\ndiffernetiate between horizontal and vertical edges\nonly right children can be connected horizontally\ntwo horizontal edges can follow each other subsequently\n\n\n\n\nanderson-tree\n\n\n\nif we include only the vertical edges wenn calculating the length of the RS-paths then min(|RS|) = max(|RS|), which is virtually perfectly balanced\nthe “actual” depth is at most twice as much as the “virtual” / “effective” one, since at most every 2nd edge horizontal the factor 2 is absorbed in O().",
    "crumbs": [
      "Weekly Summary",
      "Week 9"
    ]
  },
  {
    "objectID": "ss25/alda/sum/w09.html#vl-18---12.05.25",
    "href": "ss25/alda/sum/w09.html#vl-18---12.05.25",
    "title": "Week 9",
    "section": "VL 18 - 12.05.25",
    "text": "VL 18 - 12.05.25\n\nrevision of: complete tree, perfectly balanced tree, balanced tree.\n\n\nSelf Balancing Trees\nprinciple:\n\nIn ‘forward’ (form root towards the leaves) recursive run insert the elements the usual way, i.e. same as with unbalanced trees\nOn the way back we repair the balance.\n\nAnderson Trees solve this problem especially simply and elegantly:\n\neach node stores additional information about the balance: the max distance to None = length of the max RS-path\nimplementation\nclass AndersonNode:\n  def __init__(self, key, value):\n    self.key = key\n    self.value = value\n    self.left = self.right = None\n    self.dist = 2\n\nBedingungen fuer gueltigen Anderson-Baum\n\nSearch-tree Condition\nThere are vertical edges (distance of a node = distance of the child + 1)\nThere are horizontal edges (distance of a node = distance of a child)\nOnly right children are allowed to be horizontal (if node.left != None then `dist = node.left.dist + 1)\nsubsequnet edges are never allowed to be horizontal:\nif node.right != None and node.right.right != None then node.dist &gt;= node.right.right.dist + 1\n\n\\(\\Rightarrow\\) repair balance s.t. 0) is always satisfied by using rotations, 1-3 will then hold again\nImplementation:\ndef anderson_insert(node, key, value):\n  if node is None: return AndersonNode(key, value)\n  if key == node.key: \n    node.value = value\n    return node\n  if key &lt; node.key:\n    node.left = anderson_insert(node.left, key, value)\n  else: \n    node.right = anderson_insert_node.right, key, value)\n  # balance condition can possibly be violated\n  if node.left is not None and nodeist == node.left.dist: # horizontal after left is not allowed\n    # transform into horizontal right\n    note = tree_rotate_right(node)\n  if node.right is not None and node.right.right is not None and node.ist == node.right.right.dist:\n    # two subsequent right horizontal =&gt; right rotation and lift up\n    node = tree_rotate_left(node)\n    node.dist += 1\n  return node\nExamples: To differentiate from the dist, keys are letters.\n\n\n\n\nanderson insert a\n\n\n\n\n\nanderson insert c\n\n\n\n\nComplexity of anderson_insert()\n\nall instructions other than the recursion are O(1) if the tree is balanced, then the right and left recursion is equally expensive on average\nthe depth of an anderson tree (also of every sub-tree) is at most twice as much as an equally large perfectly balanced tree\n\nwithout proof: if all horizontally connected nodes are united into one, this results in a perfectly balanced tree",
    "crumbs": [
      "Weekly Summary",
      "Week 9"
    ]
  },
  {
    "objectID": "ss25/alda/sum/w07.html",
    "href": "ss25/alda/sum/w07.html",
    "title": "Week 7",
    "section": "",
    "text": "we had running_mean(a, k) in O(k * N) when k = const independent from N.\nConsider two implementations, one of them uses an inappropriate data structure, and the other one a better one\n\n:\n\nr = [0] * len(a)\nfor j in range(k - 1, len(a)):\n  for i in range(j - k + 1, j + 1):\n      r[j] = r[j] + get_item(a, i) # get_item(a, i) = O(i)\n  r[j] /= k\nreturn r\ninner loop: \\(\\mathcal{O}(\\Sigma_{i = r - j + k + 1}^{i = j + 1}(i) = \\mathcal{O}(k\\cdot j - k^2 / 2)) = \\mathcal{O}(k\\cdot j)\\) (k is constant) outer loop: calculating the outer loop we get … this is the real\n\nwhen shifting the window there are many elements overlapping elements - recalculating the sum of each window is inefficient. Having calculated the sum of a single window, the next shift is simply obtained by removing the first element from the sum, and adding the first subsequent element outside of the window to the sum:\nr = [0] * len(a)\nfor i range(k): # O(k)\n   r[k - i] += a[i] \n  for j in range(k, len(a)):\n   r[j] = r[j - 1] - a[j - k] + a[j]\n  for j in range(k - 1, len(a)):\n   r[j] /= k\n  return r\n\nThis algorithm is O(N). Slight improvement reducerd the complexity.\nWhich data structure is here inappropriate? \\(\\Rightarrow\\) linked list\n\n\n\n\n\n\na linked or doubliy linked list consists of:\nclass Node:\n    value # data element\n    next # reference to another node object\n\n\n\n\nlinked-list\n\n\n\n\n\ndoubly linked list\n\n\n\n\n\nadding an element\n\n\n\nremoving an element from or adding to a linked list at an arbitrary known position is \\(\\mathbb{O}(1)\\)\ndef push_front(l, v):\n    n = Node()\n    n.value = v\n    n.next = l.start\n    l.start = n\n\nlinked list is good for inserting or removing an element at an arbitrary location\nObservation:\n\nin practice it is very often sufficient to simply add elements to the end or remove from the end, reducing the importance of linked list (after the element is added at the end of the array the array can be sorted if the order is important, instead of inserting at an arbitrary location)\nadding an element to the end of an array can be done efficiently in amortized time using dynamic arrays\n\n\n\n\n\n\nWhen appending an element to a full array, a new array with larger capacity is allocated an previous contents are copied over to the new array \\(\\Rightarrow\\) key point. The new capacity must be a multiple of old capacity. (if it is some constant the amortzied time won’t work)\ncopying is \\(\\mathcal{O}(N)\\) but when capacity is doubled or increased depending on \\(N\\), this happens rarely. More precisely if the array is full, the first insert costs N. The subsequent N insert each cost 1. Thus in total N + 1 inserts cost 2N time, averaging 2 per insert. (amortized).\n\nsometimes cheap: \\(\\mathcal{O}(1)\\)\nsometimes expensive: \\(\\mathcal{O}(N)\\)\n\n\\(\\Rightarrow\\) amortized complexity. “amortized” comes from industry or econonmics. When an expensive machine is first purchased, or a large inverstment is initially made, there is first a loss. But over time the profit of the products of the machine or the investments covers (amortizes) the initial loss.\nformally calculation with the “accounting mehtod”:\n\ndynamic array data structure carries around the infomration:\n\nsize: amount of elements in the array\ncapacity: the total number of elements that can be stored in the array\n\nlet at a given time \\(i\\) these values be size_i and capacity_i\ninvariant of the data structure: capacity_i &gt;= size_i\nDefine phi_i := size_i - capacity_i\nThen the costs of an append is: c_i = c~i + (phi_i - phi(i+1))\n\ncase 1: there is free storage \\(\\Rightarrow\\) size_(i - 1) &lt; capacity_(i - 1), capacity_i = capacity_(i - 1), size_i = size_(i - 1) + 1.\nc_i = 1 + (size_i - capacity_i) - (size_(i - 1) - capacity_(i - 1)) = 1 + 1 + 2\ncase 2: capacity is full \\(\\Rightarrow\\) double the capacity, and copy over the elements. size_i-1 = capacity_i-1, size_i = size_i-1 + 1.\nc_i~ = 1 + size_i-1, copying the elements and appending the new element\nc_i = 1 + size_i-1 + (size_i - capacity_i) - (size_i-1 - capacity_i - 1) = 1 + 1 + size_i-1 - capacity_i-1\nsimplifying we obtain",
    "crumbs": [
      "Weekly Summary",
      "Week 7"
    ]
  },
  {
    "objectID": "ss25/alda/sum/w07.html#vl-13---27.05.25",
    "href": "ss25/alda/sum/w07.html#vl-13---27.05.25",
    "title": "Week 7",
    "section": "",
    "text": "we had running_mean(a, k) in O(k * N) when k = const independent from N.\nConsider two implementations, one of them uses an inappropriate data structure, and the other one a better one\n\n:\n\nr = [0] * len(a)\nfor j in range(k - 1, len(a)):\n  for i in range(j - k + 1, j + 1):\n      r[j] = r[j] + get_item(a, i) # get_item(a, i) = O(i)\n  r[j] /= k\nreturn r\ninner loop: \\(\\mathcal{O}(\\Sigma_{i = r - j + k + 1}^{i = j + 1}(i) = \\mathcal{O}(k\\cdot j - k^2 / 2)) = \\mathcal{O}(k\\cdot j)\\) (k is constant) outer loop: calculating the outer loop we get … this is the real\n\nwhen shifting the window there are many elements overlapping elements - recalculating the sum of each window is inefficient. Having calculated the sum of a single window, the next shift is simply obtained by removing the first element from the sum, and adding the first subsequent element outside of the window to the sum:\nr = [0] * len(a)\nfor i range(k): # O(k)\n   r[k - i] += a[i] \n  for j in range(k, len(a)):\n   r[j] = r[j - 1] - a[j - k] + a[j]\n  for j in range(k - 1, len(a)):\n   r[j] /= k\n  return r\n\nThis algorithm is O(N). Slight improvement reducerd the complexity.\nWhich data structure is here inappropriate? \\(\\Rightarrow\\) linked list\n\n\n\n\n\n\na linked or doubliy linked list consists of:\nclass Node:\n    value # data element\n    next # reference to another node object\n\n\n\n\nlinked-list\n\n\n\n\n\ndoubly linked list\n\n\n\n\n\nadding an element\n\n\n\nremoving an element from or adding to a linked list at an arbitrary known position is \\(\\mathbb{O}(1)\\)\ndef push_front(l, v):\n    n = Node()\n    n.value = v\n    n.next = l.start\n    l.start = n\n\nlinked list is good for inserting or removing an element at an arbitrary location\nObservation:\n\nin practice it is very often sufficient to simply add elements to the end or remove from the end, reducing the importance of linked list (after the element is added at the end of the array the array can be sorted if the order is important, instead of inserting at an arbitrary location)\nadding an element to the end of an array can be done efficiently in amortized time using dynamic arrays\n\n\n\n\n\n\nWhen appending an element to a full array, a new array with larger capacity is allocated an previous contents are copied over to the new array \\(\\Rightarrow\\) key point. The new capacity must be a multiple of old capacity. (if it is some constant the amortzied time won’t work)\ncopying is \\(\\mathcal{O}(N)\\) but when capacity is doubled or increased depending on \\(N\\), this happens rarely. More precisely if the array is full, the first insert costs N. The subsequent N insert each cost 1. Thus in total N + 1 inserts cost 2N time, averaging 2 per insert. (amortized).\n\nsometimes cheap: \\(\\mathcal{O}(1)\\)\nsometimes expensive: \\(\\mathcal{O}(N)\\)\n\n\\(\\Rightarrow\\) amortized complexity. “amortized” comes from industry or econonmics. When an expensive machine is first purchased, or a large inverstment is initially made, there is first a loss. But over time the profit of the products of the machine or the investments covers (amortizes) the initial loss.\nformally calculation with the “accounting mehtod”:\n\ndynamic array data structure carries around the infomration:\n\nsize: amount of elements in the array\ncapacity: the total number of elements that can be stored in the array\n\nlet at a given time \\(i\\) these values be size_i and capacity_i\ninvariant of the data structure: capacity_i &gt;= size_i\nDefine phi_i := size_i - capacity_i\nThen the costs of an append is: c_i = c~i + (phi_i - phi(i+1))\n\ncase 1: there is free storage \\(\\Rightarrow\\) size_(i - 1) &lt; capacity_(i - 1), capacity_i = capacity_(i - 1), size_i = size_(i - 1) + 1.\nc_i = 1 + (size_i - capacity_i) - (size_(i - 1) - capacity_(i - 1)) = 1 + 1 + 2\ncase 2: capacity is full \\(\\Rightarrow\\) double the capacity, and copy over the elements. size_i-1 = capacity_i-1, size_i = size_i-1 + 1.\nc_i~ = 1 + size_i-1, copying the elements and appending the new element\nc_i = 1 + size_i-1 + (size_i - capacity_i) - (size_i-1 - capacity_i - 1) = 1 + 1 + size_i-1 - capacity_i-1\nsimplifying we obtain",
    "crumbs": [
      "Weekly Summary",
      "Week 7"
    ]
  },
  {
    "objectID": "ss25/alda/sum/w05.html",
    "href": "ss25/alda/sum/w05.html",
    "title": "Week 5",
    "section": "",
    "text": "review of quick sort\n\n\n\n\n\nchoose Pivot randomly \\(\\Rightarrow\\) k can be any index in [l, r] with the same probability\ngood case - k ends up in the middle:\n\nsub arrays for the recursive calls have both size \\(\\frac{r - l}{2}\\) \\(\\Rightarrow\\) we profit from divide and conquer\n\nbad case - k ends up on the edge (k == l of k == r) \\(\\Rightarrow\\) Recursion is not advantagous, we do not profit from D&C.\ntypical case: typical case is neither the best, nor the best case - it’s the average of those, i.e. k is somewhere between being exactly in the middle or exactly on the edges - what happens here ?\n\nBefore let’s determine the run time of the best and the worst cases.\nFor recursive algorithms the following genera run time formula holds:\n\\[L_{\\text{total}}(N) = L_{\\text{local}}(N) + L_{\\text{recursive}}(N)\\]\nfor quicksort \\(L_{\\text{local}}(N) = L_{\\text{parition}}(N)\\).\n\n\n\\[\\begin{align*}\nL(N) &= (N + 1) + &&\\text{// partition} \\\\\n     &\\quad\\;\\; L(0) + L(N - 1) &&\\text{// recursive calls} \\\\\n     &= N + 1 + 0 + L(N - 1) \\quad &&\\text{// since } L(0) = 0 \\text{ and } L(1) = 0 \\\\\n     &= (N + 1) + L(N - 1) \\\\\n     &= (N + 1) + (N) + L(N - 2) \\\\\n     &\\;\\;\\vdots \\\\\n     &= (N + 1) + N + \\cdots + 2 + 1 \\\\\n     &= \\sum_{k = 1}^{N + 1} k \\\\\n     &= \\frac{(N + 1)(N + 2)}{2} \\\\\n     &\\in \\Theta(N^2)\n\\end{align*}\\]\n\n\n\nChoose N, s.t. both sub arrays are of the same size: N in {1, 3, 7, 15, …, 2^n - 1, …}. Then:\n\\[\\begin{align*}\nL(2^M - 1) &= 2^M &&\\text{partition} \\\\\n           &\\quad +\\; 2 \\cdot L(2^{M - 1} - 1) &&\\text{recursion on two halves} \\\\\n           &= 2^M + 2 \\cdot 2^{M - 1} + 2^2 \\cdot 2^{M - 2} + \\cdots + 2^{M - 1} \\cdot 2 &&\\text{expanding recursively} \\\\\n           &= \\sum_{k = 1}^{M - 1} 2^k \\cdot 2^{M - k} &&\\text{collecting all terms} \\\\\n           &= (M - 1) \\cdot 2^M &&\\text{each term simplifies to } 2^M \\\\\n           &= \\log_2(N + 1) \\cdot (N + 1) &&\\text{since } N = 2^M - 1 \\Rightarrow M = \\log_2(N + 1) \\\\\n           &\\in \\Theta(N \\log N) &&\\text{best-case runtime}\n\\end{align*}\\]\n\n\n\nThe average case of Quicksort involves a recurrence that doesn’t simplify as neatly as the worst and best cases because it includes a weighted sum over all possible pivot positions.\nThe key recurrence for the expected number of comparisons in Quicksort is:\n\\[\nL(N) = N + 1 + \\frac{1}{N} \\sum_{k=0}^{N - 1} \\left( L(k) + L(N - 1 - k) \\right)\n\\]\nThis models the expected cost when the pivot lands uniformly at random. By symmetry and change of variables, it simplifies to:\n\\[\nL(N) = N + 1 + \\frac{2}{N} \\sum_{k=0}^{N - 1} L(k)\n\\]\nThat leads to a recurrence for \\(L(N)\\) involving cumulative sums, and can be shown (via substitution or using generating functions) that the solution satisfies:\n\\[\nL(N) \\in \\Theta(N \\log N)\n\\]\nDerivation:\n\\[\\begin{align*}\nL(N) &= N + 1 &&\\text{partition step: compares } N \\text{ elements to pivot} \\\\\n     &\\quad + \\frac{1}{N} \\sum_{k=0}^{N-1} \\left( L(k) + L(N - 1 - k) \\right) &&\\text{average over all pivot positions} \\\\\n     &= N + 1 + \\frac{2}{N} \\sum_{k=0}^{N-1} L(k) &&\\text{by symmetry: } L(k) = L(N - 1 - k) \\\\\n     \\Rightarrow N L(N) &= N(N + 1) + 2 \\sum_{k=0}^{N-1} L(k) &&\\text{multiply both sides by } N \\\\\n     \\text{Let } A(N) &= \\sum_{k=0}^{N} L(k) &&\\text{define cumulative sum} \\\\\n     \\Rightarrow N L(N) &= N(N + 1) + 2 A(N - 1) \\\\\n     \\Rightarrow L(N) &= (N + 1) + \\frac{2}{N} A(N - 1) \\\\\n     \\text{Now guess: } L(N) &\\in \\Theta(N \\log N) &&\\text{solve via master theorem or induction}\n\\end{align*}\\]\nAlternatively:\nYour professor is summarizing a classic analysis of the average-case comparisons in Quicksort using a more elegant expression derived from solving the recurrence via indicator random variables or expected value over pivot positions.\n\nKey Idea:\n\nLet \\(L(N)\\) be the expected number of comparisons when sorting \\(N\\) elements with Quicksort.\nEach pair \\((i, j)\\) is compared at most once, and only if one of them is chosen as a pivot before any of the elements between them.\n\nProbability Argument:\n\nFor any two distinct elements \\(a_i\\) and \\(a_j\\), the probability that they are compared is \\(\\frac{2}{|j - i| + 1}\\).\nThis is because the first pivot chosen among \\(a_i, a_{i+1}, ..., a_j\\) must be either \\(a_i\\) or \\(a_j\\) for them to be directly compared.\n\nSumming Over All Pairs:\n\nThere are \\(\\binom{N+1}{2}\\) pairs, and the expected cost is:\n\\[\nL(N) = \\sum_{1 \\le i &lt; j \\le N+1} \\frac{2}{j - i + 1}\n\\]\nThis sum can be manipulated into the form:\n\\[\nL(N) = 2(N+1) \\sum_{k=2}^{N+1} \\frac{1}{k}\n= 2(N+1) \\left( \\sum_{k=1}^{N+1} \\frac{1}{k} - 1 \\right)\n\\]\n\nHarmonic Approximation:\n\nThe harmonic sum \\(H_{N+1} = \\sum_{k=1}^{N+1} \\frac{1}{k} \\approx \\ln(N+1) + \\gamma\\)\nSo:\n\\[\nL(N) \\approx 2(N+1)(\\ln(N+1) + \\gamma - 1) \\in \\Theta(N \\log N)\n\\]\n\n\n\\[\\begin{align*}\nL(N) &= \\sum_{1 \\le i &lt; j \\le N+1} \\mathbb{P}(\\text{elements } i \\text{ and } j \\text{ are compared}) \\\\\n     &= \\sum_{1 \\le i &lt; j \\le N+1} \\frac{2}{j - i + 1} \\\\\n     &= 2 \\sum_{d=1}^{N} (N + 1 - d) \\cdot \\frac{1}{d + 1} &&\\text{substitute } d = j - i \\\\\n     &= 2(N+1) \\sum_{k=2}^{N+1} \\frac{1}{k} \\\\\n     &= 2(N+1) \\left( \\sum_{k=1}^{N+1} \\frac{1}{k} - 1 \\right) \\\\\n     &= 2(N+1) \\left( H_{N+1} - 1 \\right) &&\\text{where } H_n = \\sum_{k=1}^{n} \\frac{1}{k} \\\\\n     &\\approx 2(N+1) \\left( \\ln(N+1) + \\gamma - 1 \\right) \\\\\n     &\\in \\Theta(N \\log N)\n\\end{align*}\\]\n\n\n\n\n\nif an algorithm is not correct, all other properties (elegance, efficiency) are irrelevant. Correctness is therefore a fundamental concept\ntwo aspecs:\n\nspecification: what not how\nimplementation: how\n\n\n\\(\\Rightarrow\\)\n\nvalidation: Deos the specification really satisfy our goal?\n\nverification: Does the algorithms really implement the specification?\n\nNothing prevents conceptually the specification to be probability based. I.e. the correct solution is given as a probability and not as a precise value \\(\\Rightarrow\\) e.g. Neural networks\n\nproblem: how are randomized algorithms tested? (open problem)\n\n\n\n\nFormal proof of Correctness (Royal way - but not practical)\nAssistance by the programming language itself\n\nProgramming language reports errors when something wrong is done\nProgramming language prevents from doing errors (strongly typed languages)\n\nSystematic software testing\n\nin python the module “unittest”\n\n\n\n\n\ntwo approaches:\n\nOn the basis of pseudo-code (does the algorithm do the the correct thing in principle, typically in literature)\nOn the basis of the implementation (very complicated, usually only applied for critical software)\n\nbank software\nparts of the operating systems in aircraft \\(\\Rightarrow\\) automated theorem prover.",
    "crumbs": [
      "Weekly Summary",
      "Week 5"
    ]
  },
  {
    "objectID": "ss25/alda/sum/w05.html#vl-9---13.05.25",
    "href": "ss25/alda/sum/w05.html#vl-9---13.05.25",
    "title": "Week 5",
    "section": "",
    "text": "review of quick sort\n\n\n\n\n\nchoose Pivot randomly \\(\\Rightarrow\\) k can be any index in [l, r] with the same probability\ngood case - k ends up in the middle:\n\nsub arrays for the recursive calls have both size \\(\\frac{r - l}{2}\\) \\(\\Rightarrow\\) we profit from divide and conquer\n\nbad case - k ends up on the edge (k == l of k == r) \\(\\Rightarrow\\) Recursion is not advantagous, we do not profit from D&C.\ntypical case: typical case is neither the best, nor the best case - it’s the average of those, i.e. k is somewhere between being exactly in the middle or exactly on the edges - what happens here ?\n\nBefore let’s determine the run time of the best and the worst cases.\nFor recursive algorithms the following genera run time formula holds:\n\\[L_{\\text{total}}(N) = L_{\\text{local}}(N) + L_{\\text{recursive}}(N)\\]\nfor quicksort \\(L_{\\text{local}}(N) = L_{\\text{parition}}(N)\\).\n\n\n\\[\\begin{align*}\nL(N) &= (N + 1) + &&\\text{// partition} \\\\\n     &\\quad\\;\\; L(0) + L(N - 1) &&\\text{// recursive calls} \\\\\n     &= N + 1 + 0 + L(N - 1) \\quad &&\\text{// since } L(0) = 0 \\text{ and } L(1) = 0 \\\\\n     &= (N + 1) + L(N - 1) \\\\\n     &= (N + 1) + (N) + L(N - 2) \\\\\n     &\\;\\;\\vdots \\\\\n     &= (N + 1) + N + \\cdots + 2 + 1 \\\\\n     &= \\sum_{k = 1}^{N + 1} k \\\\\n     &= \\frac{(N + 1)(N + 2)}{2} \\\\\n     &\\in \\Theta(N^2)\n\\end{align*}\\]\n\n\n\nChoose N, s.t. both sub arrays are of the same size: N in {1, 3, 7, 15, …, 2^n - 1, …}. Then:\n\\[\\begin{align*}\nL(2^M - 1) &= 2^M &&\\text{partition} \\\\\n           &\\quad +\\; 2 \\cdot L(2^{M - 1} - 1) &&\\text{recursion on two halves} \\\\\n           &= 2^M + 2 \\cdot 2^{M - 1} + 2^2 \\cdot 2^{M - 2} + \\cdots + 2^{M - 1} \\cdot 2 &&\\text{expanding recursively} \\\\\n           &= \\sum_{k = 1}^{M - 1} 2^k \\cdot 2^{M - k} &&\\text{collecting all terms} \\\\\n           &= (M - 1) \\cdot 2^M &&\\text{each term simplifies to } 2^M \\\\\n           &= \\log_2(N + 1) \\cdot (N + 1) &&\\text{since } N = 2^M - 1 \\Rightarrow M = \\log_2(N + 1) \\\\\n           &\\in \\Theta(N \\log N) &&\\text{best-case runtime}\n\\end{align*}\\]\n\n\n\nThe average case of Quicksort involves a recurrence that doesn’t simplify as neatly as the worst and best cases because it includes a weighted sum over all possible pivot positions.\nThe key recurrence for the expected number of comparisons in Quicksort is:\n\\[\nL(N) = N + 1 + \\frac{1}{N} \\sum_{k=0}^{N - 1} \\left( L(k) + L(N - 1 - k) \\right)\n\\]\nThis models the expected cost when the pivot lands uniformly at random. By symmetry and change of variables, it simplifies to:\n\\[\nL(N) = N + 1 + \\frac{2}{N} \\sum_{k=0}^{N - 1} L(k)\n\\]\nThat leads to a recurrence for \\(L(N)\\) involving cumulative sums, and can be shown (via substitution or using generating functions) that the solution satisfies:\n\\[\nL(N) \\in \\Theta(N \\log N)\n\\]\nDerivation:\n\\[\\begin{align*}\nL(N) &= N + 1 &&\\text{partition step: compares } N \\text{ elements to pivot} \\\\\n     &\\quad + \\frac{1}{N} \\sum_{k=0}^{N-1} \\left( L(k) + L(N - 1 - k) \\right) &&\\text{average over all pivot positions} \\\\\n     &= N + 1 + \\frac{2}{N} \\sum_{k=0}^{N-1} L(k) &&\\text{by symmetry: } L(k) = L(N - 1 - k) \\\\\n     \\Rightarrow N L(N) &= N(N + 1) + 2 \\sum_{k=0}^{N-1} L(k) &&\\text{multiply both sides by } N \\\\\n     \\text{Let } A(N) &= \\sum_{k=0}^{N} L(k) &&\\text{define cumulative sum} \\\\\n     \\Rightarrow N L(N) &= N(N + 1) + 2 A(N - 1) \\\\\n     \\Rightarrow L(N) &= (N + 1) + \\frac{2}{N} A(N - 1) \\\\\n     \\text{Now guess: } L(N) &\\in \\Theta(N \\log N) &&\\text{solve via master theorem or induction}\n\\end{align*}\\]\nAlternatively:\nYour professor is summarizing a classic analysis of the average-case comparisons in Quicksort using a more elegant expression derived from solving the recurrence via indicator random variables or expected value over pivot positions.\n\nKey Idea:\n\nLet \\(L(N)\\) be the expected number of comparisons when sorting \\(N\\) elements with Quicksort.\nEach pair \\((i, j)\\) is compared at most once, and only if one of them is chosen as a pivot before any of the elements between them.\n\nProbability Argument:\n\nFor any two distinct elements \\(a_i\\) and \\(a_j\\), the probability that they are compared is \\(\\frac{2}{|j - i| + 1}\\).\nThis is because the first pivot chosen among \\(a_i, a_{i+1}, ..., a_j\\) must be either \\(a_i\\) or \\(a_j\\) for them to be directly compared.\n\nSumming Over All Pairs:\n\nThere are \\(\\binom{N+1}{2}\\) pairs, and the expected cost is:\n\\[\nL(N) = \\sum_{1 \\le i &lt; j \\le N+1} \\frac{2}{j - i + 1}\n\\]\nThis sum can be manipulated into the form:\n\\[\nL(N) = 2(N+1) \\sum_{k=2}^{N+1} \\frac{1}{k}\n= 2(N+1) \\left( \\sum_{k=1}^{N+1} \\frac{1}{k} - 1 \\right)\n\\]\n\nHarmonic Approximation:\n\nThe harmonic sum \\(H_{N+1} = \\sum_{k=1}^{N+1} \\frac{1}{k} \\approx \\ln(N+1) + \\gamma\\)\nSo:\n\\[\nL(N) \\approx 2(N+1)(\\ln(N+1) + \\gamma - 1) \\in \\Theta(N \\log N)\n\\]\n\n\n\\[\\begin{align*}\nL(N) &= \\sum_{1 \\le i &lt; j \\le N+1} \\mathbb{P}(\\text{elements } i \\text{ and } j \\text{ are compared}) \\\\\n     &= \\sum_{1 \\le i &lt; j \\le N+1} \\frac{2}{j - i + 1} \\\\\n     &= 2 \\sum_{d=1}^{N} (N + 1 - d) \\cdot \\frac{1}{d + 1} &&\\text{substitute } d = j - i \\\\\n     &= 2(N+1) \\sum_{k=2}^{N+1} \\frac{1}{k} \\\\\n     &= 2(N+1) \\left( \\sum_{k=1}^{N+1} \\frac{1}{k} - 1 \\right) \\\\\n     &= 2(N+1) \\left( H_{N+1} - 1 \\right) &&\\text{where } H_n = \\sum_{k=1}^{n} \\frac{1}{k} \\\\\n     &\\approx 2(N+1) \\left( \\ln(N+1) + \\gamma - 1 \\right) \\\\\n     &\\in \\Theta(N \\log N)\n\\end{align*}\\]\n\n\n\n\n\nif an algorithm is not correct, all other properties (elegance, efficiency) are irrelevant. Correctness is therefore a fundamental concept\ntwo aspecs:\n\nspecification: what not how\nimplementation: how\n\n\n\\(\\Rightarrow\\)\n\nvalidation: Deos the specification really satisfy our goal?\n\nverification: Does the algorithms really implement the specification?\n\nNothing prevents conceptually the specification to be probability based. I.e. the correct solution is given as a probability and not as a precise value \\(\\Rightarrow\\) e.g. Neural networks\n\nproblem: how are randomized algorithms tested? (open problem)\n\n\n\n\nFormal proof of Correctness (Royal way - but not practical)\nAssistance by the programming language itself\n\nProgramming language reports errors when something wrong is done\nProgramming language prevents from doing errors (strongly typed languages)\n\nSystematic software testing\n\nin python the module “unittest”\n\n\n\n\n\ntwo approaches:\n\nOn the basis of pseudo-code (does the algorithm do the the correct thing in principle, typically in literature)\nOn the basis of the implementation (very complicated, usually only applied for critical software)\n\nbank software\nparts of the operating systems in aircraft \\(\\Rightarrow\\) automated theorem prover.",
    "crumbs": [
      "Weekly Summary",
      "Week 5"
    ]
  },
  {
    "objectID": "ss25/alda/sum/w05.html#vl-10---15.05.25",
    "href": "ss25/alda/sum/w05.html#vl-10---15.05.25",
    "title": "Week 5",
    "section": "VL 10 - 15.05.25",
    "text": "VL 10 - 15.05.25\n\nCorrectness (Cont.)\n\nCorrectness on the level of pseudocode:\n\nuses Axioms:\n\nPre- and Postconditions\nLoop invariants \\(\\Rightarrow\\) Induction proof\n\nexample1: finding the minimum element of an array\ndef min(a):\n  N = len(a)\n  m = 0\n  # i == 0\n  # invariant: a[m] == min(a[0..i])\n  for i in range(N-1):\n    if a[i + 1] &lt; a[m]: m = i + 1\n  # post: i == N - 1\n  return a[m]\nexample 2: selection sort\ndef selection_sort(a):\n  N = len(a)\n  # i == 0\n  # invariant: a[0..i-1] &lt;= a[i..N-1] and sorted(a[0..i-1]) and i &lt;= N\n  while i in range(N - 1): \n    m = i\n    # k == i\n    # invariant: a[m] == min(a[i..k])\n    for k in range(i, N-1):\n      if a[k+1] &lt; a[m]: m = k + 1\n    # post: k == N - 1\n    a[m], a[i] = a[i], a[m]\n  # post: i == N - 1\naxioms:\n\nPre: -\nPost:\n\nlen(a’) == len(a)\na’ has the same elements as a\na’ is sorted, i.e. a[i - 1] &lt;= a[i], for all i in [1..N] …\n\n\n\n\n\nCorrectness on the Basis of Software Testing\nExample for Testing, babylonian root method for finding square root of a number\ndef sqrt(x):\n  if x &lt; 0:\n    raise ValueError(\"sqrt(x) with x &lt; 0 not allowed\")\n  y = x // 2 # bug floor division\n  while y * y != x:\n    y = (y + x / 2) / 2\n  return y\nWe use the unit test module:\nclass SqrtTest(unittest.TestCase):\n  def testSqrt(Self): # in this function we write the tests\n    self.assertRaises(ValueError, sqrt, -1)\n    self.assertEqual(sqrt(9), 3)\n    self.assertEqual(sqrt(1), 1) # bud =&gt; y = x / 2 instead of y = x // 2\n    self.assertEqual(sqrt(1.21)**2, 1.21) # bug because the function never finishes; \n                                          # infinite loop due to while loop condition\n                                          # fix: while abs(y**2 / x - 1) &gt; 1e-15 (relative error)\n                                          # but the error is still there\n                                          # we need a new test that accepts a tolarance\n    self.assertAlmostEqual(sqrt(1.21)**2, 1.21)\n    self.assertEqual(sqrt(4), 2)\n    self.assertEqual(sqrt(0), 0) # bug because we divide by zero in the while condition\n                                 # fix: while abs(x - y**2) &gt; 1e-15",
    "crumbs": [
      "Weekly Summary",
      "Week 5"
    ]
  },
  {
    "objectID": "ss25/alda/sum/w03.html",
    "href": "ss25/alda/sum/w03.html",
    "title": "Week 3",
    "section": "",
    "text": "Review of previous lecture; containers: arrays, stack, dictionary\nStack example: redo / undo stacks: stack. Each time an edit is made, it is pushed on the undo stack.\nWhen undo is called, the top of the undo stack is undone, popped, and pushed on the redo stack. When redo is called the converse happens. (What happens when we do not call redo immediately after undo?)\n\n\n\n\nQueue:\n\nOperation: initialization: q = queue(), in python simply a list.\nOperation: top()\npython: s[0] since the top element is the first of the list, and the last as with stacks.\nOperation: pop()\npython: s.pop(0) , since the first element of the queue is the first element of the list. This is not efficient in python, though.\nOperation: push(s, v)\npython: s.append(v)\n\n\n\n\ndouble-ended queue:\n\nefficient both as a stack and as a queue. Remember that pop operation is not efficient for a queue (when list is treated as a queu)\nin python: in the collections module.\nimport collections\nd = collections.deque()\noperations:\n\nd.push(v): push at the end\nd.pop_front() pop first element efficiently\nd.pop_back() pop last element efficienty\n\n\n\n\n\n??\n\n\n\n\n\nMany algorithms work only, (or much faster) on sorted data\nSorting algorihtms are very good example for algorithmic thinking and algorihtm analysis\n\nalgorithmic thinking: in math there are often “existence” proofs, that say that an object exists, but do not prescribe a method to construct such an object. Algorithmic thinking is on the other hand always constructive in finite steps. Usually based on elementary operations. Using iteration and recursion, and principles of divide and conquer.\n\nNote: learning basic algorithms is valuable, but we should very rarely have to implement them from scratch and use library functions instead. (better implemented and more thoroughly tested)\nrules of the game:\n\nunterliegende Containerdatenstrukturen: Array (python list) \\(\\Rightarrow\\) allowed operations: len(a), v = a[i], a[i] = v s.t. i in [0..len(a) - 1]\nThe elements that are to be sorted can be compared: a[i] &lt; a[j] or a[i] &lt;= a[j], elements are totally ordered.\ntotal order:\n\nall pairs of elements can be compared.\nanti-symmetric: a &lt;= b and b &lt;= a ==&gt; a == b (or equivalently a &lt;= b and b =/= a then not b &lt;= a)\ntransitiv: a &lt;= b, b &lt;= c ==&gt; a &lt;= c\nreflexive: a &lt;= a, for all a.\n\n\npractical problem: The elements support u &lt;= v, but the algorithm requires u &lt; v. How to avert this problem?\na &lt; b &lt;=&gt; not(b &lt;= a)\ntheorem: &lt;, &gt;, &gt;=, ==, != all can be implemented with &lt;=, not, and\n\na &lt; b iff not (b &lt;= a)\na &gt; b iff b &lt; a\na &gt;= b iff b &lt;= a\na == b iff a &lt;= b and b &lt;= a\na != b iff not (a == b)\n\n\n\n\nprinciple:\n\nsorintg from left to write (left to the current position is all sorted)\nselect the appropriate element in the right-side of the current position.\n\n\ndef selection_sort(a) :\n    N = len(a)\n    # invariant: sorted(a[0..i-1]) and a[0..i-1] &lt;= a[i..N-1]\n    for i in range(N - 1) :  # iteration\n        j = i \n        # k = i + 1\n        # invariant: a[j] == min(a[i..k-1])\n        for k in range(i + 1, N) : \n            if a[k] &lt; a[j] : j = k\n        a[i], a[j] = a[j], a[i]\n\na = [3, -1, 5, 10]\nselection_sort(a)\nprint(a)\n\n[-1, 3, 5, 10]\n\n\n\n\nHow many steps does selection sort take?\n\nobviously depends on N.",
    "crumbs": [
      "Weekly Summary",
      "Week 3"
    ]
  },
  {
    "objectID": "ss25/alda/sum/w03.html#vl-5---29.04.25",
    "href": "ss25/alda/sum/w03.html#vl-5---29.04.25",
    "title": "Week 3",
    "section": "",
    "text": "Review of previous lecture; containers: arrays, stack, dictionary\nStack example: redo / undo stacks: stack. Each time an edit is made, it is pushed on the undo stack.\nWhen undo is called, the top of the undo stack is undone, popped, and pushed on the redo stack. When redo is called the converse happens. (What happens when we do not call redo immediately after undo?)\n\n\n\n\nQueue:\n\nOperation: initialization: q = queue(), in python simply a list.\nOperation: top()\npython: s[0] since the top element is the first of the list, and the last as with stacks.\nOperation: pop()\npython: s.pop(0) , since the first element of the queue is the first element of the list. This is not efficient in python, though.\nOperation: push(s, v)\npython: s.append(v)\n\n\n\n\ndouble-ended queue:\n\nefficient both as a stack and as a queue. Remember that pop operation is not efficient for a queue (when list is treated as a queu)\nin python: in the collections module.\nimport collections\nd = collections.deque()\noperations:\n\nd.push(v): push at the end\nd.pop_front() pop first element efficiently\nd.pop_back() pop last element efficienty\n\n\n\n\n\n??\n\n\n\n\n\nMany algorithms work only, (or much faster) on sorted data\nSorting algorihtms are very good example for algorithmic thinking and algorihtm analysis\n\nalgorithmic thinking: in math there are often “existence” proofs, that say that an object exists, but do not prescribe a method to construct such an object. Algorithmic thinking is on the other hand always constructive in finite steps. Usually based on elementary operations. Using iteration and recursion, and principles of divide and conquer.\n\nNote: learning basic algorithms is valuable, but we should very rarely have to implement them from scratch and use library functions instead. (better implemented and more thoroughly tested)\nrules of the game:\n\nunterliegende Containerdatenstrukturen: Array (python list) \\(\\Rightarrow\\) allowed operations: len(a), v = a[i], a[i] = v s.t. i in [0..len(a) - 1]\nThe elements that are to be sorted can be compared: a[i] &lt; a[j] or a[i] &lt;= a[j], elements are totally ordered.\ntotal order:\n\nall pairs of elements can be compared.\nanti-symmetric: a &lt;= b and b &lt;= a ==&gt; a == b (or equivalently a &lt;= b and b =/= a then not b &lt;= a)\ntransitiv: a &lt;= b, b &lt;= c ==&gt; a &lt;= c\nreflexive: a &lt;= a, for all a.\n\n\npractical problem: The elements support u &lt;= v, but the algorithm requires u &lt; v. How to avert this problem?\na &lt; b &lt;=&gt; not(b &lt;= a)\ntheorem: &lt;, &gt;, &gt;=, ==, != all can be implemented with &lt;=, not, and\n\na &lt; b iff not (b &lt;= a)\na &gt; b iff b &lt; a\na &gt;= b iff b &lt;= a\na == b iff a &lt;= b and b &lt;= a\na != b iff not (a == b)\n\n\n\n\nprinciple:\n\nsorintg from left to write (left to the current position is all sorted)\nselect the appropriate element in the right-side of the current position.\n\n\ndef selection_sort(a) :\n    N = len(a)\n    # invariant: sorted(a[0..i-1]) and a[0..i-1] &lt;= a[i..N-1]\n    for i in range(N - 1) :  # iteration\n        j = i \n        # k = i + 1\n        # invariant: a[j] == min(a[i..k-1])\n        for k in range(i + 1, N) : \n            if a[k] &lt; a[j] : j = k\n        a[i], a[j] = a[j], a[i]\n\na = [3, -1, 5, 10]\nselection_sort(a)\nprint(a)\n\n[-1, 3, 5, 10]\n\n\n\n\nHow many steps does selection sort take?\n\nobviously depends on N.",
    "crumbs": [
      "Weekly Summary",
      "Week 3"
    ]
  },
  {
    "objectID": "ss25/alda/sum/w01.html",
    "href": "ss25/alda/sum/w01.html",
    "title": "Week 1",
    "section": "",
    "text": "Heico: Rundmails, Pruefungen\nMaMPF: VL Skript & Videos, Uebungen\nDiscord: Fragen, Teamfindung\nMuesli: Uebungspunkte\nAlgorithms and programming with python (python crashcourse next week)\nwebsite\n\n\n\n\n\n\nAlgorithmus:\n\nloest eint bestimmtes (wohldefiniertes) Problem - “Spezifikation”\nloest das Problem in endlich vielen Schritten - “Komplexitaetstheorie”\nAlle Schritte sind elementar (Bekannte einfache Subalgorithmen)\n\nProblem - Spezifikation:\n\nformale Beschreibung der Aufgabe\nenthaelt nicht die Loesung\n\nVorbedingugen: Notwendiger Zustand der “Welt”, damit der Algorithmus andwendbar ist. (Anforderungen and die Eingaben, evts, auch andere Umbegung)\nNachbedingen: Zustand der “Welt” am Ende des Algoirhtmus.\nBsp: Quadratwurzel \\(y = \\sqrt{x}\\)\n\nVorbedingung: \\(x \\in \\mathbb{N}\\), oder \\(x \\in \\mathbb{R}^{\\geq 0}\\) oder \\(x \\in \\mathbb{R}\\), if \\(y \\in \\mathbb{C}\\)\nNachbedingung: \\(y \\cdot y == x\\), falls Vorbedingung erfuellt, anderfalls, d.h. \\(x \\notin \\mathbb{R}\\), or \\(x &lt; 0\\), dann Fehlermeldung:\n\\(x \\in \\texttt{string}\\): Type error\n\\(x &lt; 0\\): Value error\n\nElementare Schritte:\n\npragmatische Definition: alles, was die Hardware, Programmierscprahe & Standartbibliothek schoin anbietet.\nformale Definition: Theoretische informatik - spezifikation Elementarer Operationen.\n\nBeispiel aus der Geometrie: Konstruktionen mit Zirkel und Lineal\n\nElementare Operationen:\n\ndefniere inen Punkt: (a. beliebig, b. als Schnittpunkt)\nmit Zirkel Abstand zwischen zwei Punkte abgleichen\nmit Zirkel einen Kres um einen geg. Punkt schneiden\n\nRadius beliebig\naus 2)\n\nMit Lineal Gerade durch zwei Punkte zeichnen.\n\n\n\n\nTheoretische Informatik:\n\nZiel der Theoretischen Informatik: mit moeglischst wenig Regeln (Elementare op.) moeglichst viele Algorithmen.\n\\(\\lambda\\)-Calculus\nRecursive Functions Theory\nTuring Machine Computability\nWhile-computability\nUeberraschendes Theorem der Theoretischen Informatik: Die Menge der realisierbaren Algorithmen ist gleich bei allen Systemen (berechenbare Funktionen)\nChurch-Turing These: Es gibt kein maechtigeres Regelsystemen (model of computability).\nwhile-programme:\n\naddition einer Konstante\nsubstraktion einer Konstante\nnacheinander Ausfuehrung zwei Programme (Anweisungen)\nwhile-schleife",
    "crumbs": [
      "Weekly Summary",
      "Week 1"
    ]
  },
  {
    "objectID": "ss25/alda/sum/w01.html#vl-1---15.04.25",
    "href": "ss25/alda/sum/w01.html#vl-1---15.04.25",
    "title": "Week 1",
    "section": "",
    "text": "Heico: Rundmails, Pruefungen\nMaMPF: VL Skript & Videos, Uebungen\nDiscord: Fragen, Teamfindung\nMuesli: Uebungspunkte\nAlgorithms and programming with python (python crashcourse next week)\nwebsite\n\n\n\n\n\n\nAlgorithmus:\n\nloest eint bestimmtes (wohldefiniertes) Problem - “Spezifikation”\nloest das Problem in endlich vielen Schritten - “Komplexitaetstheorie”\nAlle Schritte sind elementar (Bekannte einfache Subalgorithmen)\n\nProblem - Spezifikation:\n\nformale Beschreibung der Aufgabe\nenthaelt nicht die Loesung\n\nVorbedingugen: Notwendiger Zustand der “Welt”, damit der Algorithmus andwendbar ist. (Anforderungen and die Eingaben, evts, auch andere Umbegung)\nNachbedingen: Zustand der “Welt” am Ende des Algoirhtmus.\nBsp: Quadratwurzel \\(y = \\sqrt{x}\\)\n\nVorbedingung: \\(x \\in \\mathbb{N}\\), oder \\(x \\in \\mathbb{R}^{\\geq 0}\\) oder \\(x \\in \\mathbb{R}\\), if \\(y \\in \\mathbb{C}\\)\nNachbedingung: \\(y \\cdot y == x\\), falls Vorbedingung erfuellt, anderfalls, d.h. \\(x \\notin \\mathbb{R}\\), or \\(x &lt; 0\\), dann Fehlermeldung:\n\\(x \\in \\texttt{string}\\): Type error\n\\(x &lt; 0\\): Value error\n\nElementare Schritte:\n\npragmatische Definition: alles, was die Hardware, Programmierscprahe & Standartbibliothek schoin anbietet.\nformale Definition: Theoretische informatik - spezifikation Elementarer Operationen.\n\nBeispiel aus der Geometrie: Konstruktionen mit Zirkel und Lineal\n\nElementare Operationen:\n\ndefniere inen Punkt: (a. beliebig, b. als Schnittpunkt)\nmit Zirkel Abstand zwischen zwei Punkte abgleichen\nmit Zirkel einen Kres um einen geg. Punkt schneiden\n\nRadius beliebig\naus 2)\n\nMit Lineal Gerade durch zwei Punkte zeichnen.\n\n\n\n\nTheoretische Informatik:\n\nZiel der Theoretischen Informatik: mit moeglischst wenig Regeln (Elementare op.) moeglichst viele Algorithmen.\n\\(\\lambda\\)-Calculus\nRecursive Functions Theory\nTuring Machine Computability\nWhile-computability\nUeberraschendes Theorem der Theoretischen Informatik: Die Menge der realisierbaren Algorithmen ist gleich bei allen Systemen (berechenbare Funktionen)\nChurch-Turing These: Es gibt kein maechtigeres Regelsystemen (model of computability).\nwhile-programme:\n\naddition einer Konstante\nsubstraktion einer Konstante\nnacheinander Ausfuehrung zwei Programme (Anweisungen)\nwhile-schleife",
    "crumbs": [
      "Weekly Summary",
      "Week 1"
    ]
  },
  {
    "objectID": "ss25/alda/sum/w01.html#vl-2---17.04.25",
    "href": "ss25/alda/sum/w01.html#vl-2---17.04.25",
    "title": "Week 1",
    "section": "VL 2 - 17.04.25",
    "text": "VL 2 - 17.04.25\n\nMinimal set of elementary operations\n\nwhile-programs:\nset of operations:\n\naccesing a memory cell, read / write: X[i]\nadding a constant \\(c\\) to a memory cell and storing the result in an arbitary cell: X[j] := X[i] + c\nsubstracting a constant \\(c\\) from a memory cell: X[j] := X[i] - c\ncomposition of programs: \\(P\\), \\(Q\\) programs, then \\(P;Q\\) is a program\nwhile loop:\nwhile x[i] != 0 do\n  P // an arbitrary Program\ndone\nin this definition such a while program is valid if x[i] eventually becomes 0. (otherwise, infinite loop =&gt; no algorithm)\ntheorem: with these Elementary operations, all Turing-computable functions can be computed.\nexample: addition of two cells\n\nprecondition: X[j] &gt;= 0\nposcondition: X[i]' == X[i] + X[j]\nimplementation:\nread(X[i]);\nread(X[j]);\n// X[j] &gt;= 0 && X[i] + X[j] == X + Y\nwhile X[j] &gt; 0 do\n  X[j] = X[j] - 1;\n  X[i] = X[i] + 1 \ndone\n// X[j] == 0\n\n\nAround 1400 there was a disagreement about elementary operations. Two camps: Abacus-oriented vs Algorithm-oriented\n\nAbacus-oriented: Roman numerals + abacus operations. (Abacus operations were extremely fast with roman numbers)\nAlgorithm-oriented: how to compute with indian number, pen and paper - based on the book of Al Quarismi.\n\n1500 Adam Ries: “Rechnen auf der Federn und Linien” (Pen and Abacus)\nHeute: pragmatic definition of elementary operations based on the capabilities of the CPU, or on the programming language level the basic operations of the programming language.\n\n\n\nData Structures\nDefinition: Storing data in a way that can be found easily and interpreted correctly.\nExample: Consider sequence of bits: 1101,0110,0110,1100. The ‘meaning’ of this sequence depends on how\nit is interpreted:\n\ninterpretation as a positive binary number.\ninterpretation as a binary integer =&gt; 2s complement.\ninterpreted as a sequence of 8-bit characters:\nimterpreted as a floating point number according to the IEEE standard: S | EEEEE | MMMMMMMMMM\n\nmain point: same symbol can be interpreted in various ways, depending on the system.\n\nprogramming languages: variables have associated types and the compiler or the interpreter determines what type it is and interprets this accordingly.\nFiles:\n\nending of the file .jpg\nmagic number in file”: 255 216 255\ntype standards for communication and Operating system.\n\n\n\nData-structure Triangle\n\n\n\ndata-triangle\n\n\nADT: abstract data type = no implementation is given, no bit sequence is defined. Supported operations and range of values is defined.",
    "crumbs": [
      "Weekly Summary",
      "Week 1"
    ]
  },
  {
    "objectID": "ss25/alda/index.html",
    "href": "ss25/alda/index.html",
    "title": "Algorithms and Data Structures",
    "section": "",
    "text": "notes\nsolutions\nweekly sum",
    "crumbs": [
      "alda"
    ]
  },
  {
    "objectID": "ss24/r/sum.html",
    "href": "ss24/r/sum.html",
    "title": "Weekly Summary",
    "section": "",
    "text": "date:\n\n\n\ndate:"
  },
  {
    "objectID": "ss24/r/sum.html#week-1",
    "href": "ss24/r/sum.html#week-1",
    "title": "Weekly Summary",
    "section": "",
    "text": "date:\n\n\n\ndate:"
  },
  {
    "objectID": "ss24/r/sum.html#week-2",
    "href": "ss24/r/sum.html#week-2",
    "title": "Weekly Summary",
    "section": "Week 2",
    "text": "Week 2\n\nLecture 1\ndate: 22/4/24\n\nresources for R\n\nadvanced R ~ hadley wickham link\nr graphics cookbook link\ntidyverse\n\npackages\n\ninstalling and removing packages, package dependencies; shiny, learnr\nnamespaces and name collisions;\ndatasets package for learning; mtcars, iris\n\nfunction return value and print side effect\nclearing variables with rm(x)\nplotting\n\nbasic data plotting with plot\nbasic function plotting with plot and curve\n\n\n\n\nLecture 2\ndate: 26/4/24\n\ncontent: moodle videos 3 & 4\nr is object oriented - everything is an object\n\nclass vs typeof (small difference)\n\nr is also functional: everything that happens is a function call\n\ndifference between functions and function calls sin vs sin()\n\natomic vectors vs lists\ntype casting\nzero length vectors\nmachine numbers (representation of real numbers in R)\nbig integers - r represents large integers instead of 2’s complement.\nvectors have no dimension, array has one dimension. vectors are not arrays.\nfor data-sets use data-frames like: tibbles, data tables…"
  },
  {
    "objectID": "ss24/r/sum.html#week-3",
    "href": "ss24/r/sum.html#week-3",
    "title": "Weekly Summary",
    "section": "Week 3",
    "text": "Week 3\n\nLecture 1\ndate:\n\n\nLecture 2\ndate: 03/05/24 - Fri\n\nnew features in R\n\nother constructs instead of for loops that are faster\nanonymous functions, lambda notation\ndiscrete probability distributions\nstrings as factors\n.class2 can show all classes to which an object belongs, including implicit classes.\n\nnew features in r 4.1\n\nfactor function behavior\nnative pipe operator, analogous to shell pipes, eliminating the need for magrittr pipes.\nas.vector() removes attributes from a vector, like component names.\ncolon operator precedence.\ntail recursion"
  },
  {
    "objectID": "ss24/r/sum.html#week-1-1",
    "href": "ss24/r/sum.html#week-1-1",
    "title": "Weekly Summary",
    "section": "Week 1",
    "text": "Week 1\n\nLecture 1\ndate:\n\n\nLecture 2\ndate:"
  },
  {
    "objectID": "ss24/r/sum.html#week-1-2",
    "href": "ss24/r/sum.html#week-1-2",
    "title": "Weekly Summary",
    "section": "Week 1",
    "text": "Week 1\n\nLecture 1\ndate:\n\n\nLecture 2\ndate:"
  },
  {
    "objectID": "ss24/r/sum.html#week-1-3",
    "href": "ss24/r/sum.html#week-1-3",
    "title": "Weekly Summary",
    "section": "Week 1",
    "text": "Week 1\n\nLecture 1\ndate:\n\n\nLecture 2\ndate:"
  },
  {
    "objectID": "ss24/r/sum.html#week-1-4",
    "href": "ss24/r/sum.html#week-1-4",
    "title": "Weekly Summary",
    "section": "Week 1",
    "text": "Week 1\n\nLecture 1\ndate:\n\n\nLecture 2\ndate:"
  },
  {
    "objectID": "ss24/r/sum.html#week-1-5",
    "href": "ss24/r/sum.html#week-1-5",
    "title": "Weekly Summary",
    "section": "Week 1",
    "text": "Week 1\n\nLecture 1\ndate:\n\n\nLecture 2\ndate:"
  },
  {
    "objectID": "ss24/r/sum.html#week-1-6",
    "href": "ss24/r/sum.html#week-1-6",
    "title": "Weekly Summary",
    "section": "Week 1",
    "text": "Week 1\n\nLecture 1\ndate:\n\n\nLecture 2\ndate:"
  },
  {
    "objectID": "ss24/r/sum.html#week-1-7",
    "href": "ss24/r/sum.html#week-1-7",
    "title": "Weekly Summary",
    "section": "Week 1",
    "text": "Week 1\n\nLecture 1\ndate:\n\n\nLecture 2\ndate:"
  },
  {
    "objectID": "ss24/r/sum.html#week-1-8",
    "href": "ss24/r/sum.html#week-1-8",
    "title": "Weekly Summary",
    "section": "Week 1",
    "text": "Week 1\n\nLecture 1\ndate:\n\n\nLecture 2\ndate:"
  },
  {
    "objectID": "ss24/r/sum.html#week-1-9",
    "href": "ss24/r/sum.html#week-1-9",
    "title": "Weekly Summary",
    "section": "Week 1",
    "text": "Week 1\n\nLecture 1\ndate:\n\n\nLecture 2\ndate:"
  },
  {
    "objectID": "ss24/r/sum.html#week-1-10",
    "href": "ss24/r/sum.html#week-1-10",
    "title": "Weekly Summary",
    "section": "Week 1",
    "text": "Week 1\n\nLecture 1\ndate:\n\n\nLecture 2\ndate:"
  },
  {
    "objectID": "ss24/r/sum.html#week-1-11",
    "href": "ss24/r/sum.html#week-1-11",
    "title": "Weekly Summary",
    "section": "Week 1",
    "text": "Week 1\n\nLecture 1\ndate:\n\n\nLecture 2\ndate:"
  },
  {
    "objectID": "ss24/r/sum.html#week-1-12",
    "href": "ss24/r/sum.html#week-1-12",
    "title": "Weekly Summary",
    "section": "Week 1",
    "text": "Week 1\n\nLecture 1\ndate:\n\n\nLecture 2\ndate:"
  },
  {
    "objectID": "ss24/oop/sum.html",
    "href": "ss24/oop/sum.html",
    "title": "Weekly Summary",
    "section": "",
    "text": "date:\n\n\n\ndate:"
  },
  {
    "objectID": "ss24/oop/sum.html#week-1",
    "href": "ss24/oop/sum.html#week-1",
    "title": "Weekly Summary",
    "section": "",
    "text": "date:\n\n\n\ndate:"
  },
  {
    "objectID": "ss24/oop/sum.html#week-2",
    "href": "ss24/oop/sum.html#week-2",
    "title": "Weekly Summary",
    "section": "Week 2",
    "text": "Week 2\n\nLecture 1\ndate:\n\n\nLecture 2\ndate: 25/4/24\n\nslides: 27 -\ntesting with googletests\n\ngoogletests has to be cloned into the project as a submodule and build together with your code.\n\ntemplates\ntemplate templates\nrenaming types with typedef\nclasses and structs\n\nessentially the same thing, only difference being the default visibility: public for struct.\naccess specifiers: private, public, protected\nfriend class\nconstructors (they are a little messy in c++ due to legacy code issues)\n\nmove constructor:\nconverting constructors (implicit type conversions) \\(\\rightarrow\\) hard to find bugs.\nexplicit keyword.\nconversion operators.\ndelegating constructors.\n\ndestructors \\(\\rightarrow\\) RAII (resource acquisition if initialization)\nthe rule of zero or five.\nmutable members\nthe singleton pattern (single source of truth)"
  },
  {
    "objectID": "ss24/oop/sum.html#week-3",
    "href": "ss24/oop/sum.html#week-3",
    "title": "Weekly Summary",
    "section": "Week 3",
    "text": "Week 3\n\nLecture 1\ndate: 29/04/24 - Monday\n\nbasic cmake concepts\nSheet 2 execises discussion\n\nbasic googletest inclusion - slides are sufficient\n\npresentation proposal should have a presentation date.\n\n\n\nLecture 2\ndate: 02/05/24 - Thursday\n\nslides: 02 - standard\nreview of past lecture:\n\ntemplates\ntypedef\nclasses & structs\nconstructors\ndestructors: deep copy\n\nfundamental concepts of c++: inheritence\n\nlittle inheritence example\ndeadly diamond of death\ncomposition over inheritence\npublic vs private/protected inheritence\nis-a vs has-a\nSOLID principles\n\ndiscussion of the Liskov Subtitution Principle (LSP)\n\n\nproject structure\n\nheader files, header guards\nsource files separate from headers (templates go against this)\nnamespaces to structure and prevent name clashes\n\nc++ standard library:\n\noverview\nstandard input/output, standard error\ncontainers and companion classes\n\nsequences \\(\\approx\\) arrays: array, vector, deque, list, forward list\ncontainer adaptors stack, queue, priority queue\nassociative containers \\(\\approx\\) hash map: set, multiset, map, multimap\nunsorted associative containers: unordered set/multiset/map/multimap\n\niterators: generalization of pointer concept, they can be dereferenced and advanced to show to the subsequent element\nalgorithms: tailored to dfferent iterator categories, make full use of the capabilities of the container.\n\nfunctional programming examples\n\n\nError Handling:\n\nassert: Run-time sanity check\nstatic_assert: compile check with a similar purpose for template meta programming\nexception: error handling mechanism for situation that should not be the norm\n\ndiscussion of exceptions"
  },
  {
    "objectID": "ss24/oop/sum.html#week-4",
    "href": "ss24/oop/sum.html#week-4",
    "title": "Weekly Summary",
    "section": "Week 4",
    "text": "Week 4\n\nLecture 1\ndate: 06/05/24\n\nslides: 03 - advanced\nInheritence & dynamic polymorphism further discussions.\npolymorphism\n\nstatic polymorphism \\(\\approx\\) early binding\ndynamic polymorphism \\(\\approx\\) late binding\nsimple c++ illustration with the keyword virtual, references etc…\n\nc++ implementation of dynamic polymorphism using vtables (dispatch tables).\ncost of polymorphism\ncopying polymorphic types\n\nabstract base class (with a pure vitual function)\nfactory pattern (it is a creational pattern)\n\n\n\nLecture 2\ndate:\nholiday"
  },
  {
    "objectID": "ss24/oop/sum.html#week-5",
    "href": "ss24/oop/sum.html#week-5",
    "title": "Weekly Summary",
    "section": "Week 5",
    "text": "Week 5\n\nLecture 1\ndate: 13/5/24\n\nsolutions to the exercise sheet 2\npresentations:\n\ngit branches & tags\nIDE’s\nvalgrind\n\n\n\n\nLecture 2\ndate: 16/5/24\n\nc++ 11 features: (slide 95)\n\nautomatic type deduction\ntrailing return type\nmove semantics. Before c++ 11 there were two types of variables:\n\nvalues: assignment creates a new independent entity\nreferences and pointers: assignment creates and alias\ncopy constructor:\n\ndeep copy \\(\\Rightarrow\\) value semantics\nshallow copy \\(\\Rightarrow\\) reference semantics\n\nc++ 11: third type of semantics: move semantics\nforwarding references are not\n\nsmart pointers\n\nunique pointers: unique_ptr\nshared pointers: shared_ptr\nweak pointers (non-owning): weak_ptr\nraw pointer usage should be avoided\nso basically in modern c++ two types of pointers\n\nowning pointers: unique_ptr, shared_ptr\nnon-owning pointers: weak_ptr\n\npitfalls:\n\nlambda expressions & closures: generalization of functors in c++\n\ncurrying: \\((\\mathbb{N} \\times \\mathbb{N}) \\rightarrow \\mathbb{N} \\equiv \\mathbb{N} \\rightarrow (\\mathbb{N} \\rightarrow \\mathbb{N})\\)\nbuilder pattern with lambda functions\n\nrandom number generators\ntime measurement (slide 123)"
  },
  {
    "objectID": "ss24/oop/sum.html#week-6",
    "href": "ss24/oop/sum.html#week-6",
    "title": "Weekly Summary",
    "section": "Week 6",
    "text": "Week 6\n\nLecture 1\ndate: 20/05/24\n\nholiday\n\n\n\nLecture 2\ndate: 23/05/24\n\nsolutions & presentations"
  },
  {
    "objectID": "ss24/oop/sum.html#week-7",
    "href": "ss24/oop/sum.html#week-7",
    "title": "Weekly Summary",
    "section": "Week 7",
    "text": "Week 7\n\nLecture 1\ndate: 05/27/24\n\nreview of c++ 11 features from previews lectures:\n\nauto keyword: when to use it.\nmove constructors and move assignment\nshared pointers, weak pointers, unique pointers.\nlambda expressions, functors.\nrandom number generators\n\nrange based loops\nadvanced topics:\n\ninheritance and dynamic polymorphism\nresource acquisition is initialization (RAII)\ntemplate specialization:\n\nfunction template specialization, vs\nclass template specialization\n\ntemplates vs inheritance\n\nachieving the same effect as dynamic polymorphism with static polymorphisdm \\(\\Rightarrow\\) CRTP (curiously recurring template pattern)\n\n\n\n\n\n\nLecture 2\ndate:"
  },
  {
    "objectID": "ss24/oop/sum.html#week-1-1",
    "href": "ss24/oop/sum.html#week-1-1",
    "title": "Weekly Summary",
    "section": "Week 1",
    "text": "Week 1\n\nLecture 1\ndate:\n\n\nLecture 2\ndate:"
  },
  {
    "objectID": "ss24/oop/sum.html#week-1-2",
    "href": "ss24/oop/sum.html#week-1-2",
    "title": "Weekly Summary",
    "section": "Week 1",
    "text": "Week 1\n\nLecture 1\ndate:\n\n\nLecture 2\ndate:"
  },
  {
    "objectID": "ss24/oop/sum.html#week-1-3",
    "href": "ss24/oop/sum.html#week-1-3",
    "title": "Weekly Summary",
    "section": "Week 1",
    "text": "Week 1\n\nLecture 1\ndate:\n\n\nLecture 2\ndate:"
  },
  {
    "objectID": "ss24/oop/sum.html#week-1-4",
    "href": "ss24/oop/sum.html#week-1-4",
    "title": "Weekly Summary",
    "section": "Week 1",
    "text": "Week 1\n\nLecture 1\ndate:\n\n\nLecture 2\ndate:"
  },
  {
    "objectID": "ss24/oop/sum.html#week-1-5",
    "href": "ss24/oop/sum.html#week-1-5",
    "title": "Weekly Summary",
    "section": "Week 1",
    "text": "Week 1\n\nLecture 1\ndate:\n\n\nLecture 2\ndate:"
  },
  {
    "objectID": "ss24/oop/sum.html#week-1-6",
    "href": "ss24/oop/sum.html#week-1-6",
    "title": "Weekly Summary",
    "section": "Week 1",
    "text": "Week 1\n\nLecture 1\ndate:\n\n\nLecture 2\ndate:"
  },
  {
    "objectID": "ss24/oop/sum.html#week-1-7",
    "href": "ss24/oop/sum.html#week-1-7",
    "title": "Weekly Summary",
    "section": "Week 1",
    "text": "Week 1\n\nLecture 1\ndate:\n\n\nLecture 2\ndate:"
  },
  {
    "objectID": "ss24/oop/sum.html#week-1-8",
    "href": "ss24/oop/sum.html#week-1-8",
    "title": "Weekly Summary",
    "section": "Week 1",
    "text": "Week 1\n\nLecture 1\ndate:\n\n\nLecture 2\ndate:"
  },
  {
    "objectID": "ss24/oop/index.html",
    "href": "ss24/oop/index.html",
    "title": "OOP Sci",
    "section": "",
    "text": "Object Oriented Programming for Scientific Computing",
    "crumbs": [
      "OOP Sci"
    ]
  },
  {
    "objectID": "ss24/oop/index.html#section",
    "href": "ss24/oop/index.html#section",
    "title": "OOP Sci",
    "section": "",
    "text": "weekly summary\nnotes\nsol\nplan\nexam",
    "crumbs": [
      "OOP Sci"
    ]
  },
  {
    "objectID": "ss24/index.html#courses",
    "href": "ss24/index.html#courses",
    "title": "SS 24",
    "section": "Courses",
    "text": "Courses\n\nAlda\nR\nOOP",
    "crumbs": [
      "Bachelor",
      "SS 24"
    ]
  },
  {
    "objectID": "ss24/index.html#time",
    "href": "ss24/index.html#time",
    "title": "SS 24",
    "section": "Time",
    "text": "Time\n\ntimes",
    "crumbs": [
      "Bachelor",
      "SS 24"
    ]
  },
  {
    "objectID": "ss24/alda/plan.html#reading-list",
    "href": "ss24/alda/plan.html#reading-list",
    "title": "Study Plan",
    "section": "Reading List",
    "text": "Reading List\n\nAlgorithms\n\nBasic:\n\nUnderstanding Algorithms. Brunskill\nAlgorithms Unlocked. Cormen\nFirst Course in Algorithms Through Puzzles. Ryuhei Uehara\nAlgorithmic Thinking. Daniel Zingaro\nPrincipes of Algorithmic Problem Solving. Johan Sannemo\nGrokking Algorithms. Aditya Bhargava\n\nIntermediate:\n\nDesign and Analysis of Algorithms. Jeffrey Smith\nAlgorithms. Jeff Erickson\nHow to Think About Algorithms. Jeff Edmonds\nProblems on Algorithmics. Ian Perberry\nFundamentals of Algorithmics. Brassard, Bratley.\nAlgorithmen & Datenstrukturen - Grundwerkzeuge. Kurt Melhorn\nLecture Notes on Algorithms. Ian Perberry\nAlgorithm Design. Kleinberg, Tardos\nAlgorithms Illuminated. Roughgarden\nCompared to What. G. J. E. Rawlins\nFoundations of Algorithms. Richard Neapolitan\nData Structurese & Their Algorithms. Harry Lewis, Larry Denenberg\nAlgorithms + Data Structures = Programs. Niklaus Wirth\nAlgorithms and Data Structures - Design, Correctness, Analysis. Jeffrey H. Kingston\nComputer Algorithms. Baase\n\nC++:\n\nData Structures & Problem Solving Using C++. M. A. Weiss\nData Structures & Algorithm Analysis in C++. M. A. Weiss\nData & Algorithms in C++. Drozdek.\nData Structures other Objects using C++. Walter Savitch\nPrinciples of Algorithmic Problem Solving. Johan Sannemo\nGuide to Competitive Programming. Antti Laaksonen\n\nC:\n\nAlgorithms and Data Structures - An Approach in C. Bowman\nFoundations of Computer Science. Aho, Ullman\nPrograms and Datastructures in C. Ameraal\n\nPython:\n\nData Structures and Algorithms Using Python. Rance D. Necaise.\nData Structures & Algorithms in Python. Canning, Broder, Lafore\nCompetitive Programaming with Python. Duerr, Vie\nProblem Solving with Algorithms and Data Structures Using Python. Franklin, Beedle\n\n\n\n\nGraph Theory and Discrete Mathematics\n\nGeneral Discrete Mathematics:\n\nMathematical Structures for Computer Science. Judith Gersting\nDiscrete & Combinatorial Mathematics. Grimaldi\nConcrete Mathematics Knuth\nDiskerte Mathematik fuer Einsteiger. Beutelspacher\nDiscrete Mathematics in Computer Science. Golovnev, Kulikov\n\nGraph Theory Specific\n\nGraph Theory - A Poblem Oriented Approach. Daniel A. Marcus\nAlgorithmic Graph Theory. Alan M. Gibbons\nSets, Puzzles & Postmen. Higgins\n\n\nSpecifically:\n\nGersting:\n\n3: Recurrence relations & analysis of algorithms\n5: Graphs & Trees\n7: Graph algorithms\n\nRosen:\n\n3: Algorithms\n5: Induction & Recursion\n8: Advancerd Counting: recurrence relations\n10: Graphs\n11: Trees\n\nGrimaldi:\n\n4: Mathematical induction\n5.7, 5.8: Analysis of Algorithms\n10: Recurrence relations\n11, 12, 13: Graph Theory"
  },
  {
    "objectID": "ss24/alda/exam1.html",
    "href": "ss24/alda/exam1.html",
    "title": "My Uni Notes",
    "section": "",
    "text": "what was asked on the exam:\n\nAdjacency Matrix \\(A\\) of a graph \\(G\\):\n\nwrite the pseudocode for an optimal algorithm that determines the total number of nodes in the graph with equal in- and out-degrees.\nAnalyze the complexity of your algorithm and prove optimality\n\nHeaps\nHashing\n\nOpen hashing. Insert a sequence of elements\nOpen hashing. Delete a sequence of elements\n\nStandard questions for comparing functions asymptotically, determining growth rate of reccurence relations with and without applications of the master theorem, analyzing complexity of short pseudocode algorithms like\nread(t)\nk := 1\ni : = 1\nwhile (k &lt;= t) :\n    i := i + 1\n    k := k + i\nShort proofs of statements regarding (a, b) trees and graphs\nSimple proof by induction problem\nDijkstra shortest path algorithm\n\nwasn’t asked:\n\nsorting\nproblems specifically for divide & conquer / recursion\nproblems specifically for dynamic programming\nlinear programming\napproximation / heuristic algorithms\nalgorithms on strings"
  },
  {
    "objectID": "ss24/alda/exam1.html#alda-sose-24-1st-exam",
    "href": "ss24/alda/exam1.html#alda-sose-24-1st-exam",
    "title": "My Uni Notes",
    "section": "",
    "text": "what was asked on the exam:\n\nAdjacency Matrix \\(A\\) of a graph \\(G\\):\n\nwrite the pseudocode for an optimal algorithm that determines the total number of nodes in the graph with equal in- and out-degrees.\nAnalyze the complexity of your algorithm and prove optimality\n\nHeaps\nHashing\n\nOpen hashing. Insert a sequence of elements\nOpen hashing. Delete a sequence of elements\n\nStandard questions for comparing functions asymptotically, determining growth rate of reccurence relations with and without applications of the master theorem, analyzing complexity of short pseudocode algorithms like\nread(t)\nk := 1\ni : = 1\nwhile (k &lt;= t) :\n    i := i + 1\n    k := k + i\nShort proofs of statements regarding (a, b) trees and graphs\nSimple proof by induction problem\nDijkstra shortest path algorithm\n\nwasn’t asked:\n\nsorting\nproblems specifically for divide & conquer / recursion\nproblems specifically for dynamic programming\nlinear programming\napproximation / heuristic algorithms\nalgorithms on strings"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Uni Notes",
    "section": "",
    "text": "Plan"
  },
  {
    "objectID": "index.html#semesters",
    "href": "index.html#semesters",
    "title": "Uni Notes",
    "section": "Semesters",
    "text": "Semesters\n\nWS 23/24\nSS 24\nWS 24/25\nSS 25"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "My uni notes & resources"
  },
  {
    "objectID": "plan.html",
    "href": "plan.html",
    "title": "Plan",
    "section": "",
    "text": "Pflicht (total = 54LP)\n\nAlda 8LP\nIBN 8LP\nISW 8LP\nBS (4LP + 2UK) 6LP\nFP 8LP\nBA 12LP\nBK 4LP\n\nWM (total = 22LP)\n\nWM-I 8LP\nWM-II 8LP\nWM-III 6LP\n\nFUK (total = 4LP)\n\nFUK-I (2UK) 2LP\nFUK-II (2UK) 2LP\n\nAG (total: 8LP)\n\nTheoPhys-II 8LP\n\n\nsum total: 88LP\nprogress: 92/180 \\(\\approx\\) 51%\n\n\nVersion 1\n\nSS 24 (total: 16LP)\n\nAlda 8LP\nWM-I 6LP\nFUK-I (2UK) 2LP\n\nWS 24/25 (total: 18LP)\n\nISW 8LP\nWM-II 8LP\nFUK-II (2UK) 2LP\n\nSS 25 (total: 24LP)\n\nFP 8LP\nIBN 8LP\nAG: TheoPhys-II 8LP\n\nWS 25/26 (total: 14LP)\n\nWM-III 8LP\nBS 6LP\n\nSS 26 (total: 16LP)\n\nBA 12LP\nBK 4LP"
  },
  {
    "objectID": "plan.html#to-do-as-of-ss-24",
    "href": "plan.html#to-do-as-of-ss-24",
    "title": "Plan",
    "section": "",
    "text": "Pflicht (total = 54LP)\n\nAlda 8LP\nIBN 8LP\nISW 8LP\nBS (4LP + 2UK) 6LP\nFP 8LP\nBA 12LP\nBK 4LP\n\nWM (total = 22LP)\n\nWM-I 8LP\nWM-II 8LP\nWM-III 6LP\n\nFUK (total = 4LP)\n\nFUK-I (2UK) 2LP\nFUK-II (2UK) 2LP\n\nAG (total: 8LP)\n\nTheoPhys-II 8LP\n\n\nsum total: 88LP\nprogress: 92/180 \\(\\approx\\) 51%\n\n\nVersion 1\n\nSS 24 (total: 16LP)\n\nAlda 8LP\nWM-I 6LP\nFUK-I (2UK) 2LP\n\nWS 24/25 (total: 18LP)\n\nISW 8LP\nWM-II 8LP\nFUK-II (2UK) 2LP\n\nSS 25 (total: 24LP)\n\nFP 8LP\nIBN 8LP\nAG: TheoPhys-II 8LP\n\nWS 25/26 (total: 14LP)\n\nWM-III 8LP\nBS 6LP\n\nSS 26 (total: 16LP)\n\nBA 12LP\nBK 4LP"
  },
  {
    "objectID": "ss24/alda/index.html",
    "href": "ss24/alda/index.html",
    "title": "Alda",
    "section": "",
    "text": "Algorithms and Data Structures",
    "crumbs": [
      "Alda"
    ]
  },
  {
    "objectID": "ss24/alda/index.html#section",
    "href": "ss24/alda/index.html#section",
    "title": "Alda",
    "section": "",
    "text": "weekly summary\nnotes\nsolutions\nplan\nexam",
    "crumbs": [
      "Alda"
    ]
  },
  {
    "objectID": "ss24/alda/sum.html",
    "href": "ss24/alda/sum.html",
    "title": "Weekly Summary",
    "section": "",
    "text": "date:\n\n\n\ndate:"
  },
  {
    "objectID": "ss24/alda/sum.html#week-1",
    "href": "ss24/alda/sum.html#week-1",
    "title": "Weekly Summary",
    "section": "",
    "text": "date:\n\n\n\ndate:"
  },
  {
    "objectID": "ss24/alda/sum.html#week-2",
    "href": "ss24/alda/sum.html#week-2",
    "title": "Weekly Summary",
    "section": "Week 2",
    "text": "Week 2\n\nLecture 1\ndate:\n\n\nLecture 2\ndate: 23/04/24\n\nfachschaft\n\nstapel\nkaffeklatsch\nLernraum - time & place tba (instagram, whatsapp groups)\nRoom: INF 205 Mathematikon\nwebsite\n\ncompiler explorer\nSmall assembly example; modern compilers are very clever at optimizing\nassertions, pre- and post conditions, invariants.\nfast power algorithm, it’s correctness proof\nrules for calculating time complexity of sequential programs\na common reccurence relation pattern that comes up often,\n\n\\[\\begin{align}\nR(n) &= a, &&\\text{if n = 1} \\\\\n     &= cn + d\\cdot R(n/b) &&\\text{if n &gt; 1, divide and conquer}\n\\end{align}\\]\n\ncomplexity of this reccurence relation and its proof\nIntro to Graphs, basic definitions"
  },
  {
    "objectID": "ss24/alda/sum.html#week-3",
    "href": "ss24/alda/sum.html#week-3",
    "title": "Weekly Summary",
    "section": "Week 3",
    "text": "Week 3\n\nLecture 1\ndate:\n\n\nLecture 2\ndate: 30/04/24\n\nsimply linked list\n\nsplicing\n\narrays\n\nassembly realization of array access via the website compiler explorer link with c++ and rust. Differences between c++ and rust.\nmemory allocation in c++ with alloc() and free()\ntime complexity of array memory allocation in c++ with alloc(): an experiment\npushBack() and popBack() for arrays\n\ntheir realization and complexity"
  },
  {
    "objectID": "ss24/alda/sum.html#week-4",
    "href": "ss24/alda/sum.html#week-4",
    "title": "Weekly Summary",
    "section": "Week 4",
    "text": "Week 4\n\nLecture 1\ndate: 6/5/24\n\nintroduction to amortized complexity\n\namortized complexity analysis of pushBack() and popBack() operations: both \\(\\mathcal{O}(1)\\)\n\nstacks and queues - introductory discussions\n\ndouble-ended queues\n\nring buffer implementation of a queue\nintroduction to hashing\n\n\n\nLecture 2\ndate: 07/05/24\n\ndivision by a constant is optimized by the compiler (this cannot be done for division by a variable)\nbook recommendation: Hacker’s Delight\nHashing:\n\nintro & some applications\nsome defs:\n\n\\(M \\subseteq T\\). \\(M\\): set of Elements of a certain type \\(T\\), that we want to store (or have stored) in a table and access via their keys.\n\\(m\\): number of memory slots.\n\\(|M|\\): total number of elements stored in the table.\n\\(\\text{key}: M\\rightarrow Key\\): Function that maps elements to their key values\n\\(Key = im(key) = key[M]\\): the set of key values (the range of the key function)\n\\(h: Key\\rightarrow [0, m)\\): the hashing function that maps key values to memory slots \\(0\\dots m - 1\\).\n\npefect hashing: if there are more memory slots than possible key values, \\(h\\) can map each key to a single slot \\(\\Rightarrow\\) over-optimistic, we don’t have so much memory, since usually \\(|Key|\\gg m\\) (Number of possible key values much greater than number of slots).\nimperfect hashing: \\(\\exists e_1, e_2\\in M\\) s.t. \\(e_1 \\neq e_2\\) but \\(h(key(e_1)) = h(key(e_2))\\). This is called collision.\nclosed hashing: an example for imperfect hashing , where the elements of the table are simply linked lists, supporting the operations:\n\ninsert(e). Insert en element \\(e\\in M\\)\nremove(k). Remove an element \\(e\\) whose key is \\(k\\), returning \\(e\\)\nfind(k): Find element \\(e\\) with the key k, return \\(e\\) if found.\nTime-complexities:\n\ninsert(e): \\(\\mathcal{O}(1)\\)\nremove(k): \\(\\mathcal{O}(\\text{List length})\\)\nfind(k): \\(\\mathcal{O}(\\text{List length})\\)\nworst case: bad hash function that maps all keys to the same slot ! \\(\\Rightarrow \\mathcal{O}(|M|)\\)\n\nStochastic analysis of random hash functions & proofs of (with an introductory discussion of birthday paradox):\n\ntheorem: random hash function is likely to be perfect if \\(m \\in \\Omega(n^2)\\)\ntheorem: random hash functions lead to lists of length \\(\\mathcal{O}(1)\\), if \\(|M| \\in \\mathcal{O}(m)\\)"
  },
  {
    "objectID": "ss24/alda/sum.html#week-5",
    "href": "ss24/alda/sum.html#week-5",
    "title": "Weekly Summary",
    "section": "Week 5",
    "text": "Week 5\n\nLecture 1\ndate: 13/5/24\n\nUniversal hashing functions: definition & Theorem 1 holds also for universal hashing functions\n\nA family of universal hashing functions, proof of theorem 1 for these functions\nanother example for a universal hashing function based on bit matrix multiplication\nexample: tabulation hashing\n\nopen hashing: a different way to resolve collisions, without using linked lists, instead looking for the next free table slot:\n\nadvantage: contigious address values \\(\\Rightarrow\\) less cache misses, faster.\nbounded linear probing:\n\ninsert & find algorithms, invariants.\nremove is more difficult \\(\\Rightarrow\\) its implementation.\n\n\nintro to sorting\n\nintro to insertion sort.\n\n\n\n\nLecture 2\ndate:"
  },
  {
    "objectID": "ss24/alda/sum.html#week-6",
    "href": "ss24/alda/sum.html#week-6",
    "title": "Weekly Summary",
    "section": "Week 6",
    "text": "Week 6\n\nLecture 1\ndate: 20/05/24\n\nonline: Heap data structure\n\n\n\nLecture 2\ndate: 21/05/24\n\nprogramming example with compiler explorer: fibonacci iterative and recursive implementation, gcc optimization. (short detour: call stack) compiler can automatically implement tail recursion.\ninsertion algorithms:\n\ninsertion sort: pseudo-code implementation.\nmerge sort:\n\ndivide and conquer,\nmerging (\\(\\mathcal{O}(n))\\))\ntime-complexity of merge sort: assuming (without loss of generality) \\(n = 2^k\\) leads to master theorem. (wlog: we can extend a list to be \\(2^k\\))\n\n\\(\\Theta(n\\cdot\\log(n))\\) is the best we can do for comparison-based sorting\n\ndefinition of comparison-based sorting, fundamental operations\nany such algorithm must at least be able to differentiate between all \\(n!\\) permutations of the list \\(\\Rightarrow\\) lower-bound analysis via a comparison tree.\n\n\n\npartial integration\n\n\n\nquicksort: divide-and-conquer, but “reversed”\n\ncomplexity of quicksort (worst case is \\(\\mathcal{O}(n^2))\\)\nPivot is always the median \\(\\Rightarrow\\) \\(\\mathcal{O}(n\\cdot\\log{n})\\)\nproblem: finding the median (a good pivot) is not easy.\ncomplexity-analysis:"
  },
  {
    "objectID": "ss24/alda/sum.html#week-7",
    "href": "ss24/alda/sum.html#week-7",
    "title": "Weekly Summary",
    "section": "Week 7",
    "text": "Week 7\n\nLecture 1\ndate: 27/05/24\n\n\n\n\n\nLecture 2\ndate: 28/05/24\n\nexample: the c++ compiler optimizes calculation of middle value by generating assembly for r + (r - l) / 2 instead of (l + r) / 2, in order to reduce chance of overflow.\nquicksort with tail-recursion to reduce stack depth (revisiion of the previous lecture)\nquickselect algorithm from quicksort: can be used to determine the median of a sequence in \\(\\mathcal{O}(n)\\).\nsorting faster than \\(\\mathcal{\\Omega}(n\\log(n))\\) (without comparisons):\n\nKSort (Bucketsort), array implementation of bucketsort (in-place).\n\\(K^d\\) Sort: Least Significant Digit Radix Sorting.\n\nbinary search\n\nintermezzo: insertion sort in \\(\\mathcal{O}(\\log(n)\\cdot n)\\): library sort link"
  },
  {
    "objectID": "ss24/alda/sum.html#week-8",
    "href": "ss24/alda/sum.html#week-8",
    "title": "Weekly Summary",
    "section": "Week 8",
    "text": "Week 8\n\nLecture 1\ndate:\n\n\nLecture 2\ndate: 06/04/24\n\nc++ unique pointer implementation and move semantics\n(a, b) Trees - inserting"
  },
  {
    "objectID": "ss24/alda/sum.html#week-1-1",
    "href": "ss24/alda/sum.html#week-1-1",
    "title": "Weekly Summary",
    "section": "Week 1",
    "text": "Week 1\n\nLecture 1\ndate:\n\n\nLecture 2\ndate:"
  },
  {
    "objectID": "ss24/alda/sum.html#week-1-2",
    "href": "ss24/alda/sum.html#week-1-2",
    "title": "Weekly Summary",
    "section": "Week 1",
    "text": "Week 1\n\nLecture 1\ndate:\n\n\nLecture 2\ndate:"
  },
  {
    "objectID": "ss24/alda/sum.html#week-1-3",
    "href": "ss24/alda/sum.html#week-1-3",
    "title": "Weekly Summary",
    "section": "Week 1",
    "text": "Week 1\n\nLecture 1\ndate:\n\n\nLecture 2\ndate:"
  },
  {
    "objectID": "ss24/alda/sum.html#week-1-4",
    "href": "ss24/alda/sum.html#week-1-4",
    "title": "Weekly Summary",
    "section": "Week 1",
    "text": "Week 1\n\nLecture 1\ndate:\n\n\nLecture 2\ndate:"
  },
  {
    "objectID": "ss24/alda/sum.html#week-1-5",
    "href": "ss24/alda/sum.html#week-1-5",
    "title": "Weekly Summary",
    "section": "Week 1",
    "text": "Week 1\n\nLecture 1\ndate:\n\n\nLecture 2\ndate:"
  },
  {
    "objectID": "ss24/alda/sum.html#week-1-6",
    "href": "ss24/alda/sum.html#week-1-6",
    "title": "Weekly Summary",
    "section": "Week 1",
    "text": "Week 1\n\nLecture 1\ndate:\n\n\nLecture 2\ndate:"
  },
  {
    "objectID": "ss24/alda/sum.html#week-1-7",
    "href": "ss24/alda/sum.html#week-1-7",
    "title": "Weekly Summary",
    "section": "Week 1",
    "text": "Week 1\n\nLecture 1\ndate:\n\n\nLecture 2\ndate:"
  },
  {
    "objectID": "ss24/oop/exam1.html",
    "href": "ss24/oop/exam1.html",
    "title": "My Uni Notes",
    "section": "",
    "text": "OOP for Scientific Computing First Exam\npoints:\n\n\n\n1\n2\n3\n4\n5\n\\(\\Sigma\\)\n\n\n\n\n15\n20\n10\n10\n10\n65\n\n\n\nFollowing was asked in the exam:\n\nExplain following aspects of C++ :\n\nclass vs struct?\nnamespace. What is it, when & why is it used?\nprivate vs protected?\ncopy elision?\nheader guards. What, why?\nrule of five vs fule of zero?\ntemporaries? literals?\nhow does a shared pointer work?\nSFINAE?\nSOLID?\n\nShort code snippets were given and asked to explain & extend. With following topics:\n\ndefault function args\nconcepts\ntemplate parameters, default template parameters\nConstructor, copy constructor, copy assignment operator, move constructor, move assignment operator, overloaded constructor.\nLambda expression, functional programming\nCompile time branching\nCRTP\nInheritence. How to improve the given implementation\n\nHorner Schema: Variadic templates, recursion with templates(?), template metaprogramming\nAn incomplete implementation of an OOP system was given that was supposed to implement a simulation a prey-predator dynamic system, modeled by the Lotka-Volterra differential equations:\n\\[\\begin{align*}\n&\\frac{dx}{dt} = \\alpha x - \\beta xy, \\\\\n&\\frac{dy}{dt} = \\gamma y - \\delta xy\n\\end{align*}\\]\nThe incomplete OOP system had the following basic structure:\n\n\n\nLotka-Volterra OOP System\n\n\n\nImplement the PreyPredatorData Simulate(int steps, double dt) that creates a PreyPredatorData object, populates its members with the simulation data based on the Lotka-Volterra difference equations and returns the object:\n\n\\[\\begin{align*}\n&X_{n + 1} = X_n + \\Delta t (\\alpha x_n - \\beta x_ny_n) \\\\\n&Y_{n + 1} = Y_n + \\Delta t (\\delta x_ny_n- \\gamma y_n)\n\\end{align*}\\]\n\n?\n\n?"
  },
  {
    "objectID": "ss24/oop/plan.html#reading-list",
    "href": "ss24/oop/plan.html#reading-list",
    "title": "Study Plan",
    "section": "Reading List",
    "text": "Reading List\n\nBasic, General, Intro:\n\nBig C++. Cay Horstmann\nProgramming Principles & Practice with C++. Stroustrup.\nA Tour of C++. Stroustrup\nC++ Crash Course. Josh Lospinoso\nC++ Primer. Lippmann\n\nModern, General, Scientific:\n\nDiscovering Modern C++ 2nd ed. Gottschild.\nModern C++ Programming Techniques for Scientific Computing - Lecture Notes. Ole Klein\n\nModern, Specific:\n\nFunctional Programming in C++. Cukic\nMove Semantics. Josuttis\nC++ Templates. Vandervoorde\nC++ STL. Josuttis\nOOP in C++. Josuttis"
  },
  {
    "objectID": "ss24/r/index.html",
    "href": "ss24/r/index.html",
    "title": "R Programming",
    "section": "",
    "text": "R Programming and its Applications to Stochastic",
    "crumbs": [
      "R Programming"
    ]
  },
  {
    "objectID": "ss24/r/index.html#section",
    "href": "ss24/r/index.html#section",
    "title": "R Programming",
    "section": "",
    "text": "weekly summary\nnotes\nsolutions",
    "crumbs": [
      "R Programming"
    ]
  },
  {
    "objectID": "ss24/time/index.html",
    "href": "ss24/time/index.html",
    "title": "Time Log",
    "section": "",
    "text": "calendar (with week numbers) pertaining to this semester:\nUsage: cal [general options] [-jy] [[month] year]\n       cal [general options] [-j] [-m month] [year]\n       ncal -C [general options] [-jy] [[month] year]\n       ncal -C [general options] [-j] [-m month] [year]\n       ncal [general options] [-bhJjpwySM] [-H yyyy-mm-dd] [-s country_code] [-W number of days] [[month] year]\n       ncal [general options] [-Jeo] [year]\nGeneral options: [-31] [-A months] [-B months] [-d yyyy-mm]\nTwo main tags:"
  },
  {
    "objectID": "ss24/time/index.html#weekly",
    "href": "ss24/time/index.html#weekly",
    "title": "Time Log",
    "section": "Weekly",
    "text": "Weekly\n\ntotal times:\n\n\n\n                 Total\n2024  Week 31   15h45m\n      Week 32   14h10m\n      Week 33    3h30m\n      Week 35    1h20m\n      Week 37    5h10m\n      Week 38    3h35m\n              ========\n                43h30m\n\n\n\nalgorithms and data structures (#alda) and oop for scientific computing (#sci-oop):\n\n\n\n                 Total                   Total\n2024  Week 31    7h40m  2024  Week 31     8h5m\n      Week 32   10h25m        Week 32    4h15m\n      Week 37    5h10m        Week 33    3h30m\n      Week 38    3h50m        Week 35    1h20m\n              ========                ========\n                 27h5m                  17h10m"
  },
  {
    "objectID": "ss24/time/index.html#daily",
    "href": "ss24/time/index.html#daily",
    "title": "Time Log",
    "section": "Daily",
    "text": "Daily\n\ntotal:\n\n\n\n                       Total\n2024 Jul    Tue 30.     5h5m\n            Wed 31.       5h\n     Aug    Thu  1.    4h10m\n            Fri  2.    1h30m\n            Mon  5.    2h30m\n            Tue  6.    5h35m\n            Wed  7.    3h25m\n            Thu  8.    2h40m\n            Tue 13.    1h35m\n            Wed 14.    1h55m\n            Mon 26.       1h\n            Tue 27.      20m\n     Sep    Tue 10.    2h30m\n            Fri 13.    2h40m\n            Mon 16.    3h35m\n                    ========\n                      43h30m\n\n\n\n#alda and #sci-oop side by side;\n\n\n\n                       Total                           Total\n2024 Jul    Tue 30.    1h15m    2024 Jul    Tue 30.    3h50m\n            Wed 31.       4h                Wed 31.       1h\n     Aug    Thu  1.    2h25m         Aug    Thu  1.    1h45m\n            Mon  5.    2h30m                Fri  2.    1h30m\n            Tue  6.    2h50m                Tue  6.    2h45m\n            Wed  7.    3h25m                Thu  8.    1h30m\n            Thu  8.    1h10m                Tue 13.    1h35m\n            Fri  9.      30m                Wed 14.    1h55m\n     Sep    Tue 10.    2h30m                Mon 26.       1h\n            Fri 13.    2h40m                Tue 27.      20m\n            Mon 16.    3h50m                        ========\n                    ========                          17h10m\n                       27h5m"
  },
  {
    "objectID": "ss24/time/index.html#entries",
    "href": "ss24/time/index.html#entries",
    "title": "Time Log",
    "section": "Entries",
    "text": "Entries\nTime log entries:\n\n\n\n2024-07-30 (6h!)\n#study #uni #exam\n    10:45 - 12:45 #sci-oop video reviews on moodle, Vid 1\n    -10m Break (#sci-oop)\n    14:00 - 15:45 #sci-oop video reviews on moodle, Vid 2\n    16:15 - 16:30 #sci-oop video reviews on moodle, Vid 2 \n        range based loops\n    16:30 - 17:45 #alda big #cpp sorting & serching\n\n2024-07-31 (6h!)\n#study #uni #exam\n    10:05 - 12:05 #alda big #cpp binary search ch 12\n    14:05 - 16:05 #alda big #cpp most frequent element ch 12\n        implemented most frequent element\n    16:35 - 17:35 #sci-oop move semantics ~ josuttis #cpp\n\n2024-08-01 (6h!)\n#study #uni #exam\n    10:35 - 13:15 #alda tutor notes, probleklausur\n    -15m Break (#alda)\n    14:45 - 16:45 #sci-oop move semantics ~ josuttis\n    -15m Break (#sci-oop)\n\n2024-08-02 (6h!)\n#study #uni #exam\n    10:45 - 12:45 #sci-oop move semantics ~ josuttis\n        finished ch 1.1\n    -30m Break (#sci-oop)\n\n2024-08-05 (6h!)\n#study #uni #exam\n    12:05 - 13:10 #alda big #cpp shell sort\n    -15m (#alda #cpp) Break\n    15:30 - 17:40 #alda big #cpp shell sort\n        implemented shell sort\n    -30m (#alda #cpp) Break\n\n2024-08-06\n    &lt;23:30 - 2:00 klog documentation\n    -45m Break & Distractions\n\n2024-08-06 (6h!)\n#study #uni #exam\n    9:45 - 11:15 #sci-oop move semantics \n        ch 1.2 - 1.5, 5p\n    11:35 - 12:50 #sci-oop move semantics ch 2\n        finished until summary chapter, 6p\n    14:45 - 15:35 #alda foundations of algorithms ~ neapolitan\n        preface & ch 1.0  \n    16:00 - 18:40 #alda foundations of algoroithms ~ neapolitan\n        ch 1.1 excluding matrix multiplication example\n    -40m #alda Break & Distractions\n\n2024-08-07 (5h30m!)\n#study #uni #exam\n    10:25 - 12:50 #alda foundations of algorithms ~ neapolitan\n        matrix multiplication example & ch 1.1\n        implemented & tested binary search\n    -20m #alda Break & Distractions\n    0m organized reading lists for ALDA and SCI-OOP \n        around 3.5 hours\n    18:00 - 19:20 #alda foundations of algorithms ~ neapolitan\n        ch1.2 fibonacci recursive and dynamic, impl and runtime\n\n2024-08-08 (4h!)\n#study #uni #exam\n    11:20 - 12:30 #alda implemented merge and mergesort\n        with proofs of correctness \n    0m sci-oop exam 14:20 - 16:00\n    17:00 - 18:30 #sci-oop typeset exam questions\n        and published them to the website\n\n2024-08-09\n    11:30 - 12:00 #alda typesetting first exam questions\n\n2024-08-13\n    10:10 - 12:05 quarto documentation, pdf options\n\n2024-08-13 (4h!)\n#study #uni #exam\n    12:25 - 13:20 #sci-oop josuttis #cpp move semantics\n        ch3.1 - Move semantics in ordinary classes \n    -10m Break #sci-oop #cpp\n    14:35 - 15:25 #sci-oop josuttis #cpp move semantics \n        ch3.2 - Implementing special copy/move member functions\n\n2024-08-13\n    19:15 - 20:15 #sports bodyshape fitness\n\n2024-08-14 (4h!)\n#study #uni #exam\n    10:45 - 12:55 #sci-oop josuttis #cpp move semantics \n        ch 3.3 - Special member functions\n    -15m Break #sci-oop #cpp\n\n2024-08-26 (3h30m!)\n#study #uni #exam\n    11:50 - 13:05 #sci-oop josuttis #cpp move semantics\n        revise ch 3\n    -15m Break #sci-oop #cpp\n\n2024-08-27 (3h!)\n#study #uni #exam\n    13:00 - 13:30 #sci-oop josuttis #cpp move semantics\n        ch 3.1.1 - \n    -10m Break #sci-oop #cpp\n\n2024-09-10 (5h!)\n#study #uni #exam\n    12:15 - 13:05 #alda gersting ch3 Recurrence\n        p. 157 - 159\n    15:30 - 17:30 #alda gersting ch3 recurrence relation\n        p. 159 - 163 until ex 7\n    -20m Break #alda\n\n2024-09-13 (4h!)\n#study #uni #exam\n    13:55 - 14:50 #alda gersting ch3 recurrence relations\n        p.163 - 165 \n    15:25 - 16:10 #alda gerstin ch3 recurrence relations\n        p.165 - 168 until Example 11\n    16:40 - 17:40 #alda \n        until p.169, implemented selection sort both iterative\n        and recursive versions\n\n2024-09-16 (4h!)\n#study #uni #exam\n    11:10 - 12:40 #alda gersting ch3 recurrence relations\n        p.169 - 171 implemented recursive & iterative binary search \n    -15m Break #alda\n    14:50 - 16:30 #alda gersting ch3 recurrence relations\n        p.171 - 171 Exercises 1, 2\n    -15m Break \n    17:00 - 17:55 #alda gersting ch3 recurrence relations\n        p.171 - 172 Exercises 2 - 12\n\n2024-09-20 (5h!)\n#self-study \n    10:35 - 13:30 #finance Beancount docs\n        command line accounting in context: custom scripting - end of chpater\n        finished chapter\n    -35m Break #finance \n    16:05 - 19:00 #finance Beancount docs\n        DE Counting Method Intro - types of accounts \n        read two chapters\n\n2024-09-24 (4h!)\n#self-study\n    12:10 - 13:40 #finance Beancount docs\n        DE Counting method: types of accounts - income statement\n    15:15 - 16:15 #finance Beancount docs\n        DE Counting method: clearing income - Balance Sheet \n    16:45 - 17:30 #finance Beancount docs\n        DE Coumting method: Summarizing - 17:30\n\n2024-09-30 (4h!)\n#self-study\n    11:45 - 12:45 #finance Beancount docs\n        DE Counting method: trial balance\n    15:15 - 16:00 #finance Beancount docs\n        DE Counting method: Balance Sheet\n    16:20 - 18:50 #finance Beancount docs\n        DE Counting method: Period Reporting - Chart of Accounts(excl)\n    -50m Break\n\n2024-10-01 (4h30m!)\n#self-study\n    10:25 - 11:40 Business and finance books glean\n    11:50 - 12:45 #finance Beancount docs\n        DE CM: Period reporting - plain-text accounting (excl)\n    14:10 - 14:40 #finance Beancount docs\n        DE CM: Plain-text accounting - the table perspective\n        Distracting a lot - gleaning productivity books"
  },
  {
    "objectID": "ss25/alda/sum/index.html",
    "href": "ss25/alda/sum/index.html",
    "title": "Weekly Summary",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nTitle\n\n\nDate\n\n\n\n\n\n\nWeek 1\n\n\nApr 15, 2025\n\n\n\n\nWeek 10\n\n\nJun 17, 2025\n\n\n\n\nWeek 11\n\n\nJun 23, 2025\n\n\n\n\nWeek 2\n\n\nApr 22, 2025\n\n\n\n\nWeek 3\n\n\nApr 29, 2025\n\n\n\n\nWeek 4\n\n\nMay 6, 2025\n\n\n\n\nWeek 5\n\n\nMay 13, 2025\n\n\n\n\nWeek 6\n\n\nMay 20, 2025\n\n\n\n\nWeek 7\n\n\nMay 27, 2025\n\n\n\n\nWeek 8\n\n\nJun 3, 2025\n\n\n\n\nWeek 9\n\n\nJun 10, 2025\n\n\n\n\n \n\n\n \n\n\n\n\n \n\n\n \n\n\n\n\n \n\n\n \n\n\n\n\n \n\n\n \n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Weekly Summary"
    ]
  },
  {
    "objectID": "ss25/alda/sum/w02.html",
    "href": "ss25/alda/sum/w02.html",
    "title": "Week 2",
    "section": "",
    "text": "review of previous lecture: data-structure triangle\nfor ADTs there’s lot’s of wiggle room for the concrete representation (the concrete bit sequences)\n\nexample: integers\n\n“unsigned” or “signed”\nthe number of allocated bits: uint8, int8 =&gt; uint16, int16 =&gt; uint32, int32 =&gt; uint64, int64 =&gt; infinite precision (python) (faster =&gt; slower). Python automatically switches to infinite precision if the numbers get too large.\n\nsmaller number of bits =&gt; overflow:\n\noverflow in uint8:\n\n\\(255_D = 1111,1111_B\\)\n\\(255_D + 1_D = 256_D = 1,0000,0000_B\\)\n\nHow to react to this? Solutions:\n\nIncrease the number of bits dynamically (python): not practical because not easy to implement in hardware.\nError warning: not practical because way too frequent.\nDon’t do anything: e.g.: \\(255_D + 1_D = 255_D\\): Associativity doesn’t hold anymore \\(255_D + (1_D - 1_D) = 255_D \\neq 254_D =\n255_D - 1_D = (255_D + 1_D) - 1_D\\)\n(Cyclical) Modulo arithmetic, k bits =&gt; modulo-\\(2^k\\) (% in python):\n\\(255_D + 1_D = 0_D\\)\nadvantages:\n\nvery efficient in hardware =&gt; simply perform the operations where the overflow is ignored\nalgebraic laws of basic operations still hold: associativity and commutativity\n\n\n\n\n\n\n\n\n\nElementary operations are ones that are defined for all data types:\n\nconstructor:\n\n\ncreate a new instance of a data type & allocate memory to it\nthe memory is initialized to a valid initial state. (otherwise we can’t be sure if operations performed on this object will deliver correct results)\nin python the constructor has the same name as the data type:\n\n\ni = int(); f = float(); a = list()\ni2 = int(7); f2 = float(10); b = [1, 2]\nprint(\"i:\", i, \"f:\", f, \"a: \", a)\nprint(\"i2:\", i2, \"f2:\", f2, \"b: \", b)\n\ni: 0 f: 0.0 a:  []\ni2: 7 f2: 10.0 b:  [1, 2]\n\n\nconstructors for custom-data types (classes): the function __init__(self, ...) has to be implemented.\n\n\n\ndestructor: deallocating the memory\ncomparison operator (==). Two different equalities:\n\nequal: same value / contents\nidentical: same object in the memory\ncomparing the identity of objects with id() or with is\n\na = [1, 2]\nb = [1, 2]\nc = b\nprint(\"1:\", id(a) == id(b))\nprint(\"2:\", id(c) == id(b))\nprint(\"3:\", a == b)\nprint(\"4:\", id(c) == id(b))\nprint(\"5:\", c == b and c == a)\n\nprint(\"6:\", a is b or a is c)\nprint(\"7:\", c is b)\n\n1: False\n2: True\n3: True\n4: True\n5: True\n6: False\n7: True\n\n\nc and b refer to the same memory location, different from the location of a. all of a, b, c have equal values; [1, 2].\nconsequense: value-semantics vs reference-semantics (see below)\n\n\n\n\nAnalogy: Accessing a website:\n\nvalue-semantics: store a copy of website.\n\nadvantage: we have control over the contents of the copy\ndisadvantage: possibly out-of-date.\n\nreference-semantics: store the URL of the website\n\nadvantage: always up-to-date\ndisadvantage: no control over the content; possibly deleted.\n\n\nWhat does it mean for programming languages, specifically for python:\n\nvalue-semantics: data is copied and stored at another location (== is True and is is False)\nreference-semantics: two varibles refer to the same location (both == and is are True)\nIn python all assignments use reference semantics, and in general python uses reference semantics\n\nExample\n\na = [1, 2]\nb = a\na[0] = -1\nprint(\"1:\", a, b);\nprint(\"2:\", a is b)\n\n1: [-1, 2] [-1, 2]\n2: True\n\n\nif we want to create another list object with a different identity but same value as a we use .copy() operator:\n\na = [1, 2]\nb = a.copy()\nprint(\"1:\", a == b)\nprint(\"2:\", a is b)\nb[0] = -1\nprint(\"3:\", a, b)\n\n1: True\n2: False\n3: [1, 2] [-1, 2]\n\n\nElementary data-types like numbers or booleans are immutable by default, which gives the ‘illusion’ of reference semantics.\n\na = 1\nb = a\nprint(\"1:\", a is b)\nprint(\"2:\", a == b)\nprint(\"3:\", a, b)\na = 2\nprint(\"4:\", a is b)\nprint(\"5:\", a == b)\nprint(\"6:\", a, b)\n\n1: True\n2: True\n3: 1 1\n4: False\n5: False\n6: 2 1",
    "crumbs": [
      "Weekly Summary",
      "Week 2"
    ]
  },
  {
    "objectID": "ss25/alda/sum/w02.html#vl-3---22.04.2025",
    "href": "ss25/alda/sum/w02.html#vl-3---22.04.2025",
    "title": "Week 2",
    "section": "",
    "text": "review of previous lecture: data-structure triangle\nfor ADTs there’s lot’s of wiggle room for the concrete representation (the concrete bit sequences)\n\nexample: integers\n\n“unsigned” or “signed”\nthe number of allocated bits: uint8, int8 =&gt; uint16, int16 =&gt; uint32, int32 =&gt; uint64, int64 =&gt; infinite precision (python) (faster =&gt; slower). Python automatically switches to infinite precision if the numbers get too large.\n\nsmaller number of bits =&gt; overflow:\n\noverflow in uint8:\n\n\\(255_D = 1111,1111_B\\)\n\\(255_D + 1_D = 256_D = 1,0000,0000_B\\)\n\nHow to react to this? Solutions:\n\nIncrease the number of bits dynamically (python): not practical because not easy to implement in hardware.\nError warning: not practical because way too frequent.\nDon’t do anything: e.g.: \\(255_D + 1_D = 255_D\\): Associativity doesn’t hold anymore \\(255_D + (1_D - 1_D) = 255_D \\neq 254_D =\n255_D - 1_D = (255_D + 1_D) - 1_D\\)\n(Cyclical) Modulo arithmetic, k bits =&gt; modulo-\\(2^k\\) (% in python):\n\\(255_D + 1_D = 0_D\\)\nadvantages:\n\nvery efficient in hardware =&gt; simply perform the operations where the overflow is ignored\nalgebraic laws of basic operations still hold: associativity and commutativity\n\n\n\n\n\n\n\n\n\nElementary operations are ones that are defined for all data types:\n\nconstructor:\n\n\ncreate a new instance of a data type & allocate memory to it\nthe memory is initialized to a valid initial state. (otherwise we can’t be sure if operations performed on this object will deliver correct results)\nin python the constructor has the same name as the data type:\n\n\ni = int(); f = float(); a = list()\ni2 = int(7); f2 = float(10); b = [1, 2]\nprint(\"i:\", i, \"f:\", f, \"a: \", a)\nprint(\"i2:\", i2, \"f2:\", f2, \"b: \", b)\n\ni: 0 f: 0.0 a:  []\ni2: 7 f2: 10.0 b:  [1, 2]\n\n\nconstructors for custom-data types (classes): the function __init__(self, ...) has to be implemented.\n\n\n\ndestructor: deallocating the memory\ncomparison operator (==). Two different equalities:\n\nequal: same value / contents\nidentical: same object in the memory\ncomparing the identity of objects with id() or with is\n\na = [1, 2]\nb = [1, 2]\nc = b\nprint(\"1:\", id(a) == id(b))\nprint(\"2:\", id(c) == id(b))\nprint(\"3:\", a == b)\nprint(\"4:\", id(c) == id(b))\nprint(\"5:\", c == b and c == a)\n\nprint(\"6:\", a is b or a is c)\nprint(\"7:\", c is b)\n\n1: False\n2: True\n3: True\n4: True\n5: True\n6: False\n7: True\n\n\nc and b refer to the same memory location, different from the location of a. all of a, b, c have equal values; [1, 2].\nconsequense: value-semantics vs reference-semantics (see below)\n\n\n\n\nAnalogy: Accessing a website:\n\nvalue-semantics: store a copy of website.\n\nadvantage: we have control over the contents of the copy\ndisadvantage: possibly out-of-date.\n\nreference-semantics: store the URL of the website\n\nadvantage: always up-to-date\ndisadvantage: no control over the content; possibly deleted.\n\n\nWhat does it mean for programming languages, specifically for python:\n\nvalue-semantics: data is copied and stored at another location (== is True and is is False)\nreference-semantics: two varibles refer to the same location (both == and is are True)\nIn python all assignments use reference semantics, and in general python uses reference semantics\n\nExample\n\na = [1, 2]\nb = a\na[0] = -1\nprint(\"1:\", a, b);\nprint(\"2:\", a is b)\n\n1: [-1, 2] [-1, 2]\n2: True\n\n\nif we want to create another list object with a different identity but same value as a we use .copy() operator:\n\na = [1, 2]\nb = a.copy()\nprint(\"1:\", a == b)\nprint(\"2:\", a is b)\nb[0] = -1\nprint(\"3:\", a, b)\n\n1: True\n2: False\n3: [1, 2] [-1, 2]\n\n\nElementary data-types like numbers or booleans are immutable by default, which gives the ‘illusion’ of reference semantics.\n\na = 1\nb = a\nprint(\"1:\", a is b)\nprint(\"2:\", a == b)\nprint(\"3:\", a, b)\na = 2\nprint(\"4:\", a is b)\nprint(\"5:\", a == b)\nprint(\"6:\", a, b)\n\n1: True\n2: True\n3: 1 1\n4: False\n5: False\n6: 2 1",
    "crumbs": [
      "Weekly Summary",
      "Week 2"
    ]
  },
  {
    "objectID": "ss25/alda/sum/w02.html#vl-4---24.04.2025",
    "href": "ss25/alda/sum/w02.html#vl-4---24.04.2025",
    "title": "Week 2",
    "section": "VL 4 - 24.04.2025",
    "text": "VL 4 - 24.04.2025\n\nContainers - Data types\n\nContainers contain other data.\nEfficient & systematic managmeent of large amounts of data\nEfficient repeated application of the same operation\nEfficient & simple querrying (seeking) of data\nData structures are constructed hierarchically in all programming languages. (composite) Ex.: hierachically composing new structures from simple (atomic) ones\n\nnatural numbers: atomic data type\n3 numbers: RGBValue\n1024 x 1024 RGBValue: Image\nSequence of Images: Video\n\n\n\nADT Array\nArray (list in python) is the most important / fundamental container data type:\n\nOperation (pseudocode): a = new_array(size, initial_value)\n\ninterpretation: create an array with size elements all of which initially have initial_value as value\naxioms:\n\nprecondition: -\npostcondition: len(a) == size and for all i in [0,..., size - 1]: get(a, i) == initial_value\n\npython:\n\na = list() # empty array with size == 0\nb = [3] * 10\nc = [-1, 2, 3, 'c']\nprint(a, b, c, sep=\"\\n\")\n\n[]\n[3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n[-1, 2, 3, 'c']\n\n\n\nOperation (pseudocode): k = len(a)\n\ninterpretation: number of elements in a\naxioms:\n\nprecondition:\npostcondition: k == size\n\npython:\n\na = [None] * 10\nlen(a)\n\n10\n\n\n\nOperation (pseudocode): v = get(a, i )\n\ninterpretation: get the element at the position i\naxioms:\n\nprecondition: 0 &lt;= i &lt;= size - 1 and v == element of a at position i\npostcondition: len(a) == size and for all i in [0,..., size - 1]: get(a, i) == initial_value\n\npython:\n\na = [1, 2, 3]\nv = a.__getitem__(2)\nv = a[2]\nprint(a)\n\n[1, 2, 3]\n\n\n\nOperation (pseudocode): set(a, i, v)\n\ninterpretation:\naxioms:\n\nprecondition: 0 &lt;= i &lt;= size - 1\npostcondition: get(a, i) == v\n\npython:\n\na = [-1, 2, 3]\na.__setitem__(1, 'c')\na[2] = 'd'\nprint(a)\n\n[-1, 'c', 'd']\n\n\n\nArrays store their elements consequtively =&gt; more efficient than other container types, since CPU’s process consecutive memeory cells faster.\npython allows i &lt; 0: -size &lt;= i &lt;= size - 1. Then:\n\nAxiom: i &lt; 0 =&gt; get(a, i) == get(a, len(a) - |i|)\n\n\nDyanmic arrays: here we simply give the operations\n\noperation: append(a, v)\npython: a.append(v) # efficient in pyton\noperation: append(a, other_array)\npython: a.extend(other_array) # efficient\noperation: insert(a, i, v) inserts and elements after position i\npython: a.insert(i) # less efficient\noperation: pop(a) (remove last eelment)\npython: a.pop() # efficient in python\noperation: remove(a, i) (remove the ith element)\npython: a.pop(i) or del a[i]\noperation: clear(a) delete everything\npython: a.clear() # now len(a) == 0\n\n\n\nAssociative Array\nAssociative arrays are called dictionary in python.\n\nMain idea of assoc. arrays is that they allow arbitrary data structures as index.\nEx.:\n\nImmatriculation number as index: students[1234567] =&gt; \"Alice Smith\"\nStrings as index, e.g. Name: `alda_grade[‘Bob Miller’]\nIn general: any data type that implements a hash function can be an nidex\n\nAssociative arrays are internally implemented as a dynamic array, where the index is computed by the hash function.\n\n\n\nStack\nIn python lists are automatically also stacks. (They can be used as stacks)\nonly the operations and python versions:\n\nOperation: top()\npython: s[-1]\n\nOperation: top()\npython: s[-1]\n\nOperation: pop()\npython: s.pop()\n\nOperation: push(s, v)\npython: s.append(v)",
    "crumbs": [
      "Weekly Summary",
      "Week 2"
    ]
  },
  {
    "objectID": "ss25/alda/sum/w04.html",
    "href": "ss25/alda/sum/w04.html",
    "title": "Week 4",
    "section": "",
    "text": "principle:\n\nsorintg from left to write (left to the current position is all sorted)\nselect the appropriate element in the right-side of the current position.\n\n\ndef selection_sort(a) :\n    N = len(a)\n    # invariant: sorted(a[0..i-1]) and a[0..i-1] &lt;= a[i..N-1]\n    for i in range(N - 1) :  # iteration\n        j = i \n        # k = i + 1\n        # invariant: a[j] == min(a[i..k-1])\n        for k in range(i + 1, N) : \n            if a[k] &lt; a[j] : j = k\n        a[i], a[j] = a[j], a[i]\n\na = [3, -1, 5, 10]\nselection_sort(a)\nprint(a)\n\n[-1, 3, 5, 10]\n\n\n\n\nHow many steps does selection sort take?\n\nobviously depends on N.\nestimating the runtime: \\(L(N) \\approx c \\cdot f(N)\\), where \\(f(N)\\) is some simple function (the formally correct versiono later with the \\(\\mathcal{O}(N)\\) notation)\n\nThe inner loop compares a[k] &lt; a[j] for each k in range(i + 1, N), so the number of comparisons decreases as i increases. Here’s the completed table based on the structure of the nested loops:\n\n\n\ni\nk\nnumber of comparisons\n\n\n\n\n0\n1 … N - 1\nN - 1\n\n\n1\n2 … N - 1\nN - 2\n\n\n2\n3 … N - 1\nN - 3\n\n\n…\n…\n…\n\n\nN-3\nN-2 … N - 1\n2\n\n\nN-2\nN - 1\n1\n\n\nN-1\n— (no iteration)\n0\n\n\n\nTotal number of comparisons:\nTo find the overall time complexity, sum all the comparisons:\n\\[\n(N - 1) + (N - 2) + \\dots + 1 = \\frac{N(N - 1)}{2}\n\\]\nSo, the time complexity of selection sort is O(N²) in the worst, average, and best cases (it always does the same number of comparisons).\nHow to make sorting functions generic so that they can be provided in libraries? \\(\\Rightarrow\\) API (Application Programming Interface), so that the user can choose the sorting criteria the following way:\n\ndef selection_sort2(a, key = lambda x : x): \n    N = len(a)\n    # i = 0\n    # invariant sorted(a[0..i]) and a[0..i-1] &lt;= a[]\n    for i in range(N - 1) :\n        m = i\n        for  k in range(i + 1, N) :\n            if key(a[k]) &lt; key(a[m]): m = k\n        a[i], a[m] = a[m], a[i]\n\nAssume we have an array that contains tuples of students:\n\nstudents = [(\"andrej\", 20, 1.3),\n            (\"igor\", 22, 2.7),\n            (\"olga\", 21, 1.7)]\n\nThen, the following way we could sort on different criteria:\n\nselection_sort2(students, lambda x : x[0]) # sort w.r.t. names\nprint(students)\nselection_sort2(students, lambda x : x[1]) # sort w.r.t. age\nprint(students)\nselection_sort2(students, lambda x : x[2]) # sort w.r.t. grade\nprint(students)\n\n[('andrej', 20, 1.3), ('igor', 22, 2.7), ('olga', 21, 1.7)]\n[('andrej', 20, 1.3), ('olga', 21, 1.7), ('igor', 22, 2.7)]\n[('andrej', 20, 1.3), ('olga', 21, 1.7), ('igor', 22, 2.7)]\n\n\n\n\n\n\nStable sorting preserves the original order of elements with equal keys, which is especially useful when sorting complex objects.\n\nExample: Given a = [(\"Abel\", 20), (\"Babel\", 20), (\"Frey\", 19)], initially sorted by name, sorting again by age using sort(a, key=lambda x: x[1]) yields a = [(\"Frey\", 19), (\"Abel\", 20), (\"Babel\", 20)].\n\nThe idea behind stable sorting is that you can first sort by a secondary criterion (e.g., names), then by a primary one (e.g., grades), and the final result will retain the secondary order within groups that share the same primary key. Without stability, you’d need to re-sort each group manually.\n\n\n\n\ninsertion sort is stable\ninsertion sort is the fastest algorithm for small N.\nPrinciple:\n\nsorts from left to right.\ncreate a hole in the correct position for the element a[i]\n\n\n\ndef insertion_sort(a):\n    N = len(a)\n    # i = 1\n    # sorted(a[0..i-1])\n    for i in range(1, N):\n        k = i\n        while k &gt; 0:\n            if a[k] &lt; a[k - 1]: \n                a[k - 1], a[k] = a[k], a[k - 1]\n            else: break\n            k = k - 1\n\na = [1, -3, 4, 10, -99]\ninsertion_sort(a)\nprint(a)\n\n[-99, -3, 1, 4, 10]\n\n\n\n\nRuntime of insertion sort depends on whether the array was sorted before hand. (In selection sort run time is always the same formula)\n\nCase 1(Best Case): array is already sorted \\(\\Rightarrow \\mathcal{O}(n)\\) , since inner while loop always executes the break statement immediately.\nCase 2 (Worst case): array is reverse sorted \\(\\Rightarrow \\mathcal{O}(n^2)\\) , since each inner while loop always executes the whole range of \\(i\\)s.\nCase 3 (Average case): Array is randomly sorted, i.e. intuitively somewhere in between sorted and reversly sorted. In this case the inner while loop breaks after \\(\\frac{i}{2}\\). Then the complexity can be written as\n\\[T(N) = \\frac{c}{2}(1 + 2 + \\ldots + (N - 1)) \\in \\mathcal{O}(N^2)\\]\n\n\n\n\n\n\nfaster sorter algorithms that run in \\(\\mathcal{O}(N\\cdot\\log{N})\\) and \\(\\mathcal{O}(N)\\) (bucket sort).\nsuch algoriths are faster because they employ the “divide and conquer” principle (recursion):\n\ndivide the problem to smaller sub-problems and solve the smaller problems\ncombine the smaller solutions\n\ntwo classic exmples of such algorithms:\n\nmerge sort:\n\ndivide the array in two equal parts\nsorts the half errays\nmerge the sorted half arrays\n\nquick sort:\n\nchoose a pivot element\ndivide the array into “smaller than pivot” and “greater than pivot”\nsort the individual parts.",
    "crumbs": [
      "Weekly Summary",
      "Week 4"
    ]
  },
  {
    "objectID": "ss25/alda/sum/w04.html#vl-7---06.05.25",
    "href": "ss25/alda/sum/w04.html#vl-7---06.05.25",
    "title": "Week 4",
    "section": "",
    "text": "principle:\n\nsorintg from left to write (left to the current position is all sorted)\nselect the appropriate element in the right-side of the current position.\n\n\ndef selection_sort(a) :\n    N = len(a)\n    # invariant: sorted(a[0..i-1]) and a[0..i-1] &lt;= a[i..N-1]\n    for i in range(N - 1) :  # iteration\n        j = i \n        # k = i + 1\n        # invariant: a[j] == min(a[i..k-1])\n        for k in range(i + 1, N) : \n            if a[k] &lt; a[j] : j = k\n        a[i], a[j] = a[j], a[i]\n\na = [3, -1, 5, 10]\nselection_sort(a)\nprint(a)\n\n[-1, 3, 5, 10]\n\n\n\n\nHow many steps does selection sort take?\n\nobviously depends on N.\nestimating the runtime: \\(L(N) \\approx c \\cdot f(N)\\), where \\(f(N)\\) is some simple function (the formally correct versiono later with the \\(\\mathcal{O}(N)\\) notation)\n\nThe inner loop compares a[k] &lt; a[j] for each k in range(i + 1, N), so the number of comparisons decreases as i increases. Here’s the completed table based on the structure of the nested loops:\n\n\n\ni\nk\nnumber of comparisons\n\n\n\n\n0\n1 … N - 1\nN - 1\n\n\n1\n2 … N - 1\nN - 2\n\n\n2\n3 … N - 1\nN - 3\n\n\n…\n…\n…\n\n\nN-3\nN-2 … N - 1\n2\n\n\nN-2\nN - 1\n1\n\n\nN-1\n— (no iteration)\n0\n\n\n\nTotal number of comparisons:\nTo find the overall time complexity, sum all the comparisons:\n\\[\n(N - 1) + (N - 2) + \\dots + 1 = \\frac{N(N - 1)}{2}\n\\]\nSo, the time complexity of selection sort is O(N²) in the worst, average, and best cases (it always does the same number of comparisons).\nHow to make sorting functions generic so that they can be provided in libraries? \\(\\Rightarrow\\) API (Application Programming Interface), so that the user can choose the sorting criteria the following way:\n\ndef selection_sort2(a, key = lambda x : x): \n    N = len(a)\n    # i = 0\n    # invariant sorted(a[0..i]) and a[0..i-1] &lt;= a[]\n    for i in range(N - 1) :\n        m = i\n        for  k in range(i + 1, N) :\n            if key(a[k]) &lt; key(a[m]): m = k\n        a[i], a[m] = a[m], a[i]\n\nAssume we have an array that contains tuples of students:\n\nstudents = [(\"andrej\", 20, 1.3),\n            (\"igor\", 22, 2.7),\n            (\"olga\", 21, 1.7)]\n\nThen, the following way we could sort on different criteria:\n\nselection_sort2(students, lambda x : x[0]) # sort w.r.t. names\nprint(students)\nselection_sort2(students, lambda x : x[1]) # sort w.r.t. age\nprint(students)\nselection_sort2(students, lambda x : x[2]) # sort w.r.t. grade\nprint(students)\n\n[('andrej', 20, 1.3), ('igor', 22, 2.7), ('olga', 21, 1.7)]\n[('andrej', 20, 1.3), ('olga', 21, 1.7), ('igor', 22, 2.7)]\n[('andrej', 20, 1.3), ('olga', 21, 1.7), ('igor', 22, 2.7)]\n\n\n\n\n\n\nStable sorting preserves the original order of elements with equal keys, which is especially useful when sorting complex objects.\n\nExample: Given a = [(\"Abel\", 20), (\"Babel\", 20), (\"Frey\", 19)], initially sorted by name, sorting again by age using sort(a, key=lambda x: x[1]) yields a = [(\"Frey\", 19), (\"Abel\", 20), (\"Babel\", 20)].\n\nThe idea behind stable sorting is that you can first sort by a secondary criterion (e.g., names), then by a primary one (e.g., grades), and the final result will retain the secondary order within groups that share the same primary key. Without stability, you’d need to re-sort each group manually.\n\n\n\n\ninsertion sort is stable\ninsertion sort is the fastest algorithm for small N.\nPrinciple:\n\nsorts from left to right.\ncreate a hole in the correct position for the element a[i]\n\n\n\ndef insertion_sort(a):\n    N = len(a)\n    # i = 1\n    # sorted(a[0..i-1])\n    for i in range(1, N):\n        k = i\n        while k &gt; 0:\n            if a[k] &lt; a[k - 1]: \n                a[k - 1], a[k] = a[k], a[k - 1]\n            else: break\n            k = k - 1\n\na = [1, -3, 4, 10, -99]\ninsertion_sort(a)\nprint(a)\n\n[-99, -3, 1, 4, 10]\n\n\n\n\nRuntime of insertion sort depends on whether the array was sorted before hand. (In selection sort run time is always the same formula)\n\nCase 1(Best Case): array is already sorted \\(\\Rightarrow \\mathcal{O}(n)\\) , since inner while loop always executes the break statement immediately.\nCase 2 (Worst case): array is reverse sorted \\(\\Rightarrow \\mathcal{O}(n^2)\\) , since each inner while loop always executes the whole range of \\(i\\)s.\nCase 3 (Average case): Array is randomly sorted, i.e. intuitively somewhere in between sorted and reversly sorted. In this case the inner while loop breaks after \\(\\frac{i}{2}\\). Then the complexity can be written as\n\\[T(N) = \\frac{c}{2}(1 + 2 + \\ldots + (N - 1)) \\in \\mathcal{O}(N^2)\\]\n\n\n\n\n\n\nfaster sorter algorithms that run in \\(\\mathcal{O}(N\\cdot\\log{N})\\) and \\(\\mathcal{O}(N)\\) (bucket sort).\nsuch algoriths are faster because they employ the “divide and conquer” principle (recursion):\n\ndivide the problem to smaller sub-problems and solve the smaller problems\ncombine the smaller solutions\n\ntwo classic exmples of such algorithms:\n\nmerge sort:\n\ndivide the array in two equal parts\nsorts the half errays\nmerge the sorted half arrays\n\nquick sort:\n\nchoose a pivot element\ndivide the array into “smaller than pivot” and “greater than pivot”\nsort the individual parts.",
    "crumbs": [
      "Weekly Summary",
      "Week 4"
    ]
  },
  {
    "objectID": "ss25/alda/sum/w04.html#vl-8---06.05.25",
    "href": "ss25/alda/sum/w04.html#vl-8---06.05.25",
    "title": "Week 4",
    "section": "VL 8 - 06.05.25",
    "text": "VL 8 - 06.05.25\n\nEfficient Sorting Algorithms (Cont.)\n\nfor small values, \\(N &lt; 30\\) insertion sort is the fastest algorithm\nfor larger \\(N\\) efficient sorting algorithms must be preffered: merge sort and quicksort (divide & conquer)\n\n\nMerge Sort\n\nDivide the array into two equal parts\nsort the half arrays\nmerge the sorted sub parts\n\n\ndef merge_sorted_arrays(l, r): # both l and r are sorted\n  a = [] # sorted array\n  i, k = 0, 0\n  Nl, Nr = len(l), len(r)\n  # a == merged(l[0:i], r[0:k]\n  while i &lt; Nl and k &lt; Nr: \n    if l[i] &lt;= r[k]: # &lt;= instead of &lt; so that sorting is stablef\n      a.append(l[i]) # append is efficient\n      i += 1\n    else:\n      a.append(r[k])\n      k += 1\n  a += l[i : Nl] # append rest of l\n  a += r[k : Nr] # append rest of r\n  return a\n\nl = [1, 3, 10, 11]\nr = [-1, 2, 20]\n\na = merge_sorted_arrays(l, r)\nprint(a)\n\n[-1, 1, 2, 3, 10, 11, 20]\n\n\nthe final step\na += l[i : Nl] # append rest of l\na += r[k : Nr] # append rest of r\nis enabled by the elegant python slicing syntax. The usual way without slicing would be:\nwhile i &lt; Nl:\n  a.append(l[i])\n  i += 1\nwhile k &lt; Nr:\n  a.append(r[k])\n  k += 1\nNow we define merge sort recursively as follows:\n\ndef merge_sort(a):\n  N = len(a)\n  if N &lt;= 1: return a\n  l = merge_sort(a[: N//2])\n  r = merge_sort(a[N//2:])\n  return merge_sorted_arrays(l, r)\n\na = list(range(20))\nrandom.shuffle(a)\nprint(a)\na = merge_sort(a)\nprint(a)\n\n[15, 18, 12, 1, 2, 16, 14, 4, 19, 8, 11, 3, 5, 9, 13, 6, 7, 10, 0, 17]\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n\n\n\nDisadvantage: Merge sort is not in-place \\(\\Rightarrow\\) extra memory is necessary for each merge sort call\nAdvantage: Merge sort is efficient and stable\n\n\nRun-time Analysis of Merge Sort\nRuntime of merge sort satisfies the following formula both in worst and best case:\n\\[\\begin{align*}\n&T(0 || 1) = c_1 \\\\\n&T(N) = c_2 + 2\\cdot T(\\frac{N}{2}) + c_3N\n\\end{align*}\\]\nTODO: tldr diagram and solution of the recurrence relation.\n\n\n\nQuicksort\n\nIn practice somewhat faster than merge sort (with a good choice of pivot)\nnot stable but in place.\nidea:\n\nchoose an element of the array as “pivot”. (naiv, better is to choose pivot randomly)\npartition the array: (incomplete sorting)\n\npivot is at the final position.\nall elements &lt;= pivot are left to the pivot\nall elements &gt; pivot are right the pivot\nthe left and right portions are not sorted\n\ncall quicksort recursively on the left and right partitions (divide and conquer)\n\n\nfirst we define the paritioning function:\n\ndef partition(a, l, r):  # l, r are left and right boundaries (inclusive) of a\n  pivot = a[r] # naive choice: Pivot\n  i = l\n  k = r - 1\n  while True:\n    while i &lt; r and a[i] &lt;= pivot: i += 1 # increment until found a misplaced element\n    while k &gt; l and a[k] &gt;= pivot: k -= 1 # decrement until found a misplaced element\n    if i &lt; k : a[i], a[k] = a[k], a[i]\n    else : break\n  a[i], a[r] = a[r], a[i] # bring pivot to the correct position\n  return i # so that recursive calls now where the partitions are\n\nnext we define\n\ndef quick_sort_impl(a, l, r):\n  if r &lt;= l: return # no return argument since in-place\n  i = partition(a, l, r)\n  quick_sort_impl(a, l, i-1)\n  quick_sort_impl(a, i+1, r)\n\nfinally we define a wrapper function as an interface for the user:\n\ndef quick_sort(a):\n  quick_sort_impl(a, 0, len(a) - 1)\n\na = list(range(20))\nrandom.shuffle(a)\nprint(a)\nquick_sort(a)\nprint(a)\n\n[9, 1, 8, 19, 15, 13, 12, 0, 17, 18, 14, 5, 3, 11, 6, 4, 10, 16, 2, 7]\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n\n\nWhy choosing pivot as the first element is naive?\n\nif pivot always stays on the first position \\(\\Rightarrow\\) \\(N^2\\). This happens if the arrray is already sorted (TODO: understand thus)\na naive solution: shuffle the array before sorting (of course doesn’t make much sense) \\(\\Rightarrow\\) uquivalently: choose the pivot randomly each time.\nimport random\np = random.randint(l, r) \na[p], a[r] = a[r], a[p] # replace right-most element with this random element\npivot = a[r]",
    "crumbs": [
      "Weekly Summary",
      "Week 4"
    ]
  },
  {
    "objectID": "ss25/alda/sum/w06.html",
    "href": "ss25/alda/sum/w06.html",
    "title": "Week 6",
    "section": "",
    "text": "this is the second most important property of an algorithm after correctness.\nTwo aspects of efficiency:\n\n\n\n\nrun-time: wall-clock time, i.e. actual time. important for end-user, must be sufficiently fast depending on the requirements. Can be measured with tools like timeit for python of Google benchmark for c/c++.\n\nimproving runtime:\n\nin interpreted languages like python the time critical portions can be delegated to C/C++/Fortran,(think CUDA an vectorization) can also be used for own code via\n\ncython: a python subset. .pyc \\(\\Rightarrow\\)\n\n.so: shared libraries in linux / macos\n.pyd: shared libraries in Windows\n\npybind11: binds c++\n\nvectorization: very importane technique; instead of repeatedly evaluating (interpreting) the body of the loop, it is delegated to a c function, prevent the intrepretation overhead:\na = b * c\ninstead of\nfor i in rage(N): a[i] = b[i] * c[i]\n\nin compiled languages the optimizatin is performed once during the compilation and modern compilers are very good at it:\n\nreorganize the execution order\n\nCPU pipeline:\n\nDecoding a commnad\nretrive Data\nexecute the command\nwrite the results to memory 2nd step can cuase bottlenecks to the pipeline \\(\\Rightarrow\\) caches. The optimizer tries to reorder the commands s.t. the result is not changed and bottlenecks are reduced.\n\n\ncomputations that are needed in multiple places can be perfomred once by the compiler.\n\n\n\n\n\n\n\n\n\nruntime depends on concrete hardware\nabstracting away from the hardware details and actual runtime, we turn our attention to general, hardware independent statements about the approximate number of atomic operations. this simplifies the problem a lot.\nO-notation:\n\n\n\nruntime\n\n\nfor all \\(N \\geq N_0\\): \\(f(N) \\leq c\\cdot g(N)\\), for some \\(c\\).\ndefinition:\n\\[\\mathcal{O}(g(N)) = \\{f(N) : \\exists c, N_0 \\text{ s.t. } f(N) \\leq c\\cdot g(N), \\forall N \\geq N_0\\}\\]\n\\(\\mathcal{O}(\\bullet)\\) is analgous to \\(\\leq\\), i.e. \\(f \\in \\mathcal{O}(g) \\sim f\\leq g\\), and it indeed satisfies the axioms of a partial order:\n\ntransitivity: f = O(g) and g = O(h) =&gt; f = O(h)\nreflexivity: f = O(f)\n\n\n\n\n\n\n\\(f(n) = O(g(n))\\) means:\n\\[\n\\exists\\, c &gt; 0,\\, n_0 \\in \\mathbb{N} : \\forall n \\geq n_0,\\, |f(n)| \\leq c \\cdot |g(n)|\n\\]\n\nWe assume all functions are eventually non-negative, or we use absolute values for generality.\n\n\n\n\n\n\\[\nf(n) = O(f(n))\n\\]\n\n\n\nTake \\(c = 1\\), then for all \\(n\\),\n\\[\n|f(n)| \\leq 1 \\cdot |f(n)|\n\\]\nSo the condition for \\(O(f(n))\\) is satisfied with \\(n_0 = 0\\).\n\n\n\n\n\n\n\nIf \\(f = O(g)\\) and \\(g = O(h)\\), then \\(f = O(h)\\)\n\n\n\n\n\\(f(n) \\leq c_1 \\cdot g(n)\\) for all \\(n \\geq n_1\\)\n\\(g(n) \\leq c_2 \\cdot h(n)\\) for all \\(n \\geq n_2\\)\n\nThen for \\(n \\geq \\max(n_1, n_2)\\):\n\\[\nf(n) \\leq c_1 \\cdot g(n) \\leq c_1 c_2 \\cdot h(n)\n\\]\nSo \\(f = O(h)\\) with constant \\(c = c_1 c_2\\), and cutoff \\(n_0 = \\max(n_1, n_2)\\).\n\n\n\n\n\n\nIf \\(f = O(h)\\) and \\(g = O(h)\\), then \\(f + g = O(h)\\)\n\n\n\n\n\\(f(n) \\leq c_1 \\cdot h(n)\\) for \\(n \\geq n_1\\)\n\\(g(n) \\leq c_2 \\cdot h(n)\\) for \\(n \\geq n_2\\)\n\nThen for \\(n \\geq \\max(n_1, n_2)\\):\n\\[\nf(n) + g(n) \\leq (c_1 + c_2) \\cdot h(n)\n\\]\nSo \\(f + g = O(h)\\) with constant \\(c = c_1 + c_2\\)\n\n\n\n\n\n\n\nIf \\(f = O(g)\\), then \\(c \\cdot f = O(g)\\) for any \\(c &gt; 0\\)\n\n\n\n\n\\(f(n) \\leq c_1 \\cdot g(n)\\) for \\(n \\geq n_0\\)\nThen:\n\n\\[\nc \\cdot f(n) \\leq c \\cdot c_1 \\cdot g(n)\n\\]\nSo \\(c \\cdot f = O(g)\\) with constant \\(c' = c \\cdot c_1\\)\n\n\n\n\n\n\n\n\\[\nf(n) + g(n) = O(\\max(f(n), g(n)))\n\\]\n\n\n\nFor all \\(n\\):\n\\[\nf(n) + g(n) \\leq 2 \\cdot \\max(f(n), g(n))\n\\]\nSo the sum is at most twice the maximum. Thus:\n\\[\nf + g = O(\\max(f, g))\n\\]\n\n\n\n\n\n\n\nIf \\(f = O(h)\\) and \\(g = O(k)\\), then \\(f \\cdot g = O(h \\cdot k)\\)\n\n\n\n\n\\(f(n) \\leq c_1 \\cdot h(n)\\), \\(g(n) \\leq c_2 \\cdot k(n)\\)\nThen:\n\n\\[\nf(n) \\cdot g(n) \\leq c_1 c_2 \\cdot h(n) \\cdot k(n)\n\\]\nSo \\(f \\cdot g = O(h \\cdot k)\\)\n\n\n\n\n\n\n\nIf \\(f(n) \\leq g(n)\\) eventually, and \\(g = O(h)\\), then \\(f = O(h)\\)\n\n\n\n\n\\(g(n) \\leq c \\cdot h(n)\\), and \\(f(n) \\leq g(n)\\)\nThen:\n\n\\[\nf(n) \\leq g(n) \\leq c \\cdot h(n)\n\\]\nSo \\(f = O(h)\\)\n\n\n\n\n\n\n\nIf \\(f(n) = O(g(n))\\), and \\(h(n) = an + b\\), then:\n\\[\nf(h(n)) = O(g(h(n)))\n\\]\n\n\n\n\n\\(f(n) \\leq c \\cdot g(n)\\) for \\(n \\geq n_0\\)\nSince \\(h(n) \\to \\infty\\) as \\(n \\to \\infty\\), there exists \\(n_1\\) such that \\(h(n) \\geq n_0\\)\nThen:\n\n\\[\nf(h(n)) \\leq c \\cdot g(h(n)) \\quad \\text{for } n \\geq n_1\n\\]\nSo \\(f \\circ h = O(g \\circ h)\\)\n\n\n\n\n\n\n\n\n\n\n\nProperty\nStatement\n\n\n\n\nReflexivity\n\\(f = O(f)\\)\n\n\nTransitivity\n\\(f = O(g), g = O(h) \\Rightarrow f = O(h)\\)\n\n\nAdditivity\n\\(f = O(h), g = O(h) \\Rightarrow f + g = O(h)\\)\n\n\nScalar Multiplication\n\\(f = O(g) \\Rightarrow c \\cdot f = O(g)\\)\n\n\nMax-Dominance\n\\(f + g = O(\\max(f, g))\\)\n\n\nMultiplicativity\n\\(f = O(h), g = O(k) \\Rightarrow fg = O(hk)\\)\n\n\nMonotonicity\n\\(f \\leq g, g = O(h) \\Rightarrow f = O(h)\\)\n\n\nComposition Invariance\n\\(f = O(g) \\Rightarrow f(an + b) = O(g(an + b))\\)",
    "crumbs": [
      "Weekly Summary",
      "Week 6"
    ]
  },
  {
    "objectID": "ss25/alda/sum/w06.html#vl-11---20.05.25",
    "href": "ss25/alda/sum/w06.html#vl-11---20.05.25",
    "title": "Week 6",
    "section": "",
    "text": "this is the second most important property of an algorithm after correctness.\nTwo aspects of efficiency:\n\n\n\n\nrun-time: wall-clock time, i.e. actual time. important for end-user, must be sufficiently fast depending on the requirements. Can be measured with tools like timeit for python of Google benchmark for c/c++.\n\nimproving runtime:\n\nin interpreted languages like python the time critical portions can be delegated to C/C++/Fortran,(think CUDA an vectorization) can also be used for own code via\n\ncython: a python subset. .pyc \\(\\Rightarrow\\)\n\n.so: shared libraries in linux / macos\n.pyd: shared libraries in Windows\n\npybind11: binds c++\n\nvectorization: very importane technique; instead of repeatedly evaluating (interpreting) the body of the loop, it is delegated to a c function, prevent the intrepretation overhead:\na = b * c\ninstead of\nfor i in rage(N): a[i] = b[i] * c[i]\n\nin compiled languages the optimizatin is performed once during the compilation and modern compilers are very good at it:\n\nreorganize the execution order\n\nCPU pipeline:\n\nDecoding a commnad\nretrive Data\nexecute the command\nwrite the results to memory 2nd step can cuase bottlenecks to the pipeline \\(\\Rightarrow\\) caches. The optimizer tries to reorder the commands s.t. the result is not changed and bottlenecks are reduced.\n\n\ncomputations that are needed in multiple places can be perfomred once by the compiler.\n\n\n\n\n\n\n\n\n\nruntime depends on concrete hardware\nabstracting away from the hardware details and actual runtime, we turn our attention to general, hardware independent statements about the approximate number of atomic operations. this simplifies the problem a lot.\nO-notation:\n\n\n\nruntime\n\n\nfor all \\(N \\geq N_0\\): \\(f(N) \\leq c\\cdot g(N)\\), for some \\(c\\).\ndefinition:\n\\[\\mathcal{O}(g(N)) = \\{f(N) : \\exists c, N_0 \\text{ s.t. } f(N) \\leq c\\cdot g(N), \\forall N \\geq N_0\\}\\]\n\\(\\mathcal{O}(\\bullet)\\) is analgous to \\(\\leq\\), i.e. \\(f \\in \\mathcal{O}(g) \\sim f\\leq g\\), and it indeed satisfies the axioms of a partial order:\n\ntransitivity: f = O(g) and g = O(h) =&gt; f = O(h)\nreflexivity: f = O(f)\n\n\n\n\n\n\n\\(f(n) = O(g(n))\\) means:\n\\[\n\\exists\\, c &gt; 0,\\, n_0 \\in \\mathbb{N} : \\forall n \\geq n_0,\\, |f(n)| \\leq c \\cdot |g(n)|\n\\]\n\nWe assume all functions are eventually non-negative, or we use absolute values for generality.\n\n\n\n\n\n\\[\nf(n) = O(f(n))\n\\]\n\n\n\nTake \\(c = 1\\), then for all \\(n\\),\n\\[\n|f(n)| \\leq 1 \\cdot |f(n)|\n\\]\nSo the condition for \\(O(f(n))\\) is satisfied with \\(n_0 = 0\\).\n\n\n\n\n\n\n\nIf \\(f = O(g)\\) and \\(g = O(h)\\), then \\(f = O(h)\\)\n\n\n\n\n\\(f(n) \\leq c_1 \\cdot g(n)\\) for all \\(n \\geq n_1\\)\n\\(g(n) \\leq c_2 \\cdot h(n)\\) for all \\(n \\geq n_2\\)\n\nThen for \\(n \\geq \\max(n_1, n_2)\\):\n\\[\nf(n) \\leq c_1 \\cdot g(n) \\leq c_1 c_2 \\cdot h(n)\n\\]\nSo \\(f = O(h)\\) with constant \\(c = c_1 c_2\\), and cutoff \\(n_0 = \\max(n_1, n_2)\\).\n\n\n\n\n\n\nIf \\(f = O(h)\\) and \\(g = O(h)\\), then \\(f + g = O(h)\\)\n\n\n\n\n\\(f(n) \\leq c_1 \\cdot h(n)\\) for \\(n \\geq n_1\\)\n\\(g(n) \\leq c_2 \\cdot h(n)\\) for \\(n \\geq n_2\\)\n\nThen for \\(n \\geq \\max(n_1, n_2)\\):\n\\[\nf(n) + g(n) \\leq (c_1 + c_2) \\cdot h(n)\n\\]\nSo \\(f + g = O(h)\\) with constant \\(c = c_1 + c_2\\)\n\n\n\n\n\n\n\nIf \\(f = O(g)\\), then \\(c \\cdot f = O(g)\\) for any \\(c &gt; 0\\)\n\n\n\n\n\\(f(n) \\leq c_1 \\cdot g(n)\\) for \\(n \\geq n_0\\)\nThen:\n\n\\[\nc \\cdot f(n) \\leq c \\cdot c_1 \\cdot g(n)\n\\]\nSo \\(c \\cdot f = O(g)\\) with constant \\(c' = c \\cdot c_1\\)\n\n\n\n\n\n\n\n\\[\nf(n) + g(n) = O(\\max(f(n), g(n)))\n\\]\n\n\n\nFor all \\(n\\):\n\\[\nf(n) + g(n) \\leq 2 \\cdot \\max(f(n), g(n))\n\\]\nSo the sum is at most twice the maximum. Thus:\n\\[\nf + g = O(\\max(f, g))\n\\]\n\n\n\n\n\n\n\nIf \\(f = O(h)\\) and \\(g = O(k)\\), then \\(f \\cdot g = O(h \\cdot k)\\)\n\n\n\n\n\\(f(n) \\leq c_1 \\cdot h(n)\\), \\(g(n) \\leq c_2 \\cdot k(n)\\)\nThen:\n\n\\[\nf(n) \\cdot g(n) \\leq c_1 c_2 \\cdot h(n) \\cdot k(n)\n\\]\nSo \\(f \\cdot g = O(h \\cdot k)\\)\n\n\n\n\n\n\n\nIf \\(f(n) \\leq g(n)\\) eventually, and \\(g = O(h)\\), then \\(f = O(h)\\)\n\n\n\n\n\\(g(n) \\leq c \\cdot h(n)\\), and \\(f(n) \\leq g(n)\\)\nThen:\n\n\\[\nf(n) \\leq g(n) \\leq c \\cdot h(n)\n\\]\nSo \\(f = O(h)\\)\n\n\n\n\n\n\n\nIf \\(f(n) = O(g(n))\\), and \\(h(n) = an + b\\), then:\n\\[\nf(h(n)) = O(g(h(n)))\n\\]\n\n\n\n\n\\(f(n) \\leq c \\cdot g(n)\\) for \\(n \\geq n_0\\)\nSince \\(h(n) \\to \\infty\\) as \\(n \\to \\infty\\), there exists \\(n_1\\) such that \\(h(n) \\geq n_0\\)\nThen:\n\n\\[\nf(h(n)) \\leq c \\cdot g(h(n)) \\quad \\text{for } n \\geq n_1\n\\]\nSo \\(f \\circ h = O(g \\circ h)\\)\n\n\n\n\n\n\n\n\n\n\n\nProperty\nStatement\n\n\n\n\nReflexivity\n\\(f = O(f)\\)\n\n\nTransitivity\n\\(f = O(g), g = O(h) \\Rightarrow f = O(h)\\)\n\n\nAdditivity\n\\(f = O(h), g = O(h) \\Rightarrow f + g = O(h)\\)\n\n\nScalar Multiplication\n\\(f = O(g) \\Rightarrow c \\cdot f = O(g)\\)\n\n\nMax-Dominance\n\\(f + g = O(\\max(f, g))\\)\n\n\nMultiplicativity\n\\(f = O(h), g = O(k) \\Rightarrow fg = O(hk)\\)\n\n\nMonotonicity\n\\(f \\leq g, g = O(h) \\Rightarrow f = O(h)\\)\n\n\nComposition Invariance\n\\(f = O(g) \\Rightarrow f(an + b) = O(g(an + b))\\)",
    "crumbs": [
      "Weekly Summary",
      "Week 6"
    ]
  },
  {
    "objectID": "ss25/alda/sum/w06.html#vl-12---22.05.25",
    "href": "ss25/alda/sum/w06.html#vl-12---22.05.25",
    "title": "Week 6",
    "section": "VL 12 - 22.05.25",
    "text": "VL 12 - 22.05.25\n\nComplexity (Cont.)\n\nGoal: find a as simple as possible expression approximating the run time for large \\(N\\)s.\n\nfocus on the relative change: \\(\\frac{f(2N)}{N}\\), s.t. implementation details & wall clock time vernachlaessigt werden koennen\n\n\\(\\Rightarrow\\) O-notation. More general: Landau Symbols.\n\nCertainly! Here’s the completed table with the standard Landau symbols (asymptotic notations) used in algorithm analysis, including proper mathematical definitions and intuitive interpretations.\n\n\n\n\n\n\n\n\nSymbol\nDefinition\nIntuition\n\n\n\n\n\\(f(N) \\in O(g(N))\\)\n\\(\\exists\\, c &gt; 0,\\, N_0 \\in \\mathbb{N},\\ \\forall N \\geq N_0,\\ f(N) \\leq c \\cdot g(N)\\)\n\\(f(N) \\lesssim g(N)\\) (eventual upper bound)\n\n\n\\(f(N) \\in o(g(N))\\)\n\\(\\forall\\, c &gt; 0,\\, \\exists N_0 \\in \\mathbb{N},\\ \\forall N \\geq N_0,\\ f(N) &lt; c \\cdot g(N)\\)\n\\(f(N) \\ll g(N)\\) (strictly smaller order)\n\n\n\\(f(N) \\in \\Omega(g(N))\\)\n\\(\\exists\\, c &gt; 0,\\, N_0 \\in \\mathbb{N},\\ \\forall N \\geq N_0,\\ f(N) \\geq c \\cdot g(N)\\)\n\\(f(N) \\gtrsim g(N)\\) (eventual lower bound)\n\n\n\\(f(N) \\in \\omega(g(N))\\)\n\\(\\forall\\, c &gt; 0,\\, \\exists N_0 \\in \\mathbb{N},\\ \\forall N \\geq N_0,\\ f(N) &gt; c \\cdot g(N)\\)\n\\(f(N) \\gg g(N)\\) (strictly greater order)\n\n\n\\(f(N) \\in \\Theta(g(N))\\)\n\\(\\exists\\, c_1, c_2 &gt; 0,\\ N_0 \\in \\mathbb{N},\\ \\forall N \\geq N_0,\\ c_1 g(N) \\leq f(N) \\leq c_2 g(N)\\)\n\\(f(N) \\sim g(N)\\) up to constant factors\n\n\n\\(f(N) \\sim g(N)\\)\n\\(\\lim_{N \\to \\infty} \\frac{f(N)}{g(N)} = 1\\)\n\\(f(N)\\) and \\(g(N)\\) grow identically\n\n\n\nThere is an analogous landau notation when \\(N \\to 0\\) - useful in numerical mathematics to express how fast an approxaton error appraoches 0 w.r.t. the steps of algorithms.\n\nImplications Diagram:\n\n\n\nimplications diagram\n\n\n\n\nProving with Mathematical Induction\nTo prove \\(f(n) \\in O(g(n))\\), we aim to find constants \\(c &gt; 0\\) and \\(N_0 \\in \\mathbb{N}\\) such that:\n\\[\n\\forall n \\geq N_0:\\quad f(n) \\leq c \\cdot g(n)\n\\]\nWe proceed by mathematical induction, starting at \\(n = N_0\\).\n\nStep 1: Choose constants\nChoose appropriate constants \\(c &gt; 0\\) and \\(N_0 \\in \\mathbb{N}\\) based on the growth of \\(f(n)\\) and \\(g(n)\\).\n\n\nStep 2: Induction Base (IB)\nShow that the inequality holds at the base case \\(n = N_0\\):\n\\[\nf(N_0) \\leq c \\cdot g(N_0)\n\\]\n\n\nStep 3: Induction Step (IS)\nAssume the inductive hypothesis for some \\(n \\geq N_0\\):\n\\[\nf(n) \\leq c \\cdot g(n)\n\\]\nThen prove it holds for \\(n + 1\\):\n\\[\nf(n+1) \\leq c \\cdot g(n+1)\n\\]\nIf both steps succeed, the inequality holds for all \\(n \\geq N_0\\), and hence \\(f(n) \\in O(g(n))\\).\n\n\n\nExample: Prove \\(f(n) = 5n^2 + 3n \\in O(n^2)\\)\n\nGoal\nFind constants \\(c &gt; 0\\) and \\(N_0 \\in \\mathbb{N}\\) such that:\n\\[\nf(n) = 5n^2 + 3n \\leq c \\cdot n^2 \\quad \\text{for all } n \\geq N_0\n\\]\n\n\nStep 1: Choose constants\nTry \\(c = 6\\). We check when:\n\\[\n5n^2 + 3n \\leq 6n^2\n\\quad \\iff \\quad -n^2 + 3n \\leq 0\n\\quad \\iff \\quad n(n - 3) \\leq 0\n\\]\nThis holds for \\(n \\leq 3\\), so the inequality is true when \\(n \\geq 3\\). Choose:\n\n\\(c = 6\\)\n\\(N_0 = 3\\)\n\n\n\nStep 2: Induction Base (IB)\nCheck at \\(n = 3\\):\n\\[\nf(3) = 5 \\cdot 9 + 3 \\cdot 3 = 54 \\\\\nc \\cdot 3^2 = 6 \\cdot 9 = 54 \\\\\n\\Rightarrow f(3) = c \\cdot n^2\n\\]\n✓ Base case holds.\n\n\nStep 3: Induction Step (IS)\nAssume for some \\(n \\geq 3\\):\n\\[\nf(n) \\leq 6n^2\n\\]\nShow:\n\\[\nf(n+1) \\leq 6(n+1)^2\n\\]\nCompute:\n\\[\nf(n+1) = 5(n+1)^2 + 3(n+1) = 5n^2 + 10n + 5 + 3n + 3 = 5n^2 + 13n + 8\n\\]\n\\[\n6(n+1)^2 = 6n^2 + 12n + 6\n\\]\nCompare:\n\\[\n5n^2 + 13n + 8 \\leq 6n^2 + 12n + 6\n\\quad \\iff \\quad 0 \\leq n^2 - n - 2\n\\quad \\iff \\quad (n - 2)(n + 1) \\geq 0\n\\]\nThis inequality holds for \\(n \\geq 2\\), and our induction starts at \\(n \\geq 3\\), so:\n✓ Induction step verified.\n\n\n\n\nConclusion\nBy mathematical induction, we have shown that:\n\\[\nf(n) = 5n^2 + 3n \\leq 6n^2 \\quad \\text{for all } n \\geq 3\n\\Rightarrow f(n) \\in O(n^2)\n\\]\nwith constants \\(c = 6\\), \\(N_0 = 3\\). □\n\nLet me know if you want to adapt this to other asymptotic bounds like \\(\\Omega(n^2)\\) or \\(\\Theta(n^2)\\), or if you’d like a PDF-ready version.\n\n\n\nAlternative Definitnios with Limits\n\\[\n\\begin{array}{|c|c|c|c|}\n\\hline\n\\textbf{Notation} & \\textbf{Limit Definition} & \\textbf{Quantifier Definition} & \\textbf{Intuition} \\\\\n\\hline\nf(n) \\in o(g(n)) &\n\\displaystyle \\lim_{n \\to \\infty} \\frac{f(n)}{g(n)} = 0 &\n\\forall c &gt; 0,\\ \\exists N_0:\\ \\forall n \\geq N_0,\\ f(n) &lt; c \\cdot g(n) &\n\\text{Strictly smaller order} \\\\\n\\hline\nf(n) \\in O(g(n)) &\n\\displaystyle \\limsup_{n \\to \\infty} \\left| \\frac{f(n)}{g(n)} \\right| &lt; \\infty &\n\\exists c &gt; 0,\\ \\exists N_0:\\ \\forall n \\geq N_0,\\ f(n) \\leq c \\cdot g(n) &\n\\text{Grows no faster (upper bound)} \\\\\n\\hline\nf(n) \\in \\Theta(g(n)) &\n\\displaystyle \\lim_{n \\to \\infty} \\frac{f(n)}{g(n)} = L,\\ 0 &lt; L &lt; \\infty &\n\\exists c_1, c_2 &gt; 0,\\ \\exists N_0:\\ \\forall n \\geq N_0,\\ c_1 g(n) \\leq f(n) \\leq c_2 g(n) &\n\\text{Same order (tight bound)} \\\\\n\\hline\nf(n) \\in \\Omega(g(n)) &\n\\displaystyle \\liminf_{n \\to \\infty} \\left| \\frac{f(n)}{g(n)} \\right| &gt; 0 &\n\\exists c &gt; 0,\\ \\exists N_0:\\ \\forall n \\geq N_0,\\ f(n) \\geq c \\cdot g(n) &\n\\text{Grows at least as fast (lower bound)} \\\\\n\\hline\nf(n) \\in \\omega(g(n)) &\n\\displaystyle \\lim_{n \\to \\infty} \\frac{f(n)}{g(n)} = \\infty &\n\\forall c &gt; 0,\\ \\exists N_0:\\ \\forall n \\geq N_0,\\ f(n) &gt; c \\cdot g(n) &\n\\text{Strictly faster growth} \\\\\n\\hline\nf(n) \\sim g(n) &\n\\displaystyle \\lim_{n \\to \\infty} \\frac{f(n)}{g(n)} = 1 &\n\\forall \\varepsilon &gt; 0,\\ \\exists N_0:\\ \\forall n \\geq N_0,\\\n(1 - \\varepsilon)g(n) &lt; f(n) &lt; (1 + \\varepsilon)g(n) &\n\\text{Asymptotically equal (same leading term)} \\\\\n\\hline\n\\end{array}\n\\]\n\n\nExmamples from Sorting\n\nselection sort: \\(T(N) = \\frac{N}{2}(N + 1) \\in \\mathbb{O}(N^2)\\)\ninsertion sort (typical case): \\(T(N) = \\frac{N}{4}(N + 1) \\in \\mathcal{O}(N^2)\\)\nquick sort (typical case): \\(T(N) = c \\cdot (N + 1) \\ln{N + 1} \\in \\mathcal{O}(N \\cdot \\log{N})\\)\n\n\n\nExample - Running Mean\ndef running_mean(a, k) # k size of the Window, N = len(a)\n  r = [0] * len(a) # output array O(N) \n  if k &gt; len(a): # O(1)\n    raise ValueError(\"k &gt; N\") # O(1)\n  for j in range(k - 1, len(a)): # O(N - k + 1) == O(N)\n    for i in range(j - k + 1, j + 1): # O(k)\n      r[j] += a[i] # summing up \n    r[j] = r[j] / k  # mean\n  return r\nHow to obtain the complexity of the whole algorithm based on the complexity of individual lines?\n\nalgebraic rules of Complexity classes\nsequence rule: for consecutive execution the more expensive command decisive.\nLet C := C1; C2, s.t.\nC1 # O(f1(N))\nC2 # O(f2(N))\nThen complexity of C is \\(max(\\mathcal{O}(f_1(N)), \\mathcal{O}(f_2(N)))\\)\nNesting rule: complexity is the product\nLet P:\nfor k in # O(f(N)):\n  C # O(g(N))\nThen complexity of P is \\(\\mathcal{O}(f(N) \\cdot g(N))\\).\nBut this holds if the body of the loop always has the same complexity. If it’s variable we can take always the worst case \\(g(N) = max_k(g(N, k))\\) (may be too pessimistic) or sum up for each iteration.\n\nApplying these rules to the running mean example from above we get:\n\nbefore the loop: \\(\\mathcal{O}(N) + \\mathcal{O}(1) + \\mathcal{O}(1) \\Rightarrow \\mathcal{O}(N)\\)\nafter the loop: \\(\\mathcal{O}(1)\\)\ninner loop:\n\nnumber of iterations: \\(\\mathcal{O}(k)\\), loop body: \\(\\mathcal{O}(1) \\Rightarrow \\mathcal O(1 \\cdot k) = \\mathcal{O}(k)\\)\n\nouter loop:\n\nNumber of iterations: N - k + 1, loopbody O(k) + O(1) = O(k)\n=&gt; product: O((N - k + 1)*k) = O(Nk -k^2 + k) = O(kN) = O(N)\n\nalltogether: O(N) + O(1) + O(kN) = O(kN). we have k = cN. if c == 1",
    "crumbs": [
      "Weekly Summary",
      "Week 6"
    ]
  },
  {
    "objectID": "ss25/alda/sum/w08.html",
    "href": "ss25/alda/sum/w08.html",
    "title": "Week 8",
    "section": "",
    "text": "wenn \\(f(n) in \\mathcal{O}(g(N))\\) dann gilt fuer die meisten ’interessanten” der Verschiebungsregel \\(f(N + k) \\in \\mathcal{O}(g(N))\\) genau dann, wenn \\(f(N + k) \\leq b^k \\cdot f(N)\\) fuer konstante \\(b\\) und \\(N \\geq N_k\\) gilt.\nproof:\nExamples:\n\n\\(f(N) = N^D\\) for \\(D &gt; 0\\) (i.e. Monome, roots, …) \\(f(N + k) = (N + k)^D \\leq (2N)^D = 2^D\\cdot N^D = 2^D\\cdot f(N)\\), (here we set \\(N \\geq k\\))\n\\(f(N) = \\log_a N\\) \\(\\Rightarrow\\) \\(f(N + k) = \\log_a (N + k) = \\log_a N \\cdot (1 + \\frac{k}{N}) \\leq ...\\)\n\\(f(N) = a^N \\Rightarrow f(N + k) - a^{N + k} = a^k \\cdot a^N \\rightarrow b = a\\)\n\nBasically for these functions the shifting N -&gt; N + k doesn’t change the complexity class, (algorihtm will still run longer though)\n\n\n\n\nRuntime: \\(T(N) = T_{\\text{local}}(N) + \\sum_{i} T_{\\text{recursive}}(N_i)\\)\nTwo solution methods:\n\nrecursively substitute the definition of the formula within the sub formulas, until a simpler expression is reached. Even though this might be mathematically not exact it usually allows to guess a formula.\nmaster theorem:\nmaster theorem can be applied when:\n\\[T(N) = T_{\\text{local}}(N) + a \\cdot T_{\\text{recursive}}(N / b)\\]\ni.e. \\(a\\) recursive calls with smaller inputs: \\(\\frac{N}{b}\\)\n\nrecursions exponent: \\(\\rho = \\log_b a\\)\nthree cases:\n\n\\(T_{\\text{local}}(N) \\in \\mathcal{O}(N^{\\rho - \\epsilon}\\), for \\(\\epsilon &gt; 0\\). Here the local computation is a little faster than \\(N^{\\rho}\\). Then the complexity\n\\[T(N) \\in \\Theta(N^{\\rho})\\]\n\\(T_{\\text{local}}(N) \\in \\Theta(N^{\\rho})\\). Local computation as fast as \\(N^{\\rho}\\). Complexity:\n\\[T(N) \\in \\Theta(N^{\\rho} \\log N)\\]\n\\(T_{\\text{local}}(N) \\in \\Omega(N^{\\rho + \\epsilon})\\), for \\(\\epsilon &gt; 0\\) and \\(a\\cdot T_{\\text{local}}(\\frac{N}{b}) \\leq c \\cdot T_{\\text{local}}(N)\\), s.t. \\(c \\leq 1\\). then local computations dominate and we have:\n\\[T(N) \\in \\Theta(T_{\\text{local}}(N))\\]\n\n\n\n\n\n\n\nthe most important quality criteria of a large DB is a powerful search function\nfundamnetal search types:\n\nkey search ()\nrange search: keys are in an interval\nsimilarity search: finds elements whose keys are similar to the given key \\(\\Rightarrow\\)\n\ncorrecting typing mistakes like google does it\ncontent-based search (?)\n\nGraph search: shortest path (navigation)\n\n\n\n\n\n\n\nwhen there is no order, the elements are not order by key\nadvantages:\n\nno requirements of specifications how the data should be stored\nthe keys must only support “==” and don’t have to support &lt;=.\n\ndisadvantages: slow, \\(\\mathcal{O}(N)\\).\ngeneral implementation:\ndef sequential_search(a, target_key, key_function):\n  for i in ragen(len(a)):\n      if key_function(a[i]) == target_key:\n          return i # or return a[i]\n  return None # not found\nto simplify, we will omit the explicit usage of key_function. Then the elements themselves are the keys. But in practice usually a key function is necessary, since elements are objects or structures.\nThe simplified version:\ndef sequential_search(a, target_key):\n  for i in range(len(a)):\n      if a[i] == target_key: return i\n  return None\nWe want faster searching\n\n\n\n\n\nto have faster searching we need more information about the keys - they should support more than ==,\n\n&lt;= (total Order): binary search, \\(\\mathcal{O}(\\log N)\\)\nhash-function: hashtable, \\(\\mathcal{O}(1)\\)\nquantize-function: bucketarray, \\(\\mathcal{O}(1)\\) (or Bucket sort)\n\n\n\n\n\nData are in a sorted array. We need a &lt; function.\na = [...] # define an unsorted array \na.sort()\nfound = binary_search(a, key) \n\nimplementation:\n\ndef binary_search_impl(a, key, start, end):\n  size = end - start\n  if size &lt; 0 : return None # not found\n  center = (start + end) // 2\n  if a[center] == key: return center\n  if a[center] &lt; key: return binary_search_impl(a, key, center + 1, end)\n  else : return binary_search_impl(a, key, start, center - 1)\n\n\ndef binary_search(a, key):\n  return binary_search_impl(a, key, 0, len(a)) # divide and conquer\n\n\na = [2, 1, -3, 4]\na.sort()\nbinary_search(a, 1)\n\n1\n\n\n\n\n\n\n\nimplementation:\n\n\ndef binary_search_it_impl(a, key, begin, end):\n    size = end - begin\n    center = (begin + end) // 2\n    # invariant: key possibly in a[begin : end]\n    while size != 0 :\n        if a[center] == key: return center\n        if key &gt; a[center]: begin = center + 1\n        if key &lt; a[center]: end = center\n        center = (begin + end) // 2\n        size = end - begin\n    # size == 0\n    return None \n\ndef binary_search_it(a, key):\n    return binary_search_it_impl(a, key, 0, len(a))\n\nprint(a)\nprint(binary_search_it([3, 4], 4))\n\n[-3, 1, 2, 4]\n1\n\n\n\ncomplexity analsysi - we apply the master theorem: \\(T(N) = T_{\\text{local}}(N) + a T_{\\text{recursive}}(\\frac{N}{2})\\). Then we have:\n\n\\(T_l = O(1)\\)\n\\(a = 1\\)\n\\(b = 2\\)\n\\(\\rho = \\log_2 1 = 0 \\Rightarrow L_ ...\\)",
    "crumbs": [
      "Weekly Summary",
      "Week 8"
    ]
  },
  {
    "objectID": "ss25/alda/sum/w08.html#vl-15---03.06.25",
    "href": "ss25/alda/sum/w08.html#vl-15---03.06.25",
    "title": "Week 8",
    "section": "",
    "text": "wenn \\(f(n) in \\mathcal{O}(g(N))\\) dann gilt fuer die meisten ’interessanten” der Verschiebungsregel \\(f(N + k) \\in \\mathcal{O}(g(N))\\) genau dann, wenn \\(f(N + k) \\leq b^k \\cdot f(N)\\) fuer konstante \\(b\\) und \\(N \\geq N_k\\) gilt.\nproof:\nExamples:\n\n\\(f(N) = N^D\\) for \\(D &gt; 0\\) (i.e. Monome, roots, …) \\(f(N + k) = (N + k)^D \\leq (2N)^D = 2^D\\cdot N^D = 2^D\\cdot f(N)\\), (here we set \\(N \\geq k\\))\n\\(f(N) = \\log_a N\\) \\(\\Rightarrow\\) \\(f(N + k) = \\log_a (N + k) = \\log_a N \\cdot (1 + \\frac{k}{N}) \\leq ...\\)\n\\(f(N) = a^N \\Rightarrow f(N + k) - a^{N + k} = a^k \\cdot a^N \\rightarrow b = a\\)\n\nBasically for these functions the shifting N -&gt; N + k doesn’t change the complexity class, (algorihtm will still run longer though)\n\n\n\n\nRuntime: \\(T(N) = T_{\\text{local}}(N) + \\sum_{i} T_{\\text{recursive}}(N_i)\\)\nTwo solution methods:\n\nrecursively substitute the definition of the formula within the sub formulas, until a simpler expression is reached. Even though this might be mathematically not exact it usually allows to guess a formula.\nmaster theorem:\nmaster theorem can be applied when:\n\\[T(N) = T_{\\text{local}}(N) + a \\cdot T_{\\text{recursive}}(N / b)\\]\ni.e. \\(a\\) recursive calls with smaller inputs: \\(\\frac{N}{b}\\)\n\nrecursions exponent: \\(\\rho = \\log_b a\\)\nthree cases:\n\n\\(T_{\\text{local}}(N) \\in \\mathcal{O}(N^{\\rho - \\epsilon}\\), for \\(\\epsilon &gt; 0\\). Here the local computation is a little faster than \\(N^{\\rho}\\). Then the complexity\n\\[T(N) \\in \\Theta(N^{\\rho})\\]\n\\(T_{\\text{local}}(N) \\in \\Theta(N^{\\rho})\\). Local computation as fast as \\(N^{\\rho}\\). Complexity:\n\\[T(N) \\in \\Theta(N^{\\rho} \\log N)\\]\n\\(T_{\\text{local}}(N) \\in \\Omega(N^{\\rho + \\epsilon})\\), for \\(\\epsilon &gt; 0\\) and \\(a\\cdot T_{\\text{local}}(\\frac{N}{b}) \\leq c \\cdot T_{\\text{local}}(N)\\), s.t. \\(c \\leq 1\\). then local computations dominate and we have:\n\\[T(N) \\in \\Theta(T_{\\text{local}}(N))\\]\n\n\n\n\n\n\n\nthe most important quality criteria of a large DB is a powerful search function\nfundamnetal search types:\n\nkey search ()\nrange search: keys are in an interval\nsimilarity search: finds elements whose keys are similar to the given key \\(\\Rightarrow\\)\n\ncorrecting typing mistakes like google does it\ncontent-based search (?)\n\nGraph search: shortest path (navigation)\n\n\n\n\n\n\n\nwhen there is no order, the elements are not order by key\nadvantages:\n\nno requirements of specifications how the data should be stored\nthe keys must only support “==” and don’t have to support &lt;=.\n\ndisadvantages: slow, \\(\\mathcal{O}(N)\\).\ngeneral implementation:\ndef sequential_search(a, target_key, key_function):\n  for i in ragen(len(a)):\n      if key_function(a[i]) == target_key:\n          return i # or return a[i]\n  return None # not found\nto simplify, we will omit the explicit usage of key_function. Then the elements themselves are the keys. But in practice usually a key function is necessary, since elements are objects or structures.\nThe simplified version:\ndef sequential_search(a, target_key):\n  for i in range(len(a)):\n      if a[i] == target_key: return i\n  return None\nWe want faster searching\n\n\n\n\n\nto have faster searching we need more information about the keys - they should support more than ==,\n\n&lt;= (total Order): binary search, \\(\\mathcal{O}(\\log N)\\)\nhash-function: hashtable, \\(\\mathcal{O}(1)\\)\nquantize-function: bucketarray, \\(\\mathcal{O}(1)\\) (or Bucket sort)\n\n\n\n\n\nData are in a sorted array. We need a &lt; function.\na = [...] # define an unsorted array \na.sort()\nfound = binary_search(a, key) \n\nimplementation:\n\ndef binary_search_impl(a, key, start, end):\n  size = end - start\n  if size &lt; 0 : return None # not found\n  center = (start + end) // 2\n  if a[center] == key: return center\n  if a[center] &lt; key: return binary_search_impl(a, key, center + 1, end)\n  else : return binary_search_impl(a, key, start, center - 1)\n\n\ndef binary_search(a, key):\n  return binary_search_impl(a, key, 0, len(a)) # divide and conquer\n\n\na = [2, 1, -3, 4]\na.sort()\nbinary_search(a, 1)\n\n1\n\n\n\n\n\n\n\nimplementation:\n\n\ndef binary_search_it_impl(a, key, begin, end):\n    size = end - begin\n    center = (begin + end) // 2\n    # invariant: key possibly in a[begin : end]\n    while size != 0 :\n        if a[center] == key: return center\n        if key &gt; a[center]: begin = center + 1\n        if key &lt; a[center]: end = center\n        center = (begin + end) // 2\n        size = end - begin\n    # size == 0\n    return None \n\ndef binary_search_it(a, key):\n    return binary_search_it_impl(a, key, 0, len(a))\n\nprint(a)\nprint(binary_search_it([3, 4], 4))\n\n[-3, 1, 2, 4]\n1\n\n\n\ncomplexity analsysi - we apply the master theorem: \\(T(N) = T_{\\text{local}}(N) + a T_{\\text{recursive}}(\\frac{N}{2})\\). Then we have:\n\n\\(T_l = O(1)\\)\n\\(a = 1\\)\n\\(b = 2\\)\n\\(\\rho = \\log_2 1 = 0 \\Rightarrow L_ ...\\)",
    "crumbs": [
      "Weekly Summary",
      "Week 8"
    ]
  },
  {
    "objectID": "ss25/alda/sum/w08.html#vl-16---05.06.25",
    "href": "ss25/alda/sum/w08.html#vl-16---05.06.25",
    "title": "Week 8",
    "section": "VL 16 - 05.06.25",
    "text": "VL 16 - 05.06.25\nSorting is O(nlog(n)), sequential searching of unsorted structures is O(n), which is faster. Binary search is O(log(n)) and hash table search is O(1). Binary search must be performed on a sorted structure. So if we were to resort every time we insert an element it would be less effective than sequential searching. Therefore we need a better way to keep the data structure sorted \\(\\Rightarrow\\) Search trees\n\nSearch Trees\n\nGoal: inserting, removing, sorting all O(log(N))\nDef:\n\nGraph:\n\nnodes: data elements\nedges: connections\n\npath: a sequence of nodes where each node in the sequence are connected by an edge.\nTree:: any arbitrary pair of nodes are connected with a unique path. In such graphs there are no cycles.\nRoot: an arbitrarily selected node.\n\n\n\nTree Data Structure\n\n\n\ntree ds\n\n\n\nBinary Tree: a node has at most three neighbors\n\nparent: every node other than the root node (that was chosen arbitrarily) has a unique parent\nchild: every node other than the leave nodes have children\nleaf: nodes that don’t have children\ninterior node:\nsub-tree: in a binary tree the sub-trees get exponentially smaller (each time they are halved).\n\n\n\nImplementation\nclass Node:\n  def __init__(self, key, value):\n    self.key = key\n    self.value = value\n    self.left = self.right = None # a node is initially a leaf\nExample construction of a tree (to simplify we omit value, i.e. keys are values themselves):\nroot =  Node(\"R\")\nroot.left = Node(\"A\")\nroot.right = Node(\"B\")\nroot.right.left = Node(\"C\")\nroot.right.right = Node(\"D\")\n\nbinary tree \\(\\Rightarrow\\) binary search Tree when Search tree conditions are satisfied. Search tree conditions:\n\nkeys are totally ordered\nfor each node holds:\n\nall the nodes in the left substree are smaller or equal\nall the nodes in right substree are greater\n\n\ntree search: recursively:\n\nif the target_key is greater than the ucrrent node: search in right subtree\notherwise search in the left subtree\n\n\nimplementation:\ndef tree_serch(node, target_key):\n  if node is None: return None\n  if node.key == target_key: return node\n  if target_key &lt; node.key: return tree_search(node.left, target_key)\n  else return tree_search(node.right, target_key)\nThe tree should be maintained s.t. the search tree condition is always true and the tree is balanced. If the tree is not balanced it deteriorates to a linear search.\n\nInsertion: Elements should be inserted s.t. the search tree condition is always satisfied.\n\nif target key larger / smaller then current key: insert in right / left subtree\nwhat to do when the key is already there?\n\noption1: raise error (not a good solution)\noverwrite the data value with the new data value (this is what’s done in arrays anyways. )\n\n\nImplementation:\ndef tree_insert(node, key, value):\n  if node is None: return Node(key, value)\n  if node.key == key:\n    node.value = value # return value \n    return node\n  if key &lt; node.key:\n    node.left = tree_insert(node.left, key, value)\n  else:\n    node.right = tree_inssert(node.right, key, value)\n  return node\nbad sequnces of insertions cna cause a tree to detoriarete to a linked list:\n\n# good sequence\nroot = None\nroot =  tree_insert(root, 4)\nroot = tree_insert(root, 2)\nroot = tree_insert(root, 3)\nroot = tree_insert(root, 3)\nroot = tree_insert(root, 6)\n\n# bad sequence: 2, 3, 4, 6\n\n\n\ngood & bad sequences\n\n\n\nDeletion: deletion should preserve the search condition:\n\ncase 0: key doesn’t exist: raise error or don’t do anything\ncase 1: key is a leaf: remove the leaf (set it to None)\ncase 2: node has a single child: replace the none\ncase 3: node has two children (difficult case)\n\nsearch for neighbors (predeccessor or successor) of the node in the order and replace it\nremove the neighbors of its children form the prefvious posisiont\n\n\n\n# hilfsfunction for predecossor\ndef tree_predecessor(node): \n  node = node.left\n  while node right is not None:\n    node = node.right\n  return node\n\ndef tree_remove(node, key):\n  if node is None: return None # 0th case\n  if key &lt; node.key: node.left = tree_remove(node.left, key) # \n  elif key &gt; node.key: node.right = tree_remove(node.right, key)\n  else: # key == node.key\n    if node.left is None and node.right is None: return None # 1st case\n    if node.left is None: return node.right\n    if node.right is None: return node.left\n    pred = tree_predecessor(node)\n    node.key = pred.key; node.value = pred.value # copy the predecessor\n    node.left = tree_remove(node.left, pred.key) # remove old predecessor\n    return node",
    "crumbs": [
      "Weekly Summary",
      "Week 8"
    ]
  },
  {
    "objectID": "ss25/alda/sum/w10.html",
    "href": "ss25/alda/sum/w10.html",
    "title": "Week 10",
    "section": "",
    "text": "properties of anderson trees:\n\n0: search-tree condition\n1-3: horizontal edges\n4: all nodes that are not at the lowest level, i.e. not the smallest sentinel distance, have exactly two children\n\nMain point idea of Anderson: 0-4 are by appropriate impelemntation of insert and remove always satisfiable.\nLast if of insert:\nif node.right is not None and node.right.right is not None and \\\n    node.dist == node.right.right.dist: # \n        node = rotate_left(node)\n        node.dist += 1 # this node has two children, because in hte midle of the chain in the tree\nConsequence for the depth:\n\nlowest level: k = k_min =&gt; N_min(k_min) = 1\nhigher levels:\n\nCase 1: no horizontal edge: N_min(k) = 1 + 2 N_min(k - 1)\nCase 2: right horizontal edge: N_min(k) = 1 + N_min(k - 1) + 1 + 2 N_min(k - 1) = 2 + 3 N_min(k - 1)\n\n\nCase 1 is corresponds exactly to the perfect tree, k = log_2(N + 1)\n\nSummary:\n\nk_max &lt;= log_2(N + 1) always holds (k_max is the distance to the root)\nthere are never two subsequent horizontal edges =&gt; d &lt;= 2k_max\n\\(\\Rightarrow\\) d &lt;= 2 log_2(N + 1) (basically same complexity as a perfectly balanced tree).\n\n\n\n\n\nanderson-insert\n\n\n\n\n\n\nwe search for a node with highest (max priority search) or lowest priority (min priority search) (not for a specific key)\nif we have max priority we can automatically can have a min priority searching by simply redefining the priority\n\npriority -&gt; -priority\npriority -&gt; 1 / priority\n\napplications:\n\njob queues on a machine or a cpu\nshortest paths in graphs\nheapsort\nstack (LIFO) and queue (FIFO): are special cases of priority queues / priority search: waiting_time := now - arrive_time\n\n\\(\\Rightarrow\\) queue: max priority search with waiting_time\n\\(\\Rightarrow\\) stack: min priority search with waiting_time\n\n\n\n\n\n\nsequential search in an array a: finding the max element (same as inner loop of selection sort)\nm = 0\nfor i in range(1, len(a)):\n  if a[i] &gt; a[m]: m = i\n  return m\nsorted array: m = len(a) - 1: O(1), but O(N log(N)) sorting expense. This is usually inefficient, becase many elements are dynamically coming and leaving the queue. This would require resorting\nsearch tree:\nnode = root\nwhile node.right is not None:\n  node = node.right\nreturn node\nsame as tree_predecessor(node.left)\nbut we can do better! \\(\\Rightarrow\\) Heap Data Structure\n\n\n\n\nfinding the max element is O(log(N)) but the constant within are better, also heaps can be stored as arrays instead of linked nodes as with search trees, which makes it even faster in practice.\n\nHeap is a binary tree with two properties:\n\nperfectly balanced and left-leaning (can be stored as an array)\n\n\n\nheap-tree\n\n\nHeap-property: the root of every sub-tree has the highest priority in the sub-tree \\(\\Rightarrow\\) largest element is the root of the whole tree.\n\nflattening: representation of a left-leaning heap as array\n\nnodes are stored level-wise one after the other:\n\nchild of k: 2k + 1 or 2k + 2\nparent of k: (k - 1) // 2\n\n\n\n\n\n\nflattening\n\n\n\n\nclass Heap:\n    def __init__(self): self.data = []\n    def __len__(self): return len(self.data)\n    def top(self): return self.data[0]\n    def pop(self): # removes top\n    def push(self, priority) # insert\n\nbasic principle of insertion:\n\nnew element is inserted at the end of self.data. (this is efficient due to amortized O(1) complexity of heap)\nswap elements until heap condition is satisfied again: O(log N)\n\nimplementation:\ndef push(self, priority):\n  self.data.append(priority) # step 1\n  upheap(self.data, len(self.data) - 1) # step 2, len(self.data) - 1 is the index of the last element, that is possibly at wrong position\nupheap() is what we must implement\ndef upheap(a, k): \n  v = a[k] # intermediately store the element k, priority of k\n  while True: # infinite loop, left with break\n      if k == 0: # top node, can't go higher =&gt; break\n           break\n      parent = (k - 1) // 2\n      if a[parent] &gt; v: # heap condition is satisfied\n          break\n      a[k] = a[parent]\n      k = parent\n  a[k] = v",
    "crumbs": [
      "Weekly Summary",
      "Week 10"
    ]
  },
  {
    "objectID": "ss25/alda/sum/w10.html#vl-19---17.06.25",
    "href": "ss25/alda/sum/w10.html#vl-19---17.06.25",
    "title": "Week 10",
    "section": "",
    "text": "properties of anderson trees:\n\n0: search-tree condition\n1-3: horizontal edges\n4: all nodes that are not at the lowest level, i.e. not the smallest sentinel distance, have exactly two children\n\nMain point idea of Anderson: 0-4 are by appropriate impelemntation of insert and remove always satisfiable.\nLast if of insert:\nif node.right is not None and node.right.right is not None and \\\n    node.dist == node.right.right.dist: # \n        node = rotate_left(node)\n        node.dist += 1 # this node has two children, because in hte midle of the chain in the tree\nConsequence for the depth:\n\nlowest level: k = k_min =&gt; N_min(k_min) = 1\nhigher levels:\n\nCase 1: no horizontal edge: N_min(k) = 1 + 2 N_min(k - 1)\nCase 2: right horizontal edge: N_min(k) = 1 + N_min(k - 1) + 1 + 2 N_min(k - 1) = 2 + 3 N_min(k - 1)\n\n\nCase 1 is corresponds exactly to the perfect tree, k = log_2(N + 1)\n\nSummary:\n\nk_max &lt;= log_2(N + 1) always holds (k_max is the distance to the root)\nthere are never two subsequent horizontal edges =&gt; d &lt;= 2k_max\n\\(\\Rightarrow\\) d &lt;= 2 log_2(N + 1) (basically same complexity as a perfectly balanced tree).\n\n\n\n\n\nanderson-insert\n\n\n\n\n\n\nwe search for a node with highest (max priority search) or lowest priority (min priority search) (not for a specific key)\nif we have max priority we can automatically can have a min priority searching by simply redefining the priority\n\npriority -&gt; -priority\npriority -&gt; 1 / priority\n\napplications:\n\njob queues on a machine or a cpu\nshortest paths in graphs\nheapsort\nstack (LIFO) and queue (FIFO): are special cases of priority queues / priority search: waiting_time := now - arrive_time\n\n\\(\\Rightarrow\\) queue: max priority search with waiting_time\n\\(\\Rightarrow\\) stack: min priority search with waiting_time\n\n\n\n\n\n\nsequential search in an array a: finding the max element (same as inner loop of selection sort)\nm = 0\nfor i in range(1, len(a)):\n  if a[i] &gt; a[m]: m = i\n  return m\nsorted array: m = len(a) - 1: O(1), but O(N log(N)) sorting expense. This is usually inefficient, becase many elements are dynamically coming and leaving the queue. This would require resorting\nsearch tree:\nnode = root\nwhile node.right is not None:\n  node = node.right\nreturn node\nsame as tree_predecessor(node.left)\nbut we can do better! \\(\\Rightarrow\\) Heap Data Structure\n\n\n\n\nfinding the max element is O(log(N)) but the constant within are better, also heaps can be stored as arrays instead of linked nodes as with search trees, which makes it even faster in practice.\n\nHeap is a binary tree with two properties:\n\nperfectly balanced and left-leaning (can be stored as an array)\n\n\n\nheap-tree\n\n\nHeap-property: the root of every sub-tree has the highest priority in the sub-tree \\(\\Rightarrow\\) largest element is the root of the whole tree.\n\nflattening: representation of a left-leaning heap as array\n\nnodes are stored level-wise one after the other:\n\nchild of k: 2k + 1 or 2k + 2\nparent of k: (k - 1) // 2\n\n\n\n\n\n\nflattening\n\n\n\n\nclass Heap:\n    def __init__(self): self.data = []\n    def __len__(self): return len(self.data)\n    def top(self): return self.data[0]\n    def pop(self): # removes top\n    def push(self, priority) # insert\n\nbasic principle of insertion:\n\nnew element is inserted at the end of self.data. (this is efficient due to amortized O(1) complexity of heap)\nswap elements until heap condition is satisfied again: O(log N)\n\nimplementation:\ndef push(self, priority):\n  self.data.append(priority) # step 1\n  upheap(self.data, len(self.data) - 1) # step 2, len(self.data) - 1 is the index of the last element, that is possibly at wrong position\nupheap() is what we must implement\ndef upheap(a, k): \n  v = a[k] # intermediately store the element k, priority of k\n  while True: # infinite loop, left with break\n      if k == 0: # top node, can't go higher =&gt; break\n           break\n      parent = (k - 1) // 2\n      if a[parent] &gt; v: # heap condition is satisfied\n          break\n      a[k] = a[parent]\n      k = parent\n  a[k] = v",
    "crumbs": [
      "Weekly Summary",
      "Week 10"
    ]
  },
  {
    "objectID": "ss25/ibn/index.html",
    "href": "ss25/ibn/index.html",
    "title": "Operating Systems and Networks",
    "section": "",
    "text": "notes\nsolutions\nweekly summary",
    "crumbs": [
      "ibn"
    ]
  },
  {
    "objectID": "ss25/ibn/sum/w01.html",
    "href": "ss25/ibn/sum/w01.html",
    "title": "Week 1",
    "section": "",
    "text": "Definition Betriebssystem: Software, die die Computerhardware verwaltet\nComplexity of an Operating system \\(\\approx\\) 10 mil LOC\nGeschichte der BS:\n\nErste Generation 1941 - 55: Z3, Colossus, Mark I, ENIAC\n\nENIAC: program not stored in memory, rather entered physically by connecting cables.\nEDVAC: implementation of the Von Nuemann architecture, where the program data are both stored in the memory space.\nPrograms were written in Assembler or later Fortran\nProgramming via direct manipulation of cables or later punch-cards.\nNo software that managemes software. Each user/programmer had complete and singular direct use of the hardware, for the time allocated for him/her.\n\n2nd Generation 1955 - 1964\n\nInvention of Transistors =&gt; Commercialization of computers.\nUNIVAC I (1951).\nusual operation:\n\nprogrammer punches his program in chards (either fortran or assembler) and hands them to the operator\noeprator feeds the card deck to the computer system and starts the processing\nafter the process ends the operator goes to the printer and transfers the output to the output room\ninnefiencies:\n\ncomputer spent long idle time during the output. (the output is performed by other machines and takes long time)\na human operator was necessary to load the program and transfer the output.\n\n\nbatch processing:\n\n3rd generation: Multiprogramming =&gt; MULTICS =&gt; Unix\n\nmultiprogramming vs multitasking vs time sharing.",
    "crumbs": [
      "Weekly Summary",
      "Week 1"
    ]
  },
  {
    "objectID": "ss25/ibn/sum/w01.html#vl-1---14.04.25",
    "href": "ss25/ibn/sum/w01.html#vl-1---14.04.25",
    "title": "Week 1",
    "section": "",
    "text": "Definition Betriebssystem: Software, die die Computerhardware verwaltet\nComplexity of an Operating system \\(\\approx\\) 10 mil LOC\nGeschichte der BS:\n\nErste Generation 1941 - 55: Z3, Colossus, Mark I, ENIAC\n\nENIAC: program not stored in memory, rather entered physically by connecting cables.\nEDVAC: implementation of the Von Nuemann architecture, where the program data are both stored in the memory space.\nPrograms were written in Assembler or later Fortran\nProgramming via direct manipulation of cables or later punch-cards.\nNo software that managemes software. Each user/programmer had complete and singular direct use of the hardware, for the time allocated for him/her.\n\n2nd Generation 1955 - 1964\n\nInvention of Transistors =&gt; Commercialization of computers.\nUNIVAC I (1951).\nusual operation:\n\nprogrammer punches his program in chards (either fortran or assembler) and hands them to the operator\noeprator feeds the card deck to the computer system and starts the processing\nafter the process ends the operator goes to the printer and transfers the output to the output room\ninnefiencies:\n\ncomputer spent long idle time during the output. (the output is performed by other machines and takes long time)\na human operator was necessary to load the program and transfer the output.\n\n\nbatch processing:\n\n3rd generation: Multiprogramming =&gt; MULTICS =&gt; Unix\n\nmultiprogramming vs multitasking vs time sharing.",
    "crumbs": [
      "Weekly Summary",
      "Week 1"
    ]
  },
  {
    "objectID": "ss25/ibn/sum/w01.html#vl2---16.04.25",
    "href": "ss25/ibn/sum/w01.html#vl2---16.04.25",
    "title": "Week 1",
    "section": "VL2 - 16.04.25",
    "text": "VL2 - 16.04.25\n\nLecture Notes\n\nprimary tasks of an OS:\n\nan extended machine: a comfortable hardware, abstraction =&gt; System calls, API\nmanagement of resources: processors, I/O devices, memory…\n\navoiding conflicts in multiprogramming and multitasking\nfair allocation of resources\n\n\nComputer architecture crash-course:\n\ncomponets of a PC:\n\nCPU, memeory, busses, secondary memory (hard disks), I/O devices, …\nCPU and other components operate asynchronously and largely independently from each other.\n\nMemory hiearachy, categorized w.r.t. speed, latency, capacity\ncaches = buffer kl\nStack, Stack Pointer, Stack Frames…\n\nexecution modes:\n\nuser mode: some operations are forbidden, restricted access to memory\nkernel mode (priviliged mode)\n\nCPU types:\n\nCISC: powerful but possibly slow instructions\nRISC: simple but fast instructions\nnowadays hybrid, RISC + microcode…\n\nSystem calls: the interface of an OS\nShells, shell scripts",
    "crumbs": [
      "Weekly Summary",
      "Week 1"
    ]
  },
  {
    "objectID": "ss25/ibn/sum/w03.html",
    "href": "ss25/ibn/sum/w03.html",
    "title": "Week 3",
    "section": "",
    "text": "Threads are miniprocesses.\nMain idea: we want concurrency but still retain the recourses of a single process like the data, files and code. (different processes are isolated from each other and don’t share these resources)\nMultithreading is using multiple such threads in a single process \\(\\Rightarrow\\) They all use the same PCB (they coexist within the same process)\nAdvantages of threads w.r.t processes:\n\nsimple comunication between threads, they share the same resources, and it’s faster to synhronize them.\n(no system calls, no switching to kernel mode) Shared resources:\n\ncode\ndata\nfiles\n\nThis makes threads more responsive compared to processes. Different functions of the program can be executed in different threads.\nThreads are more scalabe compared to processes. (There can be much more threads than processes)\n\ntwo types of threads:\n\nuser threads: threads are managed within the process itself\nkernel threads: threads are managed by os globally.\nhow do these two types compare:\n\nswitching between user threads is faster, but since it is not controlled by the os, a blocking thread can slow down the program, whereas in kernel threads the os would issue an interrupt and switch to the next thread. In user threads the os would switch to another process (and switching between processes is in general quite expensive)\nKernel threads: in Multi-Core CPU’s various threads can be distributed among the cores by the us. (not possible in user threads)\n\n\nmixed models:\n\none-to-one\nmany-to-many\nmany-to-one\n\nthreads: threads share state, which enables faster communication, but introduces multitasking-related difficulties, like race conditions. (shared state is both an advantage and a disadvantage of threads)\nprocesses: they are isolated from each other, but this makes the communication difficult\nPOSIX Pthreads: pthread_*. An API for creating, deleting and synchronizing threads. (implementation of APIs exist both as user and kernel threads):\n\n\n\nrace conditions: lost updates \\(\\Rightarrow\\) process synchronization (how to solve such problems)",
    "crumbs": [
      "Weekly Summary",
      "Week 3"
    ]
  },
  {
    "objectID": "ss25/ibn/sum/w03.html#vl-5---28.04.25",
    "href": "ss25/ibn/sum/w03.html#vl-5---28.04.25",
    "title": "Week 3",
    "section": "",
    "text": "Threads are miniprocesses.\nMain idea: we want concurrency but still retain the recourses of a single process like the data, files and code. (different processes are isolated from each other and don’t share these resources)\nMultithreading is using multiple such threads in a single process \\(\\Rightarrow\\) They all use the same PCB (they coexist within the same process)\nAdvantages of threads w.r.t processes:\n\nsimple comunication between threads, they share the same resources, and it’s faster to synhronize them.\n(no system calls, no switching to kernel mode) Shared resources:\n\ncode\ndata\nfiles\n\nThis makes threads more responsive compared to processes. Different functions of the program can be executed in different threads.\nThreads are more scalabe compared to processes. (There can be much more threads than processes)\n\ntwo types of threads:\n\nuser threads: threads are managed within the process itself\nkernel threads: threads are managed by os globally.\nhow do these two types compare:\n\nswitching between user threads is faster, but since it is not controlled by the os, a blocking thread can slow down the program, whereas in kernel threads the os would issue an interrupt and switch to the next thread. In user threads the os would switch to another process (and switching between processes is in general quite expensive)\nKernel threads: in Multi-Core CPU’s various threads can be distributed among the cores by the us. (not possible in user threads)\n\n\nmixed models:\n\none-to-one\nmany-to-many\nmany-to-one\n\nthreads: threads share state, which enables faster communication, but introduces multitasking-related difficulties, like race conditions. (shared state is both an advantage and a disadvantage of threads)\nprocesses: they are isolated from each other, but this makes the communication difficult\nPOSIX Pthreads: pthread_*. An API for creating, deleting and synchronizing threads. (implementation of APIs exist both as user and kernel threads):\n\n\n\nrace conditions: lost updates \\(\\Rightarrow\\) process synchronization (how to solve such problems)",
    "crumbs": [
      "Weekly Summary",
      "Week 3"
    ]
  },
  {
    "objectID": "ss25/ibn/sum/w03.html#vl-6---30.04.25",
    "href": "ss25/ibn/sum/w03.html#vl-6---30.04.25",
    "title": "Week 3",
    "section": "VL 6 - 30.04.25",
    "text": "VL 6 - 30.04.25\nProcess synchronization:\n\nhow can data be communicated from one process to another?\ntaking dependencies into account: if B depends on A, i.e. if B receives data from A, than B should wait for A to be done, before executing.\nProcesses / threads shouldn’t disrupt or block each other.\n\n\nRace Conditions (Continued)\nrace condition: when multiple threads / processes read or write the same variable an update can be lost if one of the threads is interrupted before completing the update.\n\nThe producer - consumer problem: (Slide 8)\n\nproducer: writes to a buffer.\nconsumer: reads from a buffer.\nproblem: count variable is global, therefore an update , either count-- or count++ can get lost, causing count having a wrong final value (Klausurrelevant, Slide 9)\n\nSolution: processes or threads should have critical regions where the execution can not be interrupted - other processes or threads are blocked during this time. \\(\\Rightarrow\\) Mutual Exclusion\n\n\nMutual Exclusion\n\nNo two processes / threads can be executing simulatanously in their critical regions. (critical regions exclude each other)\nA process that is not in its critical region can not block other processes / threads.\n\nhow is it implemented\n\nhardware: a hardware switch (a single bit) is set to 1 when a thread is in its critical region which disables Interrupts.\nsoftware:\n\nprimitive solution: a single boolean variable (this allows only two simultanous threads / processes, therefore very limiting) (Slid 17)\nPeterson’s solution: (slide 19)\n\n\n\nGeneral solution:\n\nLocks and Lock varaibles\nSemaphores\n\n\nLocks\nidea: use a token. The ownership of the token means … (Slide 22)\n\nhardware solutions:\n\ntest and set lock - TSL. (Slide 23)\ninstruction swap - XCHG\n\n\nThese solutions are categorizing under active waiting (or spinlocks) \\(\\Rightarrow\\). Problems: * wasting of processor time. * priority inversoin (Prioritaetsumkehr).\n\n\nSemaphores\n\nintroduced by Dijkstra in 1965\nGeneralization of locks.\ntwo operations:\n\nwait()\nsingal()\n\nhow to implement mutual exclusion with semaphores (slides 29, 30)\nbinary semaphores are also called mutexes.\nlocks can be simulated with semaphores.\nlocks und semaphores (Klausurrelevant, Overview Slide 34, 35)\n\n\n\n\nA Common Situation - Waiting for a Condition / Event\n\nSimple solution - Polling: periodically querry the state of a variable in an infinite loop \\(\\Rightarrow\\) inefficient.\nbetter solution - efficient waiting: the thread is switched to ‘waiting’ state.",
    "crumbs": [
      "Weekly Summary",
      "Week 3"
    ]
  },
  {
    "objectID": "ss25/ibn/sum/w05.html",
    "href": "ss25/ibn/sum/w05.html",
    "title": "Week 5",
    "section": "",
    "text": "process first creates a shared memory segment:\nsegment_id = shmget(IPC_PRIVATE, size, S_IRUSR | S_IWSUR);\nprocesses that want access to this segment have to subscibe / attach to it.\nafter the process is done it detaches from the shared memory with shmdt(shared_memory).\nProblem: how other subscibers get this value segment_id \\(\\Rightarrow\\) fork()\nalternative (in windows): Memory-mapped-data (MMD) are used for IPC\nhandles (look up)\n\n\n\n\n\n\nSwapping: the complete memory image of a process is backed up / stored in the primary memory in order to protect the RAM and allow more processes.\nProblems:\n\nwhen processes are swapped in and out the memory addresses in the RAM change - how to remap or recalculate?\nswapping takes time - seconds\nprocesses that are currently doing I/O can’t be swapped in / out,\n\nsolution: prioritize swapping in / out the waiting or blocked processes\nadditional process state: blocked and suspended means swapped out\nadditional process state: ready and suspended means ready but still swapped out, needs to be first swapped in\n\ninternal fragmentation of RAM.\n\n\n\n\n\n\nIn past systems and actual memory and the addressable memory was the same - very limiting.\nGeneral question how to allocate memory to incoming processes ? Various approaches\n\nfixed-sized parititions: obvious problems - partitions are too small or too large\nvariable-sized partition schemes. problems:\n\naddress relocation: without virtual memory the address of the process must be reculculated in the program if swapped in and out\ncontigious address: without virtual memory the complete process has to have a contingious memory\nexternal fragmentation (btw, external fragmentation is worse in primary memory). How to solve this problem:\n\nReorganize the RAM \\(\\Rightarrow\\) compaction. Problems:\n\nThis is time intensive\nis I/O is happening problematic\nworks only for code reallocation in runtime.\n\nuse fitting schemes for memory allocation that allocated the memory well\n\nfirst fit:\nbest fit\nworst fit (usually worse than two other solutions)\n\n\n\n\n\n\n\nprevious twe problems:\n\nP1: illusion of fixed memory address from 0 to max\nP2: illusion of contigious memory\n\nare solved by introducing the concept logical memory (mapping physical to logical memory):\n\nThe cpu and the processes see (or use) completely different addresses from the actual memory addresses from the ram\nthe mapping is implemented directly in hardware by teh memory-management-unit - MMU.(usually inside the processor)\nThese logical addresses seen by the cpu and proccesses are called virtual memory / addresses.\nThe addresses in the RAM are physical memory / addresses\nhow is the mapping implemented? The process and the cpu sees the addresses from 0 to max. If the actual process is located in the memory at some address. MMU simply adds the offset to this address.\nsecond problem of contigious memory is solved with a table but the table is not refined up to bytes, rather up to pages (more rough subdivision of a process) \\(\\Rightarrow\\) page table\n\npage index (page = Seite)\nframe index (Frame = Seitenrahmen)",
    "crumbs": [
      "Weekly Summary",
      "Week 5"
    ]
  },
  {
    "objectID": "ss25/ibn/sum/w05.html#vl-9---12.05.25",
    "href": "ss25/ibn/sum/w05.html#vl-9---12.05.25",
    "title": "Week 5",
    "section": "",
    "text": "process first creates a shared memory segment:\nsegment_id = shmget(IPC_PRIVATE, size, S_IRUSR | S_IWSUR);\nprocesses that want access to this segment have to subscibe / attach to it.\nafter the process is done it detaches from the shared memory with shmdt(shared_memory).\nProblem: how other subscibers get this value segment_id \\(\\Rightarrow\\) fork()\nalternative (in windows): Memory-mapped-data (MMD) are used for IPC\nhandles (look up)\n\n\n\n\n\n\nSwapping: the complete memory image of a process is backed up / stored in the primary memory in order to protect the RAM and allow more processes.\nProblems:\n\nwhen processes are swapped in and out the memory addresses in the RAM change - how to remap or recalculate?\nswapping takes time - seconds\nprocesses that are currently doing I/O can’t be swapped in / out,\n\nsolution: prioritize swapping in / out the waiting or blocked processes\nadditional process state: blocked and suspended means swapped out\nadditional process state: ready and suspended means ready but still swapped out, needs to be first swapped in\n\ninternal fragmentation of RAM.\n\n\n\n\n\n\nIn past systems and actual memory and the addressable memory was the same - very limiting.\nGeneral question how to allocate memory to incoming processes ? Various approaches\n\nfixed-sized parititions: obvious problems - partitions are too small or too large\nvariable-sized partition schemes. problems:\n\naddress relocation: without virtual memory the address of the process must be reculculated in the program if swapped in and out\ncontigious address: without virtual memory the complete process has to have a contingious memory\nexternal fragmentation (btw, external fragmentation is worse in primary memory). How to solve this problem:\n\nReorganize the RAM \\(\\Rightarrow\\) compaction. Problems:\n\nThis is time intensive\nis I/O is happening problematic\nworks only for code reallocation in runtime.\n\nuse fitting schemes for memory allocation that allocated the memory well\n\nfirst fit:\nbest fit\nworst fit (usually worse than two other solutions)\n\n\n\n\n\n\n\nprevious twe problems:\n\nP1: illusion of fixed memory address from 0 to max\nP2: illusion of contigious memory\n\nare solved by introducing the concept logical memory (mapping physical to logical memory):\n\nThe cpu and the processes see (or use) completely different addresses from the actual memory addresses from the ram\nthe mapping is implemented directly in hardware by teh memory-management-unit - MMU.(usually inside the processor)\nThese logical addresses seen by the cpu and proccesses are called virtual memory / addresses.\nThe addresses in the RAM are physical memory / addresses\nhow is the mapping implemented? The process and the cpu sees the addresses from 0 to max. If the actual process is located in the memory at some address. MMU simply adds the offset to this address.\nsecond problem of contigious memory is solved with a table but the table is not refined up to bytes, rather up to pages (more rough subdivision of a process) \\(\\Rightarrow\\) page table\n\npage index (page = Seite)\nframe index (Frame = Seitenrahmen)",
    "crumbs": [
      "Weekly Summary",
      "Week 5"
    ]
  },
  {
    "objectID": "ss25/ibn/sum/w05.html#vl-10---14.05.25",
    "href": "ss25/ibn/sum/w05.html#vl-10---14.05.25",
    "title": "Week 5",
    "section": "VL 10 - 14.05.25",
    "text": "VL 10 - 14.05.25\n\nMemory Management\n\na page index must be translated to a frame index\n\npage is the virtual thing seen by the process and the processor,\nframe is the actual physical location in the RAM.\n\nTranslated with a function: \\(F_{\\text{pid}}(p) = f\\)\np := page index\nf := frame\n\nhow does the translation work ? examples in the lecture",
    "crumbs": [
      "Weekly Summary",
      "Week 5"
    ]
  },
  {
    "objectID": "ss25/ibn/sum/w07.html",
    "href": "ss25/ibn/sum/w07.html",
    "title": "Week 7",
    "section": "",
    "text": "page swapping - revision of algorithms\n\nfifo\nsecond-chance\nimprovement of second-chance ds \\(\\Rightarrow\\) clock algorithm\n\n\n\n\n\n\n\nin addition to the R-bit also M-bit: M-bit: the page has been modified\n\n\n\n\n\nthe least recently used page is chosen as the victim to be swapped out\npragmatic problem: page access happens extremely frequently and there are millions of pages - how to track the access data / implement LRU realistically? \\(\\Rightarrow\\) epochs and bit sequences whether the page was used during the epoch.\n\n\n\n\n\nnumber of accesses to each page is counted\n\n\n\n\n\n\ndemand paging (used in all modern operating systems):\n\npages are initially mostly in the secondary memory and loaded dynamically when page faults occure\n\ncopy-on-write: when fork() is initially called and a child process is created, the child process gets exactly all the same page table and frames in memory (instead of copying all the frames completely). The frames are copied when one of the processes tries to write\nWhy does demand paging work well ? \\(\\Rightarrow\\) localicty of reference.\n\nworking set: set of pages that are used by the process in the last time period \\(\\Delta\\).\n\nwhne a new “context” is used there is initally a spike in page faults - but it subsides quickly as the pages are loaded\nWorking sets can be used for page swapping in combination with LRU.\n\ntrashing:\n\n\n\n\n\nLast memory management topic.\n\nso far we assumed that a process has a linear, contigious logical address space.\nin reality the logical address space of a process is divided (segmented) in various spaces corresponding to different functional parts of the program:\n\nparse tree\nconstant table\nsource text\nsymbol table\nheap\nshared memory\nshared library\nstacks (sometimes two staks for a thread)\n\nThe programmer doesn’t have to manage these semgents manually.\nSegment: for completely independent parts of the program, independent logical spaces are introduced\n\nrealised with the introduction of another level of abstraction on top of the logical address space where for each segment number the base address and the limit address is stored in a segment table. T\nTranslation if also done in hardware.\nbut since nowadays we have huge logical address space the segments don’t collide\neach processs has two descriptor tables:\n\nlocal descriptor table (LDT)\nglobal descriptor table (GDT)",
    "crumbs": [
      "Weekly Summary",
      "Week 7"
    ]
  },
  {
    "objectID": "ss25/ibn/sum/w07.html#vl-13---26.05.25",
    "href": "ss25/ibn/sum/w07.html#vl-13---26.05.25",
    "title": "Week 7",
    "section": "",
    "text": "page swapping - revision of algorithms\n\nfifo\nsecond-chance\nimprovement of second-chance ds \\(\\Rightarrow\\) clock algorithm\n\n\n\n\n\n\n\nin addition to the R-bit also M-bit: M-bit: the page has been modified\n\n\n\n\n\nthe least recently used page is chosen as the victim to be swapped out\npragmatic problem: page access happens extremely frequently and there are millions of pages - how to track the access data / implement LRU realistically? \\(\\Rightarrow\\) epochs and bit sequences whether the page was used during the epoch.\n\n\n\n\n\nnumber of accesses to each page is counted\n\n\n\n\n\n\ndemand paging (used in all modern operating systems):\n\npages are initially mostly in the secondary memory and loaded dynamically when page faults occure\n\ncopy-on-write: when fork() is initially called and a child process is created, the child process gets exactly all the same page table and frames in memory (instead of copying all the frames completely). The frames are copied when one of the processes tries to write\nWhy does demand paging work well ? \\(\\Rightarrow\\) localicty of reference.\n\nworking set: set of pages that are used by the process in the last time period \\(\\Delta\\).\n\nwhne a new “context” is used there is initally a spike in page faults - but it subsides quickly as the pages are loaded\nWorking sets can be used for page swapping in combination with LRU.\n\ntrashing:\n\n\n\n\n\nLast memory management topic.\n\nso far we assumed that a process has a linear, contigious logical address space.\nin reality the logical address space of a process is divided (segmented) in various spaces corresponding to different functional parts of the program:\n\nparse tree\nconstant table\nsource text\nsymbol table\nheap\nshared memory\nshared library\nstacks (sometimes two staks for a thread)\n\nThe programmer doesn’t have to manage these semgents manually.\nSegment: for completely independent parts of the program, independent logical spaces are introduced\n\nrealised with the introduction of another level of abstraction on top of the logical address space where for each segment number the base address and the limit address is stored in a segment table. T\nTranslation if also done in hardware.\nbut since nowadays we have huge logical address space the segments don’t collide\neach processs has two descriptor tables:\n\nlocal descriptor table (LDT)\nglobal descriptor table (GDT)",
    "crumbs": [
      "Weekly Summary",
      "Week 7"
    ]
  },
  {
    "objectID": "ss25/ibn/sum/w07.html#vl-14---28.05.25",
    "href": "ss25/ibn/sum/w07.html#vl-14---28.05.25",
    "title": "Week 7",
    "section": "VL 14 - 28.05.25",
    "text": "VL 14 - 28.05.25\n\nFiles and File Systems\n\ndata (files) are the third and (final) abstraction that we will consider.\nwhy data:\n\nlong term storage\nRAM is limited\npersistency\n\nstructuring of data\n\ndata as byte sequences\nsequence of data words\ndata stored as a tree data structure (not used anymore)\n\n\n\nTypes of Files and Directories\n\nregular files: unstructured byte sequences\ndirectories: virtual ‘container / box’\nnamed pipes:\nsockets / socket devices\nvirtual device files:\nfile attributes\n\naccess rights, password, creator, owner, …\n\nHierachical Filesystem :Tree like structure organized as nested directories - it is the defacto standard today\n\ndirectories:\n\nnames of files and other directories\nI-nodes:\n\n\nCurrent file ssystems:\n\nfat, fat32, ntfs\nlinux: ext …\n\nlinks: a reference to a file or a directory\n\nlinux:\n\nsymbolic links\nhard links\n\n\n\n\n\nFile Operations\n\ncreate, delete, open, close\nread / write\nappend\nseek\n\n\n\nHow Data are Stored on Magnetic Disks\n\nhow magnetic disks are addressed:\n\nsector\ncylinder\nring number\n\nsimilar to virtual to physical address mapping, there is a logical to physical address translation\nhow is it store in:\n\ncontigious \\(\\Rightarrow\\) fragmentation\nlinked lists \\(\\Rightarrow\\) slow random access / seek\n\nimprovement \\(\\Rightarrow\\) FAT: only the ponters are stored in a table in RAM \\(\\Rightarrow\\) faster seek\n\ndisadvantages: the table itself might be too large\n\n\ni-nodes (best solution):",
    "crumbs": [
      "Weekly Summary",
      "Week 7"
    ]
  },
  {
    "objectID": "ss25/ibn/sum/w09.html",
    "href": "ss25/ibn/sum/w09.html",
    "title": "Week 9",
    "section": "",
    "text": "holiday",
    "crumbs": [
      "Weekly Summary",
      "Week 9"
    ]
  },
  {
    "objectID": "ss25/ibn/sum/w09.html#vl-17---09.06.25",
    "href": "ss25/ibn/sum/w09.html#vl-17---09.06.25",
    "title": "Week 9",
    "section": "",
    "text": "holiday",
    "crumbs": [
      "Weekly Summary",
      "Week 9"
    ]
  },
  {
    "objectID": "ss25/ibn/sum/w09.html#vl-18---11.06.25",
    "href": "ss25/ibn/sum/w09.html#vl-18---11.06.25",
    "title": "Week 9",
    "section": "VL 18 - 11.06.25",
    "text": "VL 18 - 11.06.25\n\nDeadlocks (cont)\n\nprevention of deadlocks\n\n\n\nScheduling of Processes\n\ndifferentiate between types of processes\n\nI/O intensive\nCPU intensive (CPU bound)\n\n\n\nscudulnig strategies\n\nfirst-come first-served (FCFS): simsple solution first requested process is executed until the end\n\nproblems: convoy effect\n\nshortest-job first (SJF):\nshortest-remaining time first (SRTF)",
    "crumbs": [
      "Weekly Summary",
      "Week 9"
    ]
  },
  {
    "objectID": "ss25/ibn/sum/w11.html",
    "href": "ss25/ibn/sum/w11.html",
    "title": "Week 11",
    "section": "",
    "text": "OSI\nhttp\nips, hosts, ports",
    "crumbs": [
      "Weekly Summary",
      "Week 11"
    ]
  },
  {
    "objectID": "ss25/ibn/sum/w11.html#vl-23---23.06.25",
    "href": "ss25/ibn/sum/w11.html#vl-23---23.06.25",
    "title": "Week 11",
    "section": "",
    "text": "OSI\nhttp\nips, hosts, ports",
    "crumbs": [
      "Weekly Summary",
      "Week 11"
    ]
  },
  {
    "objectID": "ss25/scicomp/index.html",
    "href": "ss25/scicomp/index.html",
    "title": "Object Oriented Scientific Computing with C++",
    "section": "",
    "text": "notes\nsolutions\nweekly-summary",
    "crumbs": [
      "scicomp"
    ]
  },
  {
    "objectID": "ss25/scicomp/sum/w05.html",
    "href": "ss25/scicomp/sum/w05.html",
    "title": "Week 05",
    "section": "",
    "text": "associative containers - ordered and unordered\ncompanion classes -\n\nstd::pair,\nstd::tuple,\nstd::variant,\nstd::any,\nstd::optional\n\nusing companion classes instead of custom structs is also advantagous due to structured bindings\nsmart pointers\n\nunique pointer\nshared pointer\nweak pointer\n\nlambda expressions and closures\n\nlambda expressoins in c++ are syntactical sugar for functors",
    "crumbs": [
      "Weekly Summary",
      "Week 05"
    ]
  },
  {
    "objectID": "ss25/scicomp/sum/w05.html#lecture-5---13.05.25",
    "href": "ss25/scicomp/sum/w05.html#lecture-5---13.05.25",
    "title": "Week 05",
    "section": "",
    "text": "associative containers - ordered and unordered\ncompanion classes -\n\nstd::pair,\nstd::tuple,\nstd::variant,\nstd::any,\nstd::optional\n\nusing companion classes instead of custom structs is also advantagous due to structured bindings\nsmart pointers\n\nunique pointer\nshared pointer\nweak pointer\n\nlambda expressions and closures\n\nlambda expressoins in c++ are syntactical sugar for functors",
    "crumbs": [
      "Weekly Summary",
      "Week 05"
    ]
  },
  {
    "objectID": "ss25/scicomp/sum/w07.html",
    "href": "ss25/scicomp/sum/w07.html",
    "title": "Week 07",
    "section": "",
    "text": "intro to oop\ninheritence:\n\npublic vs private\nhow it to compisiton\n\nSOLID principle\ninheritence vs composition\nslicing in polyorphism\ndipatch tables (virtual tables)\n\n\n\n\nmain features: dynamic polymorphism, inheritence and designing / modelling software in terms of objects.\nfurther topics:\n\nresource acquisition is initialization (RAII)\ndesing patters and design choices",
    "crumbs": [
      "Weekly Summary",
      "Week 07"
    ]
  },
  {
    "objectID": "ss25/scicomp/sum/w07.html#vl-7---27.05.25",
    "href": "ss25/scicomp/sum/w07.html#vl-7---27.05.25",
    "title": "Week 07",
    "section": "",
    "text": "intro to oop\ninheritence:\n\npublic vs private\nhow it to compisiton\n\nSOLID principle\ninheritence vs composition\nslicing in polyorphism\ndipatch tables (virtual tables)\n\n\n\n\nmain features: dynamic polymorphism, inheritence and designing / modelling software in terms of objects.\nfurther topics:\n\nresource acquisition is initialization (RAII)\ndesing patters and design choices",
    "crumbs": [
      "Weekly Summary",
      "Week 07"
    ]
  },
  {
    "objectID": "ss25/scicomp/sum/w09.html",
    "href": "ss25/scicomp/sum/w09.html",
    "title": "Week 09",
    "section": "",
    "text": "desing patterns:\n\ncreational patterns:\n\nsingleton\nfactory pattern - factory objects\nfactory pattern - extensible factory objects (used in Google Test)\nbuilder pattern - constructing complex objects\ncombining builder pattern with factories\nbuilder pattern - dynamic polymorphism through lambdas\n\nbehavioral patterns: next time\n\nprogram design outline:\ntemplate metaprogramming:\n\nfunction template specialization\nclass template specialization",
    "crumbs": [
      "Weekly Summary",
      "Week 09"
    ]
  },
  {
    "objectID": "ss25/scicomp/sum/w09.html#vl-9---10.06.25",
    "href": "ss25/scicomp/sum/w09.html#vl-9---10.06.25",
    "title": "Week 09",
    "section": "",
    "text": "desing patterns:\n\ncreational patterns:\n\nsingleton\nfactory pattern - factory objects\nfactory pattern - extensible factory objects (used in Google Test)\nbuilder pattern - constructing complex objects\ncombining builder pattern with factories\nbuilder pattern - dynamic polymorphism through lambdas\n\nbehavioral patterns: next time\n\nprogram design outline:\ntemplate metaprogramming:\n\nfunction template specialization\nclass template specialization",
    "crumbs": [
      "Weekly Summary",
      "Week 09"
    ]
  },
  {
    "objectID": "ss25/scicomp/sum/w11.html",
    "href": "ss25/scicomp/sum/w11.html",
    "title": "Week 11",
    "section": "",
    "text": "template metaprogramming (cont)\n\ndependent base classes\ncuriously recurring template pattern (CRTP)\ncompile time computations:\n\nclassic implementation vs more modern was done with constexpr\nSFINAE",
    "crumbs": [
      "Weekly Summary",
      "Week 11"
    ]
  },
  {
    "objectID": "ss25/scicomp/sum/w11.html#vl-11---23.06.25",
    "href": "ss25/scicomp/sum/w11.html#vl-11---23.06.25",
    "title": "Week 11",
    "section": "",
    "text": "template metaprogramming (cont)\n\ndependent base classes\ncuriously recurring template pattern (CRTP)\ncompile time computations:\n\nclassic implementation vs more modern was done with constexpr\nSFINAE",
    "crumbs": [
      "Weekly Summary",
      "Week 11"
    ]
  },
  {
    "objectID": "ws23-24/ipi/ipi.html",
    "href": "ws23-24/ipi/ipi.html",
    "title": "IPI",
    "section": "",
    "text": "weekly summary\nnotes\nsolutions\ncourse website",
    "crumbs": [
      "IPI"
    ]
  },
  {
    "objectID": "ws23-24/num/num.html",
    "href": "ws23-24/num/num.html",
    "title": "Numerics 0",
    "section": "",
    "text": "weekly summary\nnotes\nsolutions",
    "crumbs": [
      "Numerics 0"
    ]
  },
  {
    "objectID": "ws24-25/ds/index.html",
    "href": "ws24-25/ds/index.html",
    "title": "Discrete Structures",
    "section": "",
    "text": "weekly summary\nnotes\nsolutions"
  },
  {
    "objectID": "ws24-25/ds/index.html#section",
    "href": "ws24-25/ds/index.html#section",
    "title": "Discrete Structures",
    "section": "",
    "text": "weekly summary\nnotes\nsolutions"
  },
  {
    "objectID": "ws24-25/index.html#courses",
    "href": "ws24-25/index.html#courses",
    "title": "WS 24/25",
    "section": "Courses",
    "text": "Courses\n\nITS\nISW\nDS",
    "crumbs": [
      "Bachelor",
      "WS 24/25"
    ]
  },
  {
    "objectID": "ws24-25/isw/sum.html",
    "href": "ws24-25/isw/sum.html",
    "title": "My Uni Notes",
    "section": "",
    "text": "date: 15/10/24\nIntro\n\nLOC = lines of code.\nDevelopment time - quality\nSWE at-large vs at-small\nActivities and design decisions\n\norganisatory: moodle pass isw2425\nacrasia\n\n\n\n\n\ndate: 15/10/24\norganisatory infos\njava overview\n\n\n\n\n\ndate: 16/10/24"
  },
  {
    "objectID": "ws24-25/isw/sum.html#week-1",
    "href": "ws24-25/isw/sum.html#week-1",
    "title": "My Uni Notes",
    "section": "",
    "text": "date: 15/10/24\nIntro\n\nLOC = lines of code.\nDevelopment time - quality\nSWE at-large vs at-small\nActivities and design decisions\n\norganisatory: moodle pass isw2425\nacrasia\n\n\n\n\n\ndate: 15/10/24\norganisatory infos\njava overview\n\n\n\n\n\ndate: 16/10/24"
  },
  {
    "objectID": "ws24-25/isw/sum.html#week-2",
    "href": "ws24-25/isw/sum.html#week-2",
    "title": "My Uni Notes",
    "section": "Week 2",
    "text": "Week 2\n\nLecture 1\n\ndate: 22/10/24\nProcess & Project\nTeam organisation\nExtreme programming\nsoftware development tools like git\nrequirements engineering\nGenAI (Uni Heidelberg =&gt; Yoki)\n\n\n\nLecture 2 (tech)\n\ndate: 22/10/24\njava review 2:\n\nInheritence\nObject construction and types casts\nparameter passing: primitive types as value, objects as reference\n\ngit review:\nchatGPT overview"
  },
  {
    "objectID": "ws24-25/isw/sum.html#week-3",
    "href": "ws24-25/isw/sum.html#week-3",
    "title": "My Uni Notes",
    "section": "Week 3",
    "text": "Week 3\n\nLecture 1\n\ndate: 29/10/24\nRequirements Engineering\n\nUsage/User Description\nTask and FuncArrayListtion Description\nGUI Description\n\n\n\n\nLecture 2 (tech)\n\ndate: 29/10/24\ngit\nissue tracking\nandroid basics\n\n\n\nTutorium\n\nQuestions:\nA.3.1:\n\nhow exactly combat score?\n\n\n\n\nA.3.2:\n\nArbeitsbereich?\nUI-Struktur-Diagramm?\nkonkrete Sicht / virtual Windows ?\nexcel datei selbst erstellen oder ist sie auf confluence?\n\nA.3.3:\n\nis MainActivity a UI class or a data class?\nWhat are\n\nRecyclerView ?\nAdapter ?\nViewHolder ?\nimport de.uhd.ifi.pokemonmanager.R ?\n\nLayout Ressourcen fuer MainActivity und RecyclerView?"
  },
  {
    "objectID": "ws24-25/isw/sum.html#week-4",
    "href": "ws24-25/isw/sum.html#week-4",
    "title": "My Uni Notes",
    "section": "Week 4",
    "text": "Week 4\n\nLecture 1\n\ndate: 05/11/24\nui-design principles\nusability\n\n\n\nLecture 2 (tech)\n\ndate: 05/11/24\nsome advanced java stuff"
  },
  {
    "objectID": "ws24-25/isw/sum.html#week-5",
    "href": "ws24-25/isw/sum.html#week-5",
    "title": "My Uni Notes",
    "section": "Week 5",
    "text": "Week 5\n\nLecture 1\n\ndate:\n\n\n\nLecture 2\n\ndate:"
  },
  {
    "objectID": "ws24-25/isw/sum.html#week-6",
    "href": "ws24-25/isw/sum.html#week-6",
    "title": "My Uni Notes",
    "section": "Week 6",
    "text": "Week 6\n\nLecture 1\n\ndate:\n\n\n\nLecture 2\n\ndate:"
  },
  {
    "objectID": "ws24-25/isw/sum.html#week-7",
    "href": "ws24-25/isw/sum.html#week-7",
    "title": "My Uni Notes",
    "section": "Week 7",
    "text": "Week 7\n\nLecture 1\n\ndate:\n\n\n\nLecture 2\n\ndate:"
  },
  {
    "objectID": "ws24-25/isw/sum.html#week-8",
    "href": "ws24-25/isw/sum.html#week-8",
    "title": "My Uni Notes",
    "section": "Week 8",
    "text": "Week 8\n\nLecture 1\n\ndate:\n\n\n\nLecture 2\n\ndate:"
  },
  {
    "objectID": "ws24-25/isw/sum.html#week-9",
    "href": "ws24-25/isw/sum.html#week-9",
    "title": "My Uni Notes",
    "section": "Week 9",
    "text": "Week 9\n\nLecture 1\n\ndate:\n\n\n\nLecture 2\n\ndate:"
  },
  {
    "objectID": "ws24-25/isw/sum.html#week-10",
    "href": "ws24-25/isw/sum.html#week-10",
    "title": "My Uni Notes",
    "section": "Week 10",
    "text": "Week 10\n\nLecture 1\n\ndate:\n\n\n\nLecture 2\n\ndate:"
  },
  {
    "objectID": "ws24-25/isw/sum.html#week-11",
    "href": "ws24-25/isw/sum.html#week-11",
    "title": "My Uni Notes",
    "section": "Week 11",
    "text": "Week 11\n\nLecture 1\n\ndate:\n\n\n\nLecture 2\n\ndate:"
  },
  {
    "objectID": "ws24-25/isw/sum.html#week-12",
    "href": "ws24-25/isw/sum.html#week-12",
    "title": "My Uni Notes",
    "section": "Week 12",
    "text": "Week 12\n\nLecture 1\n\ndate:\n\n\n\nLecture 2\n\ndate:"
  },
  {
    "objectID": "ws24-25/isw/sum.html#week-13",
    "href": "ws24-25/isw/sum.html#week-13",
    "title": "My Uni Notes",
    "section": "Week 13",
    "text": "Week 13\n\nLecture 1\n\ndate:\n\n\n\nLecture 2\n\ndate:"
  },
  {
    "objectID": "ws24-25/isw/sum.html#week-14",
    "href": "ws24-25/isw/sum.html#week-14",
    "title": "My Uni Notes",
    "section": "Week 14",
    "text": "Week 14\n\nLecture 1\n\ndate:\n\n\n\nLecture 2\n\ndate:"
  },
  {
    "objectID": "ws24-25/isw/sum.html#week-15",
    "href": "ws24-25/isw/sum.html#week-15",
    "title": "My Uni Notes",
    "section": "Week 15",
    "text": "Week 15\n\nLecture 1\n\ndate:\n\n\n\nLecture 2\n\ndate:"
  },
  {
    "objectID": "ws24-25/its/sum.html",
    "href": "ws24-25/its/sum.html",
    "title": "Weekly Summary",
    "section": "",
    "text": "date: 14/10/24\nmassive increases in cyber-attacks in last 2-3 years. who is carrying out the attacks? how to find out? Is AI responsible for the increase?\nIntro & infos:\n\nLernziele:\nOrganisational:\n\nwebsite\nall informations/materials on moodle.\n\n\nransomware (check out the wiki article)\nInformation security vs IT-Security: former is a subset of the latter. IT-Security is information security in the realm of electronic information.\nGeneral 3 goals of Information security - CIA:\n\nConfidentiality\nIntegrity\nAvailability\n\nCastle analogy. Thick walls can protect, what about the gates? How to determine who belongs to the castle, who doesn’t? Increasing security measures can directly cause new vulnerabilities.\n\n\n\n\n\ndate: 15/10/24\norganisatory infos:\n\n6 Assignment sheets, 50% for the exam.\ntutorials on heico."
  },
  {
    "objectID": "ws24-25/its/sum.html#week-1",
    "href": "ws24-25/its/sum.html#week-1",
    "title": "Weekly Summary",
    "section": "",
    "text": "date: 14/10/24\nmassive increases in cyber-attacks in last 2-3 years. who is carrying out the attacks? how to find out? Is AI responsible for the increase?\nIntro & infos:\n\nLernziele:\nOrganisational:\n\nwebsite\nall informations/materials on moodle.\n\n\nransomware (check out the wiki article)\nInformation security vs IT-Security: former is a subset of the latter. IT-Security is information security in the realm of electronic information.\nGeneral 3 goals of Information security - CIA:\n\nConfidentiality\nIntegrity\nAvailability\n\nCastle analogy. Thick walls can protect, what about the gates? How to determine who belongs to the castle, who doesn’t? Increasing security measures can directly cause new vulnerabilities.\n\n\n\n\n\ndate: 15/10/24\norganisatory infos:\n\n6 Assignment sheets, 50% for the exam.\ntutorials on heico."
  },
  {
    "objectID": "ws24-25/its/sum.html#week-2",
    "href": "ws24-25/its/sum.html#week-2",
    "title": "Weekly Summary",
    "section": "Week 2",
    "text": "Week 2\n\nLecture\n\ndate: 21/10/24\nhow effective is security by obscurity? (not at all) in fact, open protocols and algorithms enhance security\nISO/OSI Reference modell.\n\nPhysical layer: not part of the lecture\ndata link layer\nnetwork layer\ntransport layer\nsession layer\npresentation layer\napplication layer\n\nsmtp(insecure), ftp, http\n\n\nProtocols\n\nARP: ARP-spoofing, ARP-poisoning (MITM: Man in the Middle)\n\n\n\n\nTutorium\n\ndate: 22/10/24\nSANS penetration testing\nSchwachstelle/Weakness: Any error oder weakness that violates the CIA principles.\n\ne.g:\n\nBedrohung/Threat: Actor, that takes advantage of a weakness\nRisiko/Risk: Damage, that can occur due to a weakness.\nExploit: The actual code or the actual process that takes advantage of the weakness.\nPenetration Test:\n\nScope: IP adress space\nRecon (Reconessence) \\(\\rightarrow\\) Scanning & initial access space \\(\\rightarrow\\) Exploitation \\(\\rightarrow\\) Post-exploitation\nnmap:\n\nTCP:\n\nSyn-pakete\n\nopen\nclosed\nfiltered\nfiltered\n\n\n\nmasscan:"
  },
  {
    "objectID": "ws24-25/its/sum.html#week-3",
    "href": "ws24-25/its/sum.html#week-3",
    "title": "Weekly Summary",
    "section": "Week 3",
    "text": "Week 3\n\nLecture\n\ndate: 28/10/24\nCrytpographic methods\n\nbasic definitions of cryptology, cryptography encryption, decryption, signing data\nClassification of cryptographic algorithms:\n\ncrypotgraphic hash-functions\nsymmetric cryptographic algorithms\nasymmetric cryptographic algorithms\n\nCryptogrpahic check values.\n\nDifference to Information/Coding theory: Coding theory random errors\n\n\ncyrptographic hash functions:\n\ncompression\nease of computation\n…\nmd5, sha-1, sha-2 families\n\n\n\n\nTutorium\n\ndate: 29/10/24 lecture review:\n\nMAC is used for authentication:\n\nMAC(K, msg) = Sha256(k || msg)\n\nct = Enc(k, pt) ct == cypher-text. pt == plain-text.\nEven more securer: ct = Enc(MAC || Enc(pt, K))\nstrong collision resistence implies weak collision resistency.\nx’ collides with x iff h(x) = h(x’)."
  },
  {
    "objectID": "ws24-25/its/sum.html#week-4",
    "href": "ws24-25/its/sum.html#week-4",
    "title": "Weekly Summary",
    "section": "Week 4",
    "text": "Week 4\n\nLecture\n\ndate: 04/11/24\nlecture review:\n\nMDC (Modification Detection Code) (Integrity)\nMAC (Message Authentification Code) (authenticity)\n\nMAC is a cryptographic checksum.\n\nBirthday paradox - relation to hashing.\nSHA\nPasswords\n\nshouldn’t be plain text\nSalt & Pepper (?)\n\nBrute-froce attack\n\nhydra\n\n\n\n\nTutorium\n\ndate: 05/11/24\nwebsites cryptography / cryptoanalhysis challenges:\n\ncryptohack\ncryptopals\njuiceshop\nhackthebox\nctftime.org"
  },
  {
    "objectID": "ws24-25/its/sum.html#week-5",
    "href": "ws24-25/its/sum.html#week-5",
    "title": "Weekly Summary",
    "section": "Week 5",
    "text": "Week 5\n\ndate:"
  },
  {
    "objectID": "ws24-25/its/sum.html#week-6",
    "href": "ws24-25/its/sum.html#week-6",
    "title": "Weekly Summary",
    "section": "Week 6",
    "text": "Week 6\n\ndate:"
  },
  {
    "objectID": "ws24-25/its/sum.html#week-7",
    "href": "ws24-25/its/sum.html#week-7",
    "title": "Weekly Summary",
    "section": "Week 7",
    "text": "Week 7\n\ndate:"
  },
  {
    "objectID": "ws24-25/its/sum.html#week-8",
    "href": "ws24-25/its/sum.html#week-8",
    "title": "Weekly Summary",
    "section": "Week 8",
    "text": "Week 8\n\ndate:"
  },
  {
    "objectID": "ws24-25/its/sum.html#week-9",
    "href": "ws24-25/its/sum.html#week-9",
    "title": "Weekly Summary",
    "section": "Week 9",
    "text": "Week 9\n\ndate:"
  },
  {
    "objectID": "ws24-25/its/sum.html#week-10",
    "href": "ws24-25/its/sum.html#week-10",
    "title": "Weekly Summary",
    "section": "Week 10",
    "text": "Week 10\n\ndate:"
  },
  {
    "objectID": "ws24-25/its/sum.html#week-11",
    "href": "ws24-25/its/sum.html#week-11",
    "title": "Weekly Summary",
    "section": "Week 11",
    "text": "Week 11\n\ndate:"
  },
  {
    "objectID": "ws24-25/its/sum.html#week-12",
    "href": "ws24-25/its/sum.html#week-12",
    "title": "Weekly Summary",
    "section": "Week 12",
    "text": "Week 12\n\ndate:"
  },
  {
    "objectID": "ws24-25/its/sum.html#week-13",
    "href": "ws24-25/its/sum.html#week-13",
    "title": "Weekly Summary",
    "section": "Week 13",
    "text": "Week 13\n\ndate:"
  },
  {
    "objectID": "ws24-25/its/sum.html#week-14",
    "href": "ws24-25/its/sum.html#week-14",
    "title": "Weekly Summary",
    "section": "Week 14",
    "text": "Week 14\n\ndate:"
  },
  {
    "objectID": "ws24-25/its/sum.html#week-15",
    "href": "ws24-25/its/sum.html#week-15",
    "title": "Weekly Summary",
    "section": "Week 15",
    "text": "Week 15\n\ndate:"
  }
]